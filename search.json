[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJan 5, 2024\n\n\nLinear Transformers Are Faster After All\n\n\nJacob Buckman, Carles Gelada\n\n\n\n\nJan 4, 2024\n\n\nOur Mission\n\n\nJacob Buckman, Carles Gelada\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "articles/linear-transformers-are-faster/index.html",
    "href": "articles/linear-transformers-are-faster/index.html",
    "title": "Linear Transformers Are Faster",
    "section": "",
    "text": "\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\Z}{\\mathbb{Z}}\n\\newcommand{\\N}{\\mathbb{N}}\n\\newcommand{\\sft}{\\text{softmax}}\n\\newcommand{\\List}{\\text{List}}\n\\newcommand{\\Seq}{\\text{Seq}}\n\\newcommand{\\SeqT}{\\text{SeqT}}\n\\newcommand{\\CSeqT}{\\text{CSeqT}}\n\\newcommand{\\Dist}{\\text{Dist}}\n\\newcommand{\\SM}{\\text{SM}}\n\\newcommand{\\Fn}{\\text{Fn}}\n\\newcommand{\\Tok}{\\text{Tok}}\n\\newcommand{\\Aij}{ A_{[i,j]}}\n\\]\nIt is well-known that removing the exponential from the attention layer of a transformer allows for a recurrent reformulation, with computational cost that is linear instead of quadratic on context length [1]. One would expect that such an architecture would be far faster to train, especially when the context size is large. However, when training deep neural networks on hardware accelerators (e.g. GPUs), reductions in FLOPs do not always straightforwardly translate into practical speedups. Initial empirical experiments showed that large language models based on linear transformers train more slowly than classic transformers, and led many to dismiss the approach as nice-in-theory but unhelpful-in-practice [2].\nAt the moment, the conventional wisdom in the field is that a quadratic-cost algorithm with highly optimized hardware utilization (e.g. FlashAttention) gives the best training throughput [3]. But this is mistaken. In this post, we explain several different ways of implementing linear transformers and we show that, in fact, they can produce massive speed-ups.\nThe experiment below showcases the central takeaway. We compare the speed of an optimized transformer, a straightforward recurrent implementation of the linear transformer (as described in [1]), and an optimized linear transformer (described in Section 3). We see that, despite exhibiting better growth with respect to context size, the linear transformer trains much more slowly than the transformer in wall-clock time. In contrast, our algorithm is strictly faster than even the highly-optimized FlashAttention baseline, at all context and model sizes. At moderately large context sizes, this speedup has an larger impact than FlashAttention itself.\nBut speed is only one part of the picture. We also need to know whether linear transformers actually minimize the loss as well as standard transformers do. Unfortunately, as the next plot shows, directly converting GPT2 to use linear transformer layers hurts learning significantly.\nThese results are discussed in detail in Section 5, but in short: linear transformers seem to become increasingly unstable as the sequence length grows, negatively affecting learning. This means that our linear transformers are somewhat useless,1 as the positive impact from the speedup seen in long contexts is undermined by the negative impact of degraded learning.\nOther variants of linear transformers have been proposed that claim resolve these learning issues [5]–[11], but we do not survey them today. Since our main motivation in this post is to share our insights on how to implement efficient linear transformers we stick with the most natural variant, which is most closely analogous to standard transformers. In a future post, we will explain how to improve the learning of linear transformers, allowing us to efficiently train unprecedentedly-long-context language models with super-fast sampling."
  },
  {
    "objectID": "articles/linear-transformers-are-faster/index.html#linear-transformers",
    "href": "articles/linear-transformers-are-faster/index.html#linear-transformers",
    "title": "Linear Transformers Are Faster",
    "section": "1. Linear Transformers",
    "text": "1. Linear Transformers\nThe inputs to a transformer layer are sequences of \\(Q_i, K_i, V_i \\in \\R^d\\) of query, key and values, where \\(i\\) ranges from \\(1\\) to the sequence length \\(t\\). The outputs are a sequence \\(Y_i\\in \\R^d\\). The well-known formula for the transformer layer, first popularized by Vaswani et al [12], is: \\[\nY_i^\\text{Transformer} = \\sum_{j=1}^i e^{Q^T_i K_j} V_j\n\\] Here we are excluding the denominator of the softmax for simplicity of notation. Although the normalization provided by the denominator is important for good learning performance, it doesn’t have a major impact on the computational cost or implementation speed, which are our main focus here.\n\n\nEven though we omit the normalization for the mathematical exposition, it is included in our implementation for all experiments.\nThe formula for the linear transformer (LT) layer is quite similar: just change the term \\(e^{Q^T_i K_j} \\to  Q^T_i K_j\\) yielding \\[\nY_i^\\text{LinearTransformer} = \\sum_{j=1}^i Q^T_i K_j V_j\n\\]\n\n\nAll our experiments with linear transformers also include a normalization factor, which we empirically found to be important for learning performance. Drawing on [1], we divide each \\(Y_i\\) by \\(\\sum_{j=1}^i Q^T_i K_j\\) after eunsuring the sum is positive by making keys and queries live in the positive quadrant using softplus.\nThis layer is “linear” in that the outputs \\(Y\\) are linearly related to all of \\(Q\\), \\(K\\), and \\(V\\).2 From now on, we will omit the superscript of \\(Y_i^\\text{LinearTransformer}\\) and just write \\(Y_i\\). To begin our exploration of the computational cost of linear transformers, consider the following implementation.\ndef LT_attention(Q, K, V):\n    \"\"\"\n    Shapes of inputs are\n     Q: [t, d]  K: [t, d]  V: [t, d]\n    Shapes of outputs are\n     Y: [t, d]\n    \"\"\"\n    t, d = Q.shape\n    Y_list = []\n    for i in range(t):           # loop cost: O(t^2 d)\n        Y_i = zeros(d)\n        Q_i = Q[i]\n        for j in range(i):       # loop cost: O(id)\n            A_ij = inner(K[j], Q_i)  # cost: O(d)\n            Y_i += A_ij * V[j]   # cost: O(d)\n        Y_list.append(Y_i)\n    return stack(Y_list)\nAnyone who has worked with self-attention will recognize that this is a terrible implementation, since nested for-loops are not GPU-friendly. Don’t worry: in the next section, we will discuss implementations that parallelize computation, and thus run much faster on modern hardware. For now, the main point of this pseudocode is to highlight that first mode of computation, which we call attention formulation, has a FLOP cost of \\(O(t^2 d)\\).\nThe key to the massive speedups we saw in the introduction comes from leveraging linearity to restructure the computation. Consider the following factorization: \\[\nY_i = \\sum_{j=1}^i Q^T_i K_j V_j = \\underbrace{ \\left (  \\sum_{j=1}^i V_j  K_j^T\\right )}_{S_i} \\; \\; Q_i\n\\] Written in this form, we notice that the term labeled \\(S_i \\in \\R^{d\\times d}\\) can be thought of as a state summarizing all the relevant information up to time \\(i\\). It’s easy to rewrite into the following recurrent equations \\[\nY_{i} = S_i Q_i\n\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\nS_i = S_{i-1} + V_i K_i^T\n\\] where we assume \\(S_{0} = 0\\in \\R^{d\\times d}\\). Written in this form, we realize that a linear transformer is an RNN. We can also write pseudocode for this approach, which we call the state formulation, and analyze the cost:\ndef LT_state(Q, K, V):\n    \"\"\"\n    Shapes of inputs are\n     Q: [t, d]  K: [t, d]  V: [t, d]\n    Shapes of outputs are\n     Y: [t, d]\n    \"\"\"\n    t, d = Q.shape\n    S_i = zeros(d, d) # shape [d,d]\n    Y_list = []\n    for i in range(t):        # loop cost: O(t d^2)\n        S_i += outer(K[i], V[i]) # cost: O(d^2)\n        Y_i = S_i @ Q[i]      # cost: O(d^2)\n        Y_list.append(Y_i)\n    return stack(Y_list)\nWe see that the cost here is \\(O(t d^2)\\).\nSo, while a standard transformer layer always has cost \\(O(t^2 d)\\), linear transformers have two formulations with different costs. By switching from the attention formulation to the state formulation, we can change the cost from \\(O(t^2 d)\\) to \\(O(t d^2)\\), trading a \\(t\\) term for a \\(d\\) term."
  },
  {
    "objectID": "articles/linear-transformers-are-faster/index.html#parallel-implementations",
    "href": "articles/linear-transformers-are-faster/index.html#parallel-implementations",
    "title": "Linear Transformers Are Faster",
    "section": "2. Parallel Implementations",
    "text": "2. Parallel Implementations\nIn general, for-loops are a terrible way to implement anything that will run on a GPU. When using high-level frameworks like PyTorch or JAX, the easiest way to get high GPU utilization is to only use primitives that are already highly optimized for GPU: matrix multiplication, elementwise operations, etc. We can rewrite our attention and state algorithms in this style to make them efficient.\nFirst, let’s do this for attention. Our main technique is to compute the attention matrix \\(A\\), which contains all the terms outer(Q[i], K[j]) that appeared inside the for-loops of LT_attention, using a single heavyweight matrix multiply.\ndef LT_attention_parallel_no_flash(Q, K, V):\n    \"\"\"\n    Shapes of inputs are\n     Q: [t, d]  K: [t, d]  V: [t, d]\n    Shapes of outputs are\n     Y: [t, d]\n    \"\"\"\n    t = Q.shape[0]\n    M = causal_mask(t)\n    A_raw = Q @ K.T  # cost O(t^2 d)\n    A = A_raw * M    # cost O(t^2)\n    Y = A @ V        # cost O(t^2 d)\n    return Y\nThis implementation lies at the core of nearly every transformer of the past five years, powering all of the AI models that have recently exploded in popularity. Since it is composed of such a small number of ultra-parallelizable ops, it obtains high GPU utilization. Recently, specialized flash attention kernels [3] have been used to get even further speedups by avoiding explicitly storing the attention matrix \\(A\\), and thereby saving both memory and time spent on memory-transfers. Algorithmically, though, flash attention is a variant of parallel attention, and has the same computational cost (as measured in FLOPs). We use LT_attention_parallel to refer to the flash attention implementation.\nNext, let’s parallelize the recurrent formulation. This optimization is less well-known. The key is to compute all the terms \\(V_i K^T_i\\) in parallel, and then use a cumulative-sum, which can be parallelized, to combine them.\ndef LT_state_parallel(Q, K, V):\n    \"\"\"\n    Shapes of inputs are\n     Q: [t, d]  K: [t, d]  V: [t, d]\n    Shapes of outputs are\n     Y: [t, d]\n    \"\"\"\n    P = V[:,:,None] @ K[:,None,:]  # cost: O(t d^2)\n    S = cumsum(P, axis=0)          # cost: O(t d^2)\n    Y = S @ Q[:,:,None]            # cost: O(t d^2)\n    return Y[:,:0]\nThe cost in FLOPs of this algorithm is \\(O(t d^2)\\).\nNow that we have four mathematically-equivalent implementations of a linear transformer, let’s time the training step of our GPT2 models and see how big of a difference it makes. For our LT_attention_parallel implementation, we use a custom linear self-attention flash kernel we implemented in Triton [13] based on OpenAI’s FlashAttention2 implementation.\n\n\n\n\nHere are some takeaways:\n\nAs expected, the attention variants all have a quadratic asymptotic cost (slope of 2 on a log-log plot3). The state variants all have linear asymptotic cost (slope 1). 4\nLT_state_parallel is an order-of-magnitude faster than LT_state.\nLT_attention_parallel_no_flash is two orders-of-magnitude faster than LT_attention.\nLT_attention_parallel seems to asymptotically stabilize into being an order-of-magnitude faster than LT_attention_parallel_no_flash.\nFor the majority of settings, LT_attention_parallel is the fastest. (This is the linear version of the algorithm used by the standard transformer.)\nParallel attention is the fastest algorithm for small context sizes. However, LT_state_parallel overcomes LT_attention_parallel_no_flash at around 13k context size, and overcomes LT_attention_parallel at around 100k.\n\nOverall, these results paint a clear picture: use the attention algorithm for small contexts and the state algorithm for large contexts. But do we really face a binary choice? Can we combine the state and attention ideas and get the best of both words?"
  },
  {
    "objectID": "articles/linear-transformers-are-faster/index.html#chunked-formulation",
    "href": "articles/linear-transformers-are-faster/index.html#chunked-formulation",
    "title": "Linear Transformers Are Faster",
    "section": "3. Chunked Formulation",
    "text": "3. Chunked Formulation\nIt’s evident that, for small context sizes, computing the \\(t\\) by \\(t\\) attention matrix is much more efficient than computing many \\(d\\) by \\(d\\) state matrices. But as \\(t\\) grows, there is a point where the quadratic cost of the attention matrix ends up dominating. Noticing that attention is extremely efficient for small \\(t\\) and that states are necessary for large \\(t\\) motivates doing one last reworking of the LT equation.\nLet \\(c \\in \\N\\) be a positive integer that we’ll call the chunk size. For any \\(i\\in \\N\\) find the unique \\(n\\in \\Z\\) s.t. \\(cn &lt; i \\le c(n+1)\\). We can easily see that the following equations are equivalent to the previous ones. \\[\nY_{i} = S_{cn}Q_i + \\sum_{j=cn+1}^i Q_i^T K_j V_j\n\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\nS_{c(n+1)} = S_{cn} + \\sum_{j=cn+1}^{c(n+1)} V_j K_j^T\n\\] The key idea is that we are only going to compute a subset of all states: \\(S_0, S_c, S_{2c}, \\cdots\\). Then, to compute each output \\(Y_i\\), we need only to take into account the contribution via the most recent state \\(S_{cn}\\), as well as the contribution (computed via attention) of all moments in time \\(j\\) in the range \\(cn &lt; j \\le i\\).\nAs pseudocode, this looks like:\ndef LT_attention_with_initial_state(S, Q, K, V):\n    \"\"\"\n    Shapes of inputs are\n     S: [d, d]  Q: [c, d]  K: [c, d]  V: [c, d]\n    Shapes of outputs are\n     Y: [c, d]\n    \"\"\"\n    Y_state = Q @ S                               # cost O(c d^2)\n    Y_attention = LT_attention_parallel(Q, K, V)  # cost O(c^2 d)\n    Y = Y_state + Y_attention                     # cost O(cd)\n    return Y\n\ndef LT_chunked(Q, K, V, c):\n    \"\"\"\n    Shapes of inputs are\n     Q: [t, d]  K: [t, d]  V: [t, d], c: int\n    Shapes of outputs are\n     Y: [t, d]\n    \"\"\"\n    t, d = Q.shape\n    assert t % c == 0\n    Q_, K_, V_ = [arr.reshape(t//c, c, d)\n    `               for arr in [Q,K,V]]\n    P_ = K_.transpose([0,2,1]) @ V_  # cost O(t d^2)\n    S_ = cumsum(P_, axis=0) - P_     # cost O((t/c)d^2)\n    Y_ = vmap(LT_attention_with_initial_state, axis=0)(\n                S_, Q_, K_, V_)      # cost O(td^2 + tcd)\n    return Y_.reshape(t, d)\nThe cost is \\(O\\left(td^2 + tcd\\right)\\), once again avoiding a quadratic dependency on \\(t\\). Also, note that this algorithm makes an inner call to LT_attention_parallel, so we can use a flash-attention kernel to do that part of the computation.\nThis algorithm has a hyperparameter, the chunk size, which must be set correctly for optimal performance. Fortunately, it is inexpensive to identify the best chunk size empirically, since measuring just a few steps of training at each chunk size is sufficient to identify which is the fastest. In the plot below, we plot the speed of the optimally-chunked algorithm in each setting.\n\n\n\n\nWe see LT_chunked gives the desired best-of-both-worlds behavior, as it is equal-or-better than all other approaches in all settings. And we now see the massive (& rapidly growing) speedup relative to standard self-attention, finally unlocking the true power of linear transformers."
  },
  {
    "objectID": "articles/linear-transformers-are-faster/index.html#sampling",
    "href": "articles/linear-transformers-are-faster/index.html#sampling",
    "title": "Linear Transformers Are Faster",
    "section": "4. Sampling",
    "text": "4. Sampling\nWhen working with language models, efficient training is not the only performance that deserves consideration. Once the model is trained, how expensive is it to utilize? When we are sampling we have a sequence of tokens, \\(z_1 \\cdots z_t\\), and we want to sample the next token, \\(z_{t+1}\\).\nThe most efficient algorithm to sample from traditional transformers is called the KV-cache algorithm [14]. This algorithm assumes that when we generate token \\(z_{t+1}\\), we will have already computed and cached all the \\(K_i, V_i\\) for all \\(0 \\le i \\le t\\). In order to compute the output of the attention layer at time \\(t+1\\) given this cached information, we can use \\[\nY_{t+1}^\\text{Transformer} = \\sum_{j=1}^{t+1} e^{Q^T_i K_j} V_j\n\\] It is easy to see that this is an \\(O(td)\\) operation. In other words, as the sequence length grows, sampling each subsequent token becomes more computationally expensive.5 This is one of the major limitations of the classic transformer architecture.\nWith linear transformers, however, we have access to the recurrent formulation. A linear transformer is an RNN where the state has size \\(O(d^2)\\). \\[\nY_{i} = S_i Q_i\n\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\nS_i = S_{i-1} + V_i K_i^T\n\\] We can compare the time it takes to generate any particular token when sampling a sequence:\n\n\n\n\nAs expected, we see that the time to sample of the linear transformer is independent of context length, whereas that of the KV-cache transformer grows linearly. This leads to a large gap in inference speed at large contexts.6"
  },
  {
    "objectID": "articles/linear-transformers-are-faster/index.html#learning-performance",
    "href": "articles/linear-transformers-are-faster/index.html#learning-performance",
    "title": "Linear Transformers Are Faster",
    "section": "5. Learning Performance",
    "text": "5. Learning Performance\nUntil now, our focus has been on how to implement linear transformers efficiently. But an architecture that runs really fast is useless unless it also learns well. We will now compare the learning curves of the GPT2 baselines with their linear transformer counterparts, at various context sizes.\nIn order to control for the fact that longer contexts help learning by introducing more tokens per update, we hold the number of tokens-per-update constant across context sizes by decreasing batch size. At all context-lengths, each update sees \\(2^{19}\\) tokens.7 Importantly, for this set of experiments, we have used the dataset c4 [4], which does not have much long-term structure: it consists of a shuffled collection of short articles, of average length ~500 tokens.\nFirst, let’s look at the parameter scaling law of GPT2 and our modified Linear-GPT2. The following plot shows the final performance after 24h of training on our 8xH100 server for each scale and each context size. Click the second tab if you want to see the full learning curves.\n\n\n\n\nBoth architectures scale similarly with respect to model size, but there is a gap in performance. This gap seems to grow as context size increases, together with more loss spikes and general instability. It’s possible that the gap can be explained by the linear transformer not effectively utilizing its context. To explore whether or not this is the case, we can use these same experiments, but visualize all context-lengths together.\n\n\n\n\nWe see a dramatically different trend between the two architectures. For GPT2, each increase in context length slows down the initial pace of learning, but ultimately all context-lengths (beyond at least 1024) converge to a similar final loss. Whereas for Linear-GPT2, not only does increasing the context slow down learning in the beginning, but it also causes convergence to a worse final performance and nasty-looking loss spikes.\nThe results for GPT2 are what we would expect from a healthy model, given the setting. Note that since our dataset consists of short articles, of average length ~500 tokens, we can assume that even a context window of 1024 would include nearly everything relevant. Thus, we wouldn’t expect that increasing the context length beyond this point would decrease the loss the model ultimately converges to. Longer-context models do however need to learn to ignore many more irrelevant tokens, explaining the slowed initial learning.8\nIn contrast, the results for Linear-GPT2 seem to indicate some underlying instability of the linear transformer architecture. It doesn’t even seem capable of ignoring irrelevant information, let alone exploiting useful long-term structure.\nRemedying these learning issues will be the main focus of our future work in linear transformers. Our current architecture is essentially a one-to-one clone of GPT2, sticking as architecturally close to the original as possible; aspects such as initializations, normalizations and hyperparameters were directly copied. It is well-known that these decisions can have a large impact on scaling and stability and often need to be tuned in an architecture-specific way. In the literature, various other linear variants of self-attention have been proposed, that incorporate techniques such as gating mechanisms in order to improve stability and learning [5]–[11]. A future post will include a thorough study of the impact of all of these choices.\nUltimately, it may be impossible for any linear transformer to perfectly match the context scaling laws of the classic transformer baseline. Although removing the exponential seems like a minor change, it represents a meaningful decrease in expressivity.9 But as we’ve shown, linear transformers can be trained orders-of-magnitude more efficiently. On equivalent compute, linear transformers can be trained for many more steps; or, keeping steps constant as well, we can train a much larger model. If this additional scaling is sufficient to compensate for the reduced expressivity, linear transformers will be the more efficient architecture overall.\n\n  \n  Subscribe to be notified of new posts:"
  },
  {
    "objectID": "articles/linear-transformers-are-faster/index.html#footnotes",
    "href": "articles/linear-transformers-are-faster/index.html#footnotes",
    "title": "Linear Transformers Are Faster",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs discussed in Section 4, a second benefit of linear transformers is that the cost to sample a token does not grow with context size. Perhaps one could argue that this improvement in sampling speed could, on its own, justify using linear transformers for applications where the inference costs vastly exceed training costs. But it is evident to us that, for linear transformers to become actually useful, we need to address these instability issues.↩︎\nIt is not named for the fact that the computational cost is linear with respect to \\(t\\)! That is just a coincidence. (And is not even always true, as we will see.)↩︎\nIf \\(y=x^2\\), a log-log plot where \\(y'=\\log_a(y)\\) and \\(x'=\\log_a(x)\\) for any base \\(a\\), then \\(y'=\\log_a(y) = \\log_a(x^2) = 2 \\log_a(x) = 2 x'\\). So the graph will be a line with slope 2.↩︎\nThe reason we see the expected slopes asymptotically is that we are timing a full GPT2 architecture which has many other components besideds the attention layer. If we were only timing the attention layer, the plots would all be straight lines.↩︎\nAn interesting connection is that the KV-cache can be understood as the state of an RNN with non-constant state size; namely, one whose state-size is \\(O(td)\\).↩︎\nThis comparison may not be completely fair. In these experiments, our implementation of neither sampling algorithm makes use of specialized kernels. A lot of the ideas of flash attention can be used to write a much faster KV cache sampling algorithm; on the other hand, it’s unclear if much improvement is possible on the recurrent sampling. Thus, it’s possible that with engineering effort the gap between the two algorithms could become smaller. However, the overall pattern will certainly remain the same.↩︎\ne.g. runs with context-size 1024 would have batch-size of \\(2^{19} / 2^{10} = 2^{9} = 512\\).↩︎\nPut another way: doubling the size of the input vastly increases the size of the function space over which gradient descent must search, and it’s intuitive that in a larger space it takes somewhat longer to find a good solution.↩︎\nWe plan to elaborate on this topic in a future blog post.↩︎"
  },
  {
    "objectID": "articles/mission/index.html",
    "href": "articles/mission/index.html",
    "title": "Our Mission",
    "section": "",
    "text": "A poem is a shadow of the act of writing poetry.\nHumanity casts many shadows. All literature, letters, recipes, and tweets. Mathematical proofs and git repositories. Laws, treaties, and declarations of war. Financial statements, employee performance reports, and bankruptcy filings. Podcasts, operas, accidental voicemails, YouTube videos and Hollywood blockbusters. Restaurant reviews and love letters and times tables. Individually, each of these pieces of information is nothing but the faintest shadow of the process that produced it. But collectively, these shadows tell a rich story about the world.\nSince the dawn of humanity, the brain alone could reconstruct the world from these shadows. But the last decade of deep learning has convinced us that this won’t be the case for much longer. Though significant challenges remain, we stand poised to solve them. If successful, it will become possible to synthesize every documentable aspect of humanity into the weights of a neural network.\nOur mission is to train a neural network to model all human output.\nThere are two primary challenges in our pursuit of this goal.\n\nCurrently, it’s not technically feasible to train a model that can ingest all the data that we can collect. Limitations around context length, modality, and throughput force us to use only a small subset of the data available to us.\nMuch of the data we will need has never been collected, curated, and organized into datasets that we can use for training.\n\nWe are building a world-class team of engineers and researchers to tackle these challenges, united around shared principles and a specific research agenda. We value clear thinking, sharing knowledge, and an extreme commitment to scientific honesty. Our research is guided by mathematical beauty and grounded in rigorous empiricism. We are committed to letting the quality of our work speak for itself. No hype, no fluff. All meat.\nIf this vision resonates, please reach out:          777b7a607577605479757a7d72716760757d3a777b79\n\n\n  \n  Or, subscribe to be notified of new posts:"
  },
  {
    "objectID": "articles/gradient-flows/index.html",
    "href": "articles/gradient-flows/index.html",
    "title": "Why Gradient Descent Minimizes the Loss",
    "section": "",
    "text": "\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\der}{\\partial}\n\\newcommand{\\dldt}{\\frac{\\der l}{\\der t}}\n\\newcommand{\\len}{\\text{length}}\n\\newcommand{\\en}{\\text{energy}}\n\\newcommand{\\dim}{\\text{dim}}\n\\newcommand{\\tr}{\\text{trace}}\n\\newcommand{\\lin}{\\text{Lin}}\n\\newcommand{\\rnk}{\\text{rank}}\n\\newcommand{\\ht}{\\widehat}\n\\newcommand{\\dwdt}{\\frac{\\der w}{\\der t}}\n\\newcommand{\\l}{\\mathscr{l}}\n\\newcommand{\\E}{\\mathbb E}\n\\]\nIt’s commonly stated that neural networks are difficult to study because they are not convex, making it hard to say much about the behavior of gradient descent. But for the past few months I’ve been looking at ways to prove convergence guarantees to low loss without the assumption of convexity. Surprisingly, just with elementary concepts of calculus, it is possible to study simple neural networks like a two-layer MLP and show that learning will minimize the training loss.\nThis article follos a very geometrical perspective. Here we think of the parameters of the neural network as a vector in a vector space, and the process of learning as a continuous curve through that space. This geometric perspective emerges out of taking the limit where the learning rate goes to \\(0\\). When the parameterers follow a differentiable path through time, we can invoke concepts like the energy of and length the path, which turn out to have profound interpretations within the context of a learning problem.\nNaturally, to show that the training loss is minimized, we need a “repalcement for convexity”. An assumption that provides similar guarantees, but which neural networks actually satisfy. What this alternative should be is perhaps the most important idea in the article. It can be summarized as: “the gradient must be large when the loss is large”. With some caveats, this property seems to hold for the neural networks that are used in practice and thus, we can guarantee that learning will minimize the loss.\nThis article is stil work in progress. Apologies for any writing or math errors. The proofs are semi-rigorous at best, and a lot important questions remain unnanswered. I plan to keep workig and updating the article to generalize the results to architectures other than 2 layer MLPs. Also, I’m uncertain about the novelty of these ideas. Please let me know if you are aware of related work."
  },
  {
    "objectID": "articles/gradient-flows/index.html#notation-and-concepts",
    "href": "articles/gradient-flows/index.html#notation-and-concepts",
    "title": "Why Gradient Descent Minimizes the Loss",
    "section": "0) Notation and Concepts",
    "text": "0) Notation and Concepts\nMuch of the theory on this article is thinks of the space of parameters as a vector space equipped with an inner product. The parameters of a neural network tend to either be elements of \\(\\R^{n}\\) or matrices in \\(\\R^{n\\times m}\\). So we need to define suitable inner products for these two types of vectors.\nDefinition 0.1: Inner Product For two vectors \\(x,y \\in \\R^n\\) we take the inner product to be \\(&lt;x, y&gt; = x^T y\\). For two matrices \\(M,N\\in \\R^{n\\times m}\\) we use \\(&lt;M, N&gt; = \\tr(M^T N)\\). Remember that any inner product induces a norm \\(|\\cdot|\\) via \\(|v| = \\sqrt{&lt;v,v&gt;}\\). And you can verify that, in the \\(\\R^n\\) and \\(M,N\\in \\R^{n\\times m}\\) cases, this induced norm takes the following forms: \\[\\begin{align}\n|x| = \\sqrt{\\sum_i x_i^2} \\quad\\quad |M| = \\sqrt{\\sum_{ij} M_{ij}^2}\n\\end{align}\\]\nWhich are also known as the L2 norms of the respective vector spaces. \\(\\blacksquare\\)\nDefinition 0.2: Derivative Let \\(f: V \\to W\\) be a function between two vector spaces. The derivative at a point \\(v\\in V\\) is a linear function \\(\\der f(v): V\\to W\\). It tells us how changing the input \\(v \\to v + u\\) will affect the output (if the change \\(u\\in V\\) is small). Concretely, \\[\n\\der f(v)(u) \\simeq f(v + u) - f(v)\n\\]\nSome people would call \\(\\der f(v)(u)\\) the directional derivative of \\(f\\) along \\(u\\) at a point \\(v\\). \\(\\blacksquare\\)\nDefinition 0.3: Path Derivative A specially important case is when we are taking the derivative of a funciton \\(h:\\R \\to V\\) (a path through \\(V\\)). Here, using the notation \\(\\der h(x): \\R \\to V\\) is a little bit heavy handed. Any linear map \\(M: \\R \\to V\\) can instead be represented by the vector \\(v = M(1)\\) together with scalar vector multiplication. Just see that \\(M(r) = r \\; v\\) for all \\(r\\in \\R\\). Effectively, Newton’s notation \\(h'\\) applies this idea to path derivatives by defining \\(h'(t) = \\der h(t)(1)\\). \\(\\blacksquare\\)\nOne important use of the derivative of a path is to define the length and energy of the path.\nDefinition 0.4: length and energy of a path Given a normed vector space \\(V\\) and a differentiable path \\(h: [0, t] \\to V\\), the length and energy of the path are defined as \\[\\begin{align}\n\\len(h) &= \\int_0^t |h'(s)| ds \\\\\n\\en(h) &= \\int_0^t |h'(s)|^2 ds\n\\end{align}\\]\n\\(\\blacksquare\\)\nThe length and energy are related by the following inequality, which will be key in proving the central results of the article.\nResult 0.5: If \\(h: [0, t] \\to W\\) is a differentiable path, then \\[\n\\len(h)^2 \\le t\\;  \\en(h)\n\\]\nProof Just note that \\[\\begin{align}\n\\len(h) &=  \\int_0^t |h'(s)| ds  \\\\\n&\\le  \\sqrt{\\int_0^t  1^2 ds } \\sqrt{\\int_0^t | h'(s)|^2 ds } \\;\\;\\;\\;\\;\\; \\text{(by Cauchy Schwarz)} \\\\\n&\\le\\sqrt{ t \\; \\en(h)} \\\\\n\\end{align}\\]\nAnd, since \\(\\len(h)\\) is positive and squaring positive numbers is a monotone function, the result follows. \\(\\blacksquare\\)\nDefinition 0.6: Gradient The gradient of a differentiable function \\(f: V \\to \\R\\), denoted \\(\\nabla f\\), is a map \\(\\nabla f: V\\to V\\) defined so that \\(\\nabla f(v)\\in V\\) is the unique vector satisfying \\[\n\\der f(v)(u) = &lt;\\nabla f(v), u&gt;\n\\]\nfor all \\(u\\in V\\). \\(\\blacksquare\\)\nNote: You might be used to a different definition of the gradient. The one you know turns out to be equivalent to the one I’m using here. If you find this confusing you should take a look at this great article.\nSometimes it is completely clearn from the context which funciton \\(f\\) we are taking the gradient of. In those cases I sometimes use the shorthand \\(\\hat v = \\nabla f(v)\\)."
  },
  {
    "objectID": "articles/gradient-flows/index.html#learning-with-gradient-flows",
    "href": "articles/gradient-flows/index.html#learning-with-gradient-flows",
    "title": "Why Gradient Descent Minimizes the Loss",
    "section": "1) Learning with Gradient Flows",
    "text": "1) Learning with Gradient Flows\nDefinition 1.1: Learning Problem: The learning problem setup consists of a tuple \\((W, w_0, f)\\) where:\n\n\\(W\\) is a vector spcae equiped with an inner product \\(&lt;\\cdot, \\cdot&gt;\\). You can think of \\(W\\) as the parameter space which controls the behavior of the model.\n\\(w_0\\in W\\) is the initial point from which the learning will proceed.\n\\(f: W \\to \\R^+\\) is the loss function that tells us how good the parameters are (presumably at fitting some dataset). A loss function must be lower bounded so it’s nice to assume wlog that \\(\\inf_{w} f(w) = 0\\).\n\nOut of \\(f, W\\) and \\(w_0\\), the following objects emerge:\n\nA path \\(\\gamma: [0, \\infty) \\to W\\) given by the learning algorithm. In this document, we take \\(\\gamma\\) to be a gradient flow (defined below) of the loss funciton \\(f\\).\nA loss curve \\(l:\\R \\to \\R\\) defined as \\(l(t) = f\\circ \\gamma(t)\\).\n\\(L = l(0) = f(w_0)\\). Since \\(\\inf_{w} f(w) = 0\\) we can think of \\(L\\) as the amount of “work” that the learning algorithm will need to do to solve the problem.\n\n\\(\\blacksquare\\)\nThe most fundamtenal learning algorithm is gradinet descent. It starts starts with the initial parameters \\(w_0\\in W\\) and slowly moves them in a direction that minimizes the loss by applying the update rule \\(w_{t+1} = w_t - \\delta \\nabla f(w_t)\\) for some learning rate hyperparameter \\(\\delta \\in \\R^+\\). We would like to understand why this algorithm works so well when training neural networks, but the discreteness of the steps complicates the analysis. It will be very useful to study the learning behaviour in the limit \\(\\delta \\to 0\\). In this case the weights follow a continuous curve \\(\\gamma:\\R \\to W\\) which is called the gradient flow of \\(\\nabla f\\).\nDefinition 1.2: Gradient Flow: Let \\(W\\) be a vector space with an inner product. Given a differentiable, lower bounded function \\(f: W\\to \\R\\). The gradient flow starting at \\(w_0\\in W\\) is defined as the path \\(\\gamma: [0, \\infty) \\to V\\) satisfying \\[\n\\gamma(0) = w_0  \\quad\\text{and}\\quad \\gamma'(t) = - \\nabla f\\circ \\gamma(t)\n\\]\nThe existance of such a path follows from the existance and uniqueness of PDEs. TODO: Explain why the flow \\(\\gamma\\) doesn’t “diverge to infinity”, that the flow isn’t just a curve \\(\\gamma:[0, \\epsilon) \\to W\\), but a map with domain \\([0, \\infty)\\). This uses the assumption that \\(f\\) is lower bouded. \\(\\blacksquare\\)\nIt is evident that gradient descent and gradient flows are deeply connected, but the distinction between them deserves careful consideration. For any concrete learning problem \\((W, w_0, f)\\), if we succeed in showing that the gradient flow will minimize the loss, then we will also know that gradient descent with a small enough learning rate is bound to also minimize the loss. But understnading how small the learning rate needs to be is of profound importance. It will determine the computational cost of solving the problem.\nAnother very important thing that this document currently omits are the ideas of stochastic gradient descnet. Modern deep learning is always based on computing gradients on a small subset of the dataset. Using these gradients results in slighly worse updates, but much cheaper ones. A tradeoff worth making because, for the same amount of compute, it allows us to make many more updates, resulting in more learning overall.\nUltimately, the questions that really matter are all centered around the computational cost of solving a learning problem. And studying the behavior of gradient flows will always fall short of that obvjective. Yet, I believe that studying gradient flows will prove to be a useful intermediate step. The rest of this section goes over some of the most important mathematical properties of gradient flows that make them so well suited to study learning.\nResult 1.3 The derivative of the loss curve is the magnitude of the gradient: \\[l'(t) = - | \\nabla f\\circ \\gamma(t)| ^2\\]\nProof \\[\\begin{align}\nl'(t) &= \\der f(w)( \\small\\dwdt)\n\\quad &\\text{(chain rule)} \\\\\n&= &lt; \\nabla f(w), \\small\\dwdt&gt;\n\\quad &\\text{(gradient def.)} \\\\\n&= -&lt; \\nabla f(w), \\nabla f(w)&gt;\n\\quad &\\text{(grad flow def.)} \\\\\n&= - | \\nabla f\\circ \\gamma(t)| ^2\n\\end{align}\\]\n\\(\\blacksquare\\)\nSo the magnitude of the gradient is the rate of change of the loss. Large gradient means fast learning. A consequence of this result is that \\(\\en(\\gamma)\\) measures how much the path \\(\\gamma\\) has managed to reduced the loss.\nResult 1.4: If \\(\\gamma:[0, t] \\to W\\) is a gradient flow of \\(f:W \\to \\R\\), then:\n\\[\n\\en(\\gamma) = L - l(t)\n\\]\nAlso, the energy is bounded by the initial loss: \\(\\en(\\gamma) \\le L\\).\nProof \\[\\begin{align}\n\\en(\\gamma) &= \\int_0^t |\\gamma'(s)|^2 ds \\\\\n&= \\int_0^t |\\nabla f \\circ \\gamma(s)|^2 ds\n= \\int_0^t  l'(s) ds \\\\\n&= l(0) - l(t)\n\\end{align}\\]\nThe last step is to notice that \\(l(t)\\ge 0\\) and \\(l(0)=L\\). \\(\\blacksquare\\)\nThis is a very interesting shift in perspective. We can reformulate questions about the loss curve \\(l: \\R\\to\\R^+\\) into questions about \\(\\en(\\gamma)\\). But it also has another very important application. To prove our results about neural networks, we will need to ensure that they satisfy that nice condition of having large gradient on points with high loss. We will see that, at initializatio \\(w_0\\), the networks do indeed satisfy this property. But that is not the case for all \\(w\\in W\\), only for \\(w_0\\) and points nearby. So we’ll need a way to guarantee that \\(\\gamma\\) won’t move the parameters too far away from \\(w_0\\), at least not too quickly. The following result does just that.\nResult 1.5 If \\(f\\) is a loss funciton, \\(w_0\\) the initial parameters, \\(L=f(w_0)\\) and \\(\\gamma\\) a gradient flow of \\(f\\) starting at \\(w_0\\), then \\[\n|\\gamma(t) - w_0| \\le \\sqrt{t\\;L}\n\\]\nProof \\[\\begin{align}\n|\\gamma(t) - \\gamma(0)|\n&= \\left |\\int_0^t  \\gamma'(s) ds \\right| \\\\\n&\\le \\int_0^t  \\left | \\gamma'(s) \\right| ds \\quad &\\text{(result A.2)} \\\\\n&= \\len(\\gamma) \\\\\n&\\le \\sqrt{t \\; \\en(\\gamma)} \\quad &\\text{(result 0.5)}\n\\end{align}\\]\nThe last step is using result 1.4, which tells us that \\(\\en(\\gamma) \\le L\\). \\(\\blacksquare\\)"
  },
  {
    "objectID": "articles/gradient-flows/index.html#a-replacement-for-convexity",
    "href": "articles/gradient-flows/index.html#a-replacement-for-convexity",
    "title": "Why Gradient Descent Minimizes the Loss",
    "section": "2) A Replacement for Convexity",
    "text": "2) A Replacement for Convexity\nWe are trying to show that neural networks trained via gradient descent will (approximately) reach 0 loss. One way to get such type of guarantees is to assume the loss funciton \\(f\\) is convex. But that is simply not the case for NNs. We need to find a replacement for convexity that is applicable to NNs. I believe this property is that the gradient \\(\\nabla f(w)\\) must be large at all points \\(w\\) where the loss \\(f(w)\\) is large. Concretely, for some \\(\\alpha \\in \\R^+\\) we need that\n\\[\n\\frac{|\\nabla f(w)|^2}{f(w)}  \\ge \\alpha\n\\]\nThis is another way to guarantee no local minima exist. Think about it. The definition of a local minima is a point \\(w\\) with no gradient but high loss, the existance of which is ruled out by the condition.\nBut the following result tells us something much stronger. If the property holds, the loss is guaranteed to decay to \\(0\\) exponentially fast with time.\nResult 2.1 If \\(f: W\\to \\R^+\\) satisfyies \\(\\frac{|\\nabla f(w)|^2}{f(w)}  \\ge \\alpha\\) for some \\(\\alpha \\in \\R^+\\), then \\[\nl(t) \\le L \\; e^{-\\alpha t}\n\\]\nProof \\[\\begin{align}\n\\frac{\\der \\ln l(t)}{\\der t} &= \\frac{l'(t)}{l(t)}\n= -\\frac{|\\nabla f \\circ \\gamma(t)|^2}{f\\circ \\gamma(t)}\n\\le  -\\alpha \\\\\n\\end{align}\\]\nso \\[\n\\ln l(t) - \\ln l(0) = \\int_0^t \\frac{\\der \\ln l(s)}{\\der s}  ds \\le - \\int_0^t \\alpha ds = - \\alpha t\n\\] And then \\(\\ln l(t)  \\le  \\ln L - \\alpha t\\). To conclude just use the fact that exponential is a monotone function and \\[\nl(t) = e^{\\ln l(t)} \\le  e^{\\ln L - \\alpha t} = L e^{ - \\alpha t}\n\\]\nas desired. \\(\\blacksquare\\)\nThe next question we should ask is obvious: Do Neural Networks Satisfy property 3.1? The short answer is No! To see that, just consider an MLP with “degenerate” parameters where all the weight matrices are \\(0\\). The MLP will have 0 gradient even when it has high loss.\nBut the long answer is more interesting. Turns out that the standard initialization techniques of NNs guarantee that, at initialization, the property hods for some large \\(\\alpha\\). And even better, the property remains true for all points near \\(w_0\\). We will look into the particular guarantees for a 2 layer MLP in the next section, but for now just assume that we are looking at a learning problem satisfying\n\\[\n\\frac{|\\nabla f(w)|^2}{f(w)}  \\ge \\alpha \\quad\\forall w\\in W \\text{ s.t. } |w-w_0| \\le \\epsilon\n\\]\nThis is where the work from the last section is going to pay off. We learned that \\(|\\gamma(t) - w_0| \\le \\sqrt{t\\;L}\\) so the parameters cannot move very far in a short period of time. Then, as long as \\(\\sqrt{t L} \\le \\epsilon\\) we can be sure that the assumptions of result 3.1 hold. And so the bound\n\\[\nl(t) = L e^{ - \\alpha t}\n\\]\nWill be true for all \\(t \\le \\frac {\\epsilon^2} L\\). Thus, \\[\n\\l_\\infty \\le l(\\frac {\\epsilon^2} L) \\le L \\exp({-\\frac {\\alpha \\epsilon^2} L })\n\\]\nThis is the simplest way we can put together all the important ideas, but not the most elegant one. Instead of assuming that the gradient \\(\\frac{|\\nabla f(w)|^2}{f(w)}  \\ge \\alpha\\) for all \\(w\\) within a ball of \\(w_0\\), we will be able to prove for a 2 layer MLP that \\(\\frac{|\\nabla f(w)|^2}{f(w)}  \\ge \\alpha - \\beta \\; |w-w_0|\\) for all \\(w\\in W\\). This results in a more elegant bound.\nResult 2.2 If \\(f\\) and \\(w_0\\) satisfy \\(\\frac{|\\nabla f(w)|^2}{f(w)}  \\ge \\alpha - \\beta \\; |w-w_0|\\) for all \\(w\\in W\\), then \\[\nl(\\infty) \\le L \\; \\exp({-\\frac {\\alpha^3}{3 \\beta^2 L} })\n\\]\nProof See appendinx \\(\\blacksquare\\)\nNow it’s time to look at a simple neural network and show that the properties of result 3.2 hold."
  },
  {
    "objectID": "articles/gradient-flows/index.html#the-simplest-neural-network",
    "href": "articles/gradient-flows/index.html#the-simplest-neural-network",
    "title": "Why Gradient Descent Minimizes the Loss",
    "section": "3) The Simplest Neural Network",
    "text": "3) The Simplest Neural Network\nThe objective of this section is very simple. Take the absolute simplest neural network, and show that the conditions of result 2.2 are satisfied. We’ll look at a 2 layer MLP with ReLU nonlinearity. It has two parameter matrices \\(M\\in \\R^{k\\times m}\\) and \\(N\\in \\R^{n\\times k}\\). The ReLU is denoted by \\(\\sigma\\). Given an input \\(x \\in \\R^m\\) the MLP produces the output \\[\ny = N \\sigma(M x)\n\\]\nIt’s convinient to break down the computation into steps to define intermediate variables. First we apply a matrix to the input \\(a = Mx\\) then the nonlineary to get \\(b=\\sigma(a)\\) and finally we produce the output \\(y=Nb\\). The variables/activations \\(a,b,y\\) are dependant on the parameters \\(M,N\\) so, when we consider changing the parameters with time, these activations will change too.\nParameter space The weight vectors are pairs of matrices \\(w = (N, M)\\) and so the parameter space is \\[W= \\R^{n\\times k} \\oplus \\R^{k\\times m}\\]\nThe loss function Keeping in with the philosophy of studying the simplest example, we’ll take the loss function to be the L2 error on a single input-target pair \\((x, q)\\in \\R^n \\oplus \\R^m\\). \\[\nf(w) = \\frac 1 2 |y - q|^2\n\\]\nWe will need \\(x\\) to be normalized, so we just assume that \\(|x|^2 = m\\). It’s also useful to define the loss variable \\(\\l = f(w)\\).\nInitialization The standard way to initialize neural networks is to sample independently all the entries in the weight matrices form some distribution. In our case we use \\[\nM_{ij} \\sim \\mathcal N (0, {\\small \\frac 2 m} ) \\quad\\quad N_{ij} \\sim \\mathcal N (0, {\\small \\frac 2 k})\n\\]\nThis is the commonly used He init, which works well for networks with ReLU nonlinearities.\nLearning Guarantees Appendix B is dediacted to showing that this choice of architecture, loss function and initialization satisfies (with high probability):\n\\[\n\\frac{|\\nabla f(w)|^2}{f(w)} \\ge 2k - 4 \\sqrt {kn} |w_0 - w|\\\\\n\\]\nSo by setting \\(\\alpha = 2k\\) and \\(\\beta = 4 \\sqrt {kn}\\), the application of result 2.2 gives \\[\nl(\\infty) \\le L \\; \\text{exp}({-\\frac {k^2}{6 n L} })\n\\]\nFrom looking at the above equation, it is apparent that the scale of the MLP helps it learn. By growing \\(k\\) we can very quickly lower the loss to any acceptable value."
  },
  {
    "objectID": "articles/gradient-flows/index.html#appendix-a",
    "href": "articles/gradient-flows/index.html#appendix-a",
    "title": "Why Gradient Descent Minimizes the Loss",
    "section": "Appendix A:",
    "text": "Appendix A:\nDefinition A.1: Operator Norm If \\(M: V \\to W\\) is a linear map between two vector spaces equipped with inner products, the the operator norm \\(|| \\cdot ||\\) is defined as \\[\n||M|| = \\max_{v\\in V} \\frac{|Mv|}{|v|}\n\\]\nWhere the numerator uses the norm of \\(W\\) and the denominator the one of \\(V\\). When the two norms on \\(V\\) and \\(W\\) arise from inner products, then \\(||M||\\) is the largest singular value of \\(M\\). \\(\\blacksquare\\)\nResult A.2: triangle inequality for integrals If \\(V\\) is a normed vector space, \\(a,b\\in \\R\\) and \\(h: [a, b] \\to V\\) is a differentiable path, then the following inequality holds: \\[\n\\left|\\int_a^b h(s) ds \\right| \\le \\int_a^b |h(s)| \\: ds\n\\]\nProof The proof is a simple application of the definition of the integral (as the limit of a sum) combined with the triangle inequality of the inner product. \\(\\blacksquare\\)\nResult A.3 Let \\(A\\in \\R^{n\\times m}\\) and recall that \\(|\\cdot|\\) and \\(||\\cdot||\\) denote the Frobenious and Operator norms respectively. Then \\[||  A || \\le { \\sqrt {\\text{rank}(A) } } \\; |A|\\]\nProof TODO \\(\\blacksquare\\)\nResult A.4 If \\(\\gamma: \\R \\to \\R^{n\\times m}\\) is a differentiable path and the derivative satisfies \\(\\text{rank}( \\gamma'(t) ) \\le r\\) for all \\(t\\), then \\[\n||M - M_0||  \\le  \\sqrt r \\; \\len(\\gamma)\n\\]\nProof First we use the triangle inequality of the operator norm \\[||M - M_0|| = || \\int_0^t \\gamma'(s) ds || \\le \\int_0^t || \\gamma'(s) || ds\n\\]\nThen, result A.3 tells us that \\(||  A || \\le \\sqrt{\\rnk(A)} \\; |A|\\) and so \\[\n||M - M_0||\n\\le \\int_0^t \\sqrt r \\;| \\gamma'(s) | ds\n= \\sqrt r \\;\\len(\\gamma)\n\\]\n\\(\\blacksquare\\)\nResult 2.2 If \\(f\\) and \\(w_0\\) satisfy \\[\n\\frac{|\\nabla f(w)|^2}{f(w)}  \\ge \\alpha - \\beta \\; |w-w_0|\n\\] for all \\(w\\in W\\), then \\[\nl(\\infty) \\le L \\; \\exp({-\\frac {\\alpha^3}{3 \\beta^2 L} })\n\\]\nProof The proof follows a very similar argument to 3.1,\n\\[\\begin{align}\n\\frac{\\der \\ln l(t)}{\\der t} &= -\\frac{|\\nabla f(w)|^2}{f(w)} \\quad &\\text{(result 1.3)} \\\\\n&\\le - \\alpha + \\beta \\;  |w - w_0| \\\\\n&\\le - \\alpha + \\beta \\sqrt {L t} \\quad &\\text{(result 1.5)} \\\\\n\\end{align}\\]\nIntegrating both sides from \\(0\\) to \\(t\\) we get \\[\n\\ln l(t) - \\ln L \\le - \\alpha t + \\frac{2}{3}  \\beta \\sqrt L \\; t^{3/2}\n\\]\nwhich implies \\[\nl(t) \\le L \\exp(- \\alpha t + \\frac{2}{3}  \\beta \\sqrt L \\; t^{3/2})\n\\]\nNow solve for the value of \\(t\\) that minimizes the term in the expoential. By setting the derivative to \\(0\\) you can easily verity that it is \\(t^* = \\frac {\\alpha^2} {\\beta^2 L}\\). At this particular time, the bound is minimized. It tells us that: \\[\nl(t^*) \\le L \\exp({-\\frac{\\alpha^3}{3\\beta^2 L}})\n\\]\nSo \\(l(\\infty) \\le l(t^*) \\le L\\exp({-\\frac{\\alpha^3}{3\\beta^2 L}})\\) proving the result. \\(\\blacksquare\\)"
  },
  {
    "objectID": "articles/gradient-flows/index.html#appendix-b",
    "href": "articles/gradient-flows/index.html#appendix-b",
    "title": "Why Gradient Descent Minimizes the Loss",
    "section": "Appendix B:",
    "text": "Appendix B:\nAn analysis of the gradients, activations at initializations, and bounds on how much the activations and gradients can move for the 2 layer MLP descrived in section 3.\n\nGradients\nThe gradients of the loss \\(\\l\\) wrt all the intermediate variables and weights are: \\[\n\\ht y = y -q, \\quad\n\\ht b = N^T \\ht y, \\quad\n\\ht a =\\der \\sigma(a)^T \\ht b, \\quad\n\\ht M = \\ht a x^T, \\quad\n\\ht N = \\ht y b^T, \\quad\n\\]\nNote that \\(|\\hat y |^2 = 2 \\l\\) so, when the loss is large, the gradient of \\(\\l\\) wrt \\(y\\) is large too. Ultimately are trying to show something similar, but about gradients of \\(\\l\\) wrt \\(M\\) to then apply result 2.2.\nBelow I’ve writen a short derivation of all these gradient formulas, but it uses some unconventional notation and techniques. If you find them confusing, just work out the gradients in your own way and confirm you get the same answers.\nStarting with \\(\\ht y\\), the gradient of \\(\\l\\) wrt \\(y\\). Let \\(\\dot y \\in \\R^n\\) denote an arbitrary change to \\(y\\). Then \\[\\begin{align}\n&lt;\\ht y, \\dot y&gt; =\\frac {\\der \\l} {\\der y} (\\dot y)\n&\\simeq \\frac 1 2 |y +\\dot y - q|^2 - \\frac 1 2 |y - q|^2 \\\\\n&= \\frac 1 2 ( &lt;\\dot y, y-q&gt; + &lt;\\dot y, \\dot y&gt; + &lt;y-q, \\dot y&gt; ) \\\\\n&\\simeq &lt;y-q, \\dot y&gt; \\quad \\text{(dropping the lower order term)} \\\\\n\\end{align}\\]\nNow let’s see how a change in \\(b\\) affects \\(y\\). Like before, let \\(\\dot b\\) denote a change to \\(b\\). The derivative \\(\\frac {\\der y}{\\der b}(\\dot b) \\simeq N(b+ \\dot b) - M b = \\dot M b\\). So, the gradient of \\(\\l\\) wrt \\(b\\) satisfies \\[\n&lt;\\ht b, \\dot b&gt;\n= {\\frac {\\der \\l}{\\der b}(\\dot b)}\n= {\\frac {\\der \\l}{\\der y}} \\left(  {\\frac {\\der y}{\\der b}}(\\dot b) \\right)\n=  &lt;\\ht y,  {\\frac {\\der y}{\\der b}} (\\dot b)&gt;\n= &lt;\\hat y, M \\dot b&gt; = &lt;M^T \\ht y, \\dot b&gt;\n\\]\nNow let’s look at \\(N\\). The derivative \\(\\frac {\\der y}{\\der N}(\\dot N) \\simeq (N+\\dot N) b - N b = \\dot N b\\). And the gradient \\[\n&lt;\\ht N, \\dot N&gt;  = &lt;\\ht y, \\dot N b&gt; = &lt;\\ht y b^T, \\dot N&gt;\n\\]\nRecall that, since we are taking the inner product of two matrices, we are using the trace under the hood. To see why the last step is true just use the cyclic property of the trace. Finally, gradients of \\(a\\) and \\(M\\). \\[\\begin{gather}\n&lt;\\hat a, \\dot a&gt;\n= &lt;\\hat b, \\der \\sigma(a)(\\dot a)&gt;\n= &lt;\\der \\sigma(a)^T \\ht b, \\dot a&gt;  \\\\\n&lt;\\ht M, \\dot M&gt;\n= &lt;\\ht a, \\dot M x&gt; = &lt;\\ht a x^T, \\dot M&gt;\n\\end{gather}\\]\n\n\nActivations at Initialization\nRemember that we are sampling the initial weight matrices via \\[\nM_{ij} \\sim \\mathcal N (0, {\\small \\frac 2 m} ) \\quad\\quad N_{ij} \\sim \\mathcal N (0, {\\small \\frac 2 k})\n\\]\nSince \\(M,N\\) are random variables, so are \\(a\\) and \\(b\\). This initialization used carefuly chosen variances to ensure that the entries of the initial activations \\(a = M x\\) and \\(b = \\sigma(a)\\) are within some reasonable range. In particular, we want the entries of \\(b_i\\) to have \\(0\\) mean and variance \\(1\\). And since all the entries are independent, that will mean that \\(|b|^2 \\simeq k\\) and so \\(b\\) will be close to the sphere of radius \\(\\sqrt k\\) with high probability. But this is only the case if the input vector is also normalized. That is why we needed the assumption that \\(|x|^2 = m\\). This section is dedicated to proving these statements.\nFirst we want to understand \\(\\E[a_i^2]\\) \\[\\begin{align}\n\\E[a_i^2]\n&= \\E[ ({\\small \\sum_j M_{ij} x_j})^2] \\\\\n&= \\E[ {\\small \\sum_j M_{ij}^2 x_j^2 + \\sum_{k\\neq j} M_{ij} x_j M_{ik} x_k } ] \\\\\n&= \\sum_j \\E[M_{ij}^2 x_j^2] \\\\\n&= \\frac 2 m |x|^2= 2\n\\end{align}\\]\nwhere we used the independence of different entries of \\(M\\) and the fact that \\(\\E[M_{ij}]=0\\). Then \\[\n\\E[|a|^2] = \\sum_i \\E[a_i^2] = 2k\n\\]\nLet \\(p_M\\) denote the pdf’s of the entries of \\(M_{ij}\\) (all entries have the same pdf because they are iid). The pdf of \\(a_i\\) will be the nested integral of \\(\\delta(a_i - M_i^T x)\\) as \\(M_{i1},\\cdots,M_{im}\\) range from \\(-\\infty \\to \\infty\\), where \\(\\delta\\) denote the Dirac delta function. \\[\np_{a}(z) = \\int_{-\\infty}^\\infty \\cdots \\int_{-\\infty}^\\infty p_M(M_{i1}) \\cdots p_M(M_{im}) \\; \\delta (z - M_i^T x)\\; d M_{i1} \\cdots d M_{im}\n\\]\nRecall that a distribution is symmetric if \\(p(z) = p(-z)\\). Since \\(p_M\\) is a gaussian it is symmetric. The next thing we need to prove is that \\(p_a\\) is symmetric too.\n\\[\\begin{align}\np_{a}(z) &= \\int_{-\\infty}^\\infty \\cdots \\int_{-\\infty}^\\infty p_M(M_{i1}) \\cdots p_M(M_{im}) \\; \\delta (z - M_i^T x)\\; d M_{i1} \\cdots d M_{im} \\\\\n&=  \\int_{-\\infty}^\\infty \\cdots \\int_{-\\infty}^\\infty p_M(-M_{i1}) \\cdots p_M(-M_{im}) \\; \\delta (z + M_i^T x)\\; d M_{i1} \\cdots d M_{im} \\\\\n&=  \\int_{-\\infty}^\\infty \\cdots \\int_{-\\infty}^\\infty p_M(M_{i1}) \\cdots p_M(M_{im}) \\; \\delta (-z - M_i^T x)\\; d M_{i1} \\cdots d M_{im} \\\\\n&= p_a(-z)\n\\end{align}\\]\nThe first step uses the change of variable \\(M_{ij}\\to -M_{ij}\\) and the second one exploits the symmetry of \\(p_M\\) and \\(\\delta\\). Finally, this allows us to compute the term we care about \\[\\begin{align}\n\\E[b_i^2] &= \\int_{-\\infty}^\\infty p_a(a_i) \\sigma(a_i)^2 da_i \\\\\n&= \\int_{-\\infty}^0 p_a(a_i) \\:0\\: da_i + \\int_0^\\infty p_a(a_i) a_i^2 da_i \\\\\n&= \\frac 1 2 \\int_{-\\infty}^\\infty p_a(a_i) a_i^2 da_i \\quad \\text{(using symmetry)}\\\\\n&= \\frac 1 2 \\E[a_i^2] \\\\\n&= k\n\\end{align}\\]\nWe have only been working out \\(\\E[|b|^2] = k\\) but we wanted to make claims about the particular samples themselves being approximately \\(|b|^2\\simeq k\\). Of course, the rigorous way to go about it is to prove statements like \\(\\Pr( k - \\epsilon \\le |b|^2 \\le k+\\epsilon) \\le \\alpha\\). But this type of argument is very tedious and adds very little insight about the ideas this document is exploring. So to conclude the proof in an elegant way, I’ll just assume that \\(|b|^2 = k\\). When the dimension \\(k\\) is large, this approximation will be very accurate.\n\n\nActivation and Gradient Bounds\nDenote by \\(a_0=M_0 x, b_0=\\sigma(a_0)\\) and \\(y_0=N_0 b_0\\) the activaions at initialization \\(w_0 = (M_0, N_0)\\). In the last section we were showing that these activations started in some reasonable range, but that isn’t enough. We need to ensure they remain stable during learning. Concretely, we want to derive upper bounds on \\(|a - a_0|\\) and \\(|y - y_0|\\) based on on \\(|w-w_0|\\). First \\[\\begin{align*}\n|a_0 - a| &\\le |M_0 x - Mx| \\\\\n          & = ||M_0 - M|| \\; |x|\\quad \\text{(operator norm def.)} \\\\\n          & = \\sqrt m |M_0 - M| \\quad \\text{(using result A.3)} \\\\\n          & \\le \\sqrt m \\; |w-w_0|\n\\end{align*}\\]\nTo bound \\(|b_0 - b |\\) we need to use the fact that the ReLU \\(\\sigma\\) is 1-Lipschitz. So \\[\n|b_0 - b| = |\\sigma(a) - \\sigma(a_0) | \\le |a_0 - a | \\le \\sqrt n \\; |w-w_0|\n\\]\nTo apply result 2.2. we need to find \\(\\alpha, \\beta \\in \\R^+\\) such that \\(|\\nabla f(w)|^2 \\ge f(w)( \\alpha - \\beta \\; |w-w_0|)\\). The first step is to lower bound the gradient magnitude of \\(N\\). \\[\\begin{align}\n| \\ht{N} |^2 &= |\\ht y b^T|^2 = \\text{trace}(b y^T y b^T)\n              = |\\ht{y}|^2 |b|^2\n              = 2 \\l |b|^2 \\\\\n              &\\ge \\l (|b_0|  - |b_0 - b|)^2\n              = 2 \\l ( \\sqrt k - |b_0 - b| )^2 \\\\\n              &= 2 \\l ( k - 2 \\sqrt k |b_0 - b| +  |b_0 - b| ^2 ) \\\\\n              &\\ge 2 \\l ( k - 2 \\sqrt k |b_0 - b| ) \\\\\n              &\\ge 2 \\l ( k - 2 \\sqrt {kn} |w_0 - w| ) \\\\\n\\end{align}\\]\nWe could also attempt to derive a lower bound for \\(|\\hat M|^2\\) but it’s not really necessary. We already have enough to apply 2.2. \\[\\begin{align}\n\\frac{|\\nabla f(w)|^2}{f(w)} &= \\frac{|\\hat M|^2 + |\\hat N|^2}{f(w)} \\\\\n&\\ge \\frac{|\\hat N|^2}{f(w)} \\\\\n&\\ge \\frac{2 \\l ( k - 2 \\sqrt {kn}|w_0 - w|)}{f(w)} \\\\\n&\\ge 2k - 4 \\sqrt {kn} |w_0 - w|\\\\\n\\end{align}\\]"
  },
  {
    "objectID": "drafts/research/index.html",
    "href": "drafts/research/index.html",
    "title": "An Architecture For Neural Language Modeling",
    "section": "",
    "text": "\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\Z}{\\mathbb{Z}}\n\\newcommand{\\N}{\\mathbb{N}}\n\\newcommand{\\sft}{\\text{softmax}}\n\\newcommand{\\List}{\\text{List}}\n\\newcommand{\\Seq}{\\text{Seq}}\n\\newcommand{\\SeqT}{\\text{SeqT}}\n\\newcommand{\\CSeqT}{\\text{CSeqT}}\n\\newcommand{\\Dist}{\\text{Dist}}\n\\newcommand{\\SM}{\\text{SM}}\n\\newcommand{\\id}{\\text{id}}\n\\newcommand{\\Fn}{\\text{Fn}}\n\\newcommand{\\Tok}{\\text{Tok}}\n\\newcommand{\\Aij}{ A_{[i,j]}}\n\\]\nA crucial decision in neural language modeling is the choice of architecture, which governs the compuatational and statistical properties of the model. A well-selected architecture can improve performance-per-dollar by orders of magnitude.\nThe objective of this work is to find an architecture with the following four properties:\nAt the time of writing, the most popular architecture for neural language modeling, the Transformer, has only three of these properties (it lacks efficient sampling). Many other architectures have been been proposed, but none that meets all four criteria.\nWe believe that such an architecture exists. This document is an expository writeup of progress we have made towards the goal of discovering it. This document contains our evolving understanding of the key concepts and results needed to understand neural language modeling, including mathematical, computational, and experimental results. Unlike a traditional research paper, this is living document that will continually be updated as we make progress.\nMuch of the content of this document will already be familiar to anyone who has thought deeply about language modeling. There is nothing particularly groundbreaking here. But we found a lot of value in explicitly formalizing the concepts, e.g. how think of Transformers and RNNs as members the same family; as well as grounding all of these ideas in rigorous mathematics. Also, in order to properly understand our specific setting, we often find that it is helpful to describe more general objects of which neural language modeling is a specific case. Where applicable and insightful, we provide such results in their full generality; and as such, this document contains many ideas that may be of independent interest to researchers in related areas.\nIt’s not about minimizing floating point operations. It’s about optimizing tokens per second on our hardware. GPUs perform well with tasks that hare highly parallelizable. RNNs, the more “clasical” architectures for sequences, are not very well suited for these types of tasks.\nimportant feature of the transformer architecture is that it can compute We find that it’s important to think about sequences, and maps between sequences as the core objects."
  },
  {
    "objectID": "drafts/research/index.html#sequence-transformations",
    "href": "drafts/research/index.html#sequence-transformations",
    "title": "An Architecture For Neural Language Modeling",
    "section": "2.1) Sequence Transformations",
    "text": "2.1) Sequence Transformations\nDefinition We say \\(h: \\Seq X \\to \\Seq Y\\) is a sequence transformation if \\(h\\) preserves the length of the sequence. Meaning that if the input sequence has lenght \\(t\\), so does the output sequence.\nWe refer to \\(\\SeqT(X, Y)\\subset \\Fn(\\Seq X, \\Seq Y)\\) as the subset of all sequence transformations.\nIn deep learning we care about composing multiple layers into more complex architectures. Sequence transformations are very well behaved in this way, as the composition of two sequence transforms is a sequence transform itself.1 This is a completely obvious fact, but since it is quite an important one it’s worth stateting it as a result.\nResult: If \\(h\\in \\SeqT(X, Y)\\) and \\(f\\in \\SeqT(Y, Z)\\) then \\(f\\circ h \\in \\SeqT(X,Z)\\).\nIn the next section we’ll see how RNNs and Transformers are examples of sequence transformations. So are elementwise functions on the sequence elements.\nJust from a quick glance at the list above, it’s quite obvious that many lerning problems naturally involve sequence transformations. For tasks like protein folding and image coloring, each entrie of the output sequence is trying to predict missing information of the same entrie on the input sequence. Thus, for this type of tasks we need the output sequence to have the same length as the input one, and so we our model architectures must be a sequence transformation. But of course that isn’t always the case. In particular our central objective of autoregressive language modeling requires architectures with a different signature \\(\\Seq X \\to Y\\).\nWhat is surprizing is that even for tasks like these, sequence transformations turn out to be an extremely useful building block that is used under the hood. To see why, take a moment to think about how you might construct an architecture with the desired input and output spaces. In one side you have the space \\(\\Seq X\\), where a point might contain an unboudedly large amount of informaiton. On the other, you have a \\(Y\\), which often is a finite dimensional vector space. \\(Y\\) is a much “bounded” space than \\(\\Seq X\\). When designing your architecture you are forced to decide where to place the projection from the variable sized space to the fixed sized one. Given that neural network architectures are allways constructed as the composition of many individual layers, there are two very natural choices of where to place the projection: in the beginning, or at the end. We would call the choice early drop and late drop respectively. Early drop architectures would look something like \\[\n\\Seq X \\to \\R^d \\to \\cdots \\to \\R^d \\to Y\n\\]\n\n\n\\(d\\) represents the width of each layer of the neural network. The arrows represent the individual layers of the neural network.\nand the late drop type would be constructed out of many sequence transforms layers except for the last one \\[\n\\Seq X \\to \\Seq \\R^d \\to \\cdots \\to \\Seq \\R^d \\to Y\n\\]\nOne can definetely construct neural network architectures in the two ways, and we are about to see some examples of both. But it does seem to be the case that all the architectures we actually used in practice (e.g. transformers and RNNs) are of the drop last type. We could speculate about why:\n\nusing sequences interally is a good inductive bias for tasks that are naturally expressed as sequences.\nadaptive computation. More complex tasks have more internal state abailable to solve them\n\nBut honestly, that just leads to a lot of talk with little substance. What is certainly true is that the second type is the one with widespread use, it seems to be the deep learning way of doing things. Moreover, in section 3 we will see how thinking about architectures based on sequence transforms enable one massive optimization to train autoregressive language models.\nTODO: Even when your architecture can be thought of in a different way, sequence transform perspective is what allows you to implement them efficiently on hardware. Unleash all your cores to compute different parts of the output all at the same time. Unconstrain yourself"
  },
  {
    "objectID": "drafts/research/index.html#example-architectures",
    "href": "drafts/research/index.html#example-architectures",
    "title": "An Architecture For Neural Language Modeling",
    "section": "2.3) Example Architectures",
    "text": "2.3) Example Architectures\nTODO: make the point that sequence transforms give us a unified perspective to think about RNNs and Transformers\n\n2.3.1) Recurrent Neural Networks\nAn RNN has the signature \\[\nr(s_{t-1}, x_t) = (s_t, y_t)\n\\]\nFor example, a very basic RNN layer might be something like \\[\ns_{i+1} = \\sigma(W s_i) + x_i  \\;\\;\\;\\;\\;\\;\\;\\;\\;\\; y_i = U s_i\n\\]\nwhere \\(x_i, s_i, y_i \\in \\R^d\\) are the inputs, states and outputs respectively and \\(W, U \\in \\R^{d\\times d}\\) are weight matrices and \\(\\sigma\\) is a nonlinearlity like a sigmoid. Of course, there is an infinitude of variations. The specific step equations matter much less than the general pattern of a layer with the signature of \\(r\\).\nAs written, it’s not clear that we have a sequence transformation in our hads. But once we pick an initial state \\(s_{-1}\\in \\R^d\\), any function with such a recurrent formulation can be Given the input sequence \\(x_1 \\cdots x_t\\) we can unrolled the recurrent step and compute the sequence \\(y_1 \\cdots y_t\\). Thus we have a sequence transform in our hands.\nIn deep learning we will want to stack many such layers \\(r\\). For example let’s consider stacking \\(n\\) layers \\(r\\) to construct a deep RNN. The stack of layers will have the same signature of our basic RNN, the only difference is that the state will consist of \\(n\\) vectors in \\(\\R^d\\). The following diagram represents the computation of a deep RNN with 2 layers and \\(t=3\\). The blue boxes represent each step of unrolling the RNN through time.\n But people don’t tend to implement deep RNNs this way! Anyone who has seen an implementation of an RNN will recognize that the actual computation we implement is instead:\n\n\n\nFile (12)\n\n\nIt’s like we try to avoid the recurrent formulation of a deep RNN and instead prefer to see it as a stack of sequence transformations. But why? Ofc they are mathematicaully equivalent, but there is a difference in practical performance in hardware: cache hits EXPAND FURTHER. We start to see how thinking about sequence transforms seems to have some computational advantages.\nTo create the function we need for our autoregressive LM we could take a stack of RNN layers and on the last layer throw out all the outputs except for \\(y_t\\). That gives us a function with signature \\(\\Seq X\\to Y\\).\n\n\n2.3.2) Transformers\nA transformer layer takes in a sequence of inputs \\(X_1 \\cdots X_t \\in \\R^d\\) and uses weight matrices \\(W_Q, W_K, W_v \\in \\R^{d\\times d}\\) to compute \\(Q_i = W_Q X_i\\) etc.. The outputs of the layer are \\[\nY_i = \\sum_{j=0}^t e^{Q_i K_j^T} V_j\n\\]\nAn the causal transformer layer is \\[\nY_i = \\sum_{j=0}^t e^{Q_i K_j^T} V_j\n\\]\nThey are both clearly sequence transformations since we get \\(t\\) outputs \\(Y_i\\).\nOne massive advantage of transformers over alternative architectures like the RNNs we’ve just seen is that they are highly parallelizable. Our GPUs with many cores can split the work of computing all the \\(Y_i\\) in parallel. You don’t need to finish computing \\(Y_1\\) before you can move on to \\(Y_2\\).\nThe potential for this parallelism is something quite natural when one thinks in terms of sequence transformations \\(h: \\Seq X \\to \\Seq Y\\). There is nothing inherent that says that one must compute the output sequence one step at a time.\nTo create a practical architecture for our autoregressive language modeling we would stack a sereies of transformer layers, normally with some MLPs in between. Just like for RNNs, we would throw out all the outputs of the last layer except for \\(Y_t\\). That will gives us a function with the desired signature \\(\\Seq X\\to Y\\).\nNote how it really doesn’t matter is we use causal or non causal transformers for this task. We can get an autoregressive LM out of each. Yes, there is a very good reason to use causal transformers, but to see why we will need to think carefuly about training costs. Something that we will do in section 3.\n\n\n2.3.3) k-gram MLP\nWe’ve seen a couple of common examples of “drop last” architectures. Let’s give 1 of the “drop first” type. As stated, this is not a common thing to do, and the experienced deep learning practitioner will recognize it’s kind of funky. Let’s do it non the less for the sake of completeness\nInspired by the classic \\(k\\)-gram models. If the input sequence is \\(\\Seq \\R^d\\), we could “stack” the \\(k\\) last inputs and apply an MLP to the vector in \\(\\R^{nd}\\). The last layer of the \\(MLP\\) would map into \\(Y\\).\nIndependent of how well this arch learns. It turns out that it is very poorly behaved computationally compared to RNNs and causal transformers. In section 3 we will see how this architecture is poorly suited for the porpuses of autoregressive language modeling in the same way than non causal transformers are."
  },
  {
    "objectID": "drafts/research/index.html#cost-of-training",
    "href": "drafts/research/index.html#cost-of-training",
    "title": "An Architecture For Neural Language Modeling",
    "section": "3.1) Cost of Training",
    "text": "3.1) Cost of Training\nNow, let’s return to considering autoregressive language modeling, \\(f : \\Seq X \\to \\Dist X\\). How many times do we have to call the underlying function \\(f\\) to evaluate the loss \\(l\\)?\nWhen we evaluate the loss we have to compute \\[ l = \\sum_{x\\in \\Seq_t X} \\sum_{i=1}^t -\\ln[f(x_1 \\cdots x_{i-1})(x_i)]\\]\nFor example, suppose our dataset consists of a single sentence: \\(\\text{\"I like penguins\"}\\). To evaluate the loss we need to evaluate our model \\(f\\) on the empty sequence \\(\\text{\"\"}\\) and the first character \\(\\text{\"I\"}\\) and \\(\\text{\"I \"}\\) and \\(\\text{\"I l\"}\\), \\(\\text{\"I lik\"}\\) etc..\n\\[\nf(\\text{\"\"}) \\\\\nf(\\text{\"I\"}) \\\\\nf(\\text{\"I \"}) \\\\\nf(\\text{\"I l\"}) \\\\\nf(\\text{\"I li\"}) \\\\\n\\vdots\n\\]\n\n\n\n385505895_353915030504501_2594593902089499332_n.jpeg\n\n\nSo, for every sequence \\(x_1 \\cdots x_t \\in D\\), to compute the loss we need to call our model on \\(t\\) different inputs.\nThe motivatiion to find a speed up We have been considering the possibility of using an architecture of the type \\[\nf:\\Seq X \\to \\Seq \\R^d \\to \\cdots \\to \\Seq \\R^d \\to Y\n\\]\nbut it might be dumb that all the steps of the computation \\(f(x\\cdots x_t)\\) involve sequences \\(t\\) and at the end we project down and output a vector.\nPerhaps, we could use a different architecture that was a sequence transformation\n\\[\ng: \\Seq X \\to \\Seq \\R^d \\to \\cdots \\to \\Seq \\R^d \\to \\Seq Y\n\\]\nThe dream would be that \\(g(\\text{\"I like penguins\"}) = [f(\\text{\"I\"}), f(\\text{\"I \"}), f(\\text{\"I l\"}), f(\\text{\"I li\"}), \\cdots]\\), so the output sequence contains all the values we need in a single call to our architecture."
  },
  {
    "objectID": "drafts/research/index.html#causal-sequence-transformations",
    "href": "drafts/research/index.html#causal-sequence-transformations",
    "title": "An Architecture For Neural Language Modeling",
    "section": "3.2) Causal Sequence Transformations",
    "text": "3.2) Causal Sequence Transformations\nThe fundamental principle of causality is that the past does not depend on the future. The way to formalize this intuition is to say that evaluating a causal sequence transformation on a subsequence produces an output sequence that is also a subsequence.\nDefinition We say \\(h\\in \\SeqT(X,Y)\\) is a causal sequence transformation if, given a sequence \\(x_1 \\cdots x_t \\in \\Seq_t X\\) with output sequence \\(y_1 \\cdots y_t = h(x_1 \\cdots x_t)\\), for any \\(i\\le t\\), evaluating \\(h\\) on the subsequence \\(x_1 \\cdots x_i \\in \\Seq_i X\\), the outputs \\(y'_1 \\cdots y'_i = h(x_1 \\cdots x_i)\\) have the property that \\(y_j = y'_j\\) for all \\(j\\le i\\).\nWe refer to \\(\\CSeqT(X, Y) \\subset \\SeqT(X, Y)\\) as the subset of all causal sequence transformations.\nThe reader is encouraged to check that all functions below are causal sequence transformations * RNNs as defined in EQX * Transformers as defined in EQY * Elementwise functions \\(h(x_1 \\cdots x_t) = \\sigma(x_1) \\cdots \\sigma(x_t)\\)\nJust like sequence transformations, causal sequence transformations form a category, meaning that they are closed under composition. This means we can use layers that we know are causal sequence transforms and combine as building blocks into more complex architectures.\nResult: If \\(h\\in \\CSeqT(X, Y)\\) and \\(f\\in \\CSeqT(Y, Z)\\) then \\(f\\circ h \\in \\CSeqT(X,Z)\\). Proof Bruh, why you expanding this proof? Don’t be lazy and do it yourself. It’s real easy.\nFor example, a full transformer architecture usually would be constructed by composing many transformer blocks. Where each block consists of a transformer layer as in EQY followed by an MLP applied elementwise to all outputs of the transformer layer. Thanks to the previous result we know that the full transformer architecture will be causal just by confirming that elementwise functions and EQY are causal sequence transformations.\nAlright, now that we understand causal sequence transformations and we have a vast space of such architectures at our dispoal, let’s see why they are so useful for the problem we were trying to solve.\nDefinition: The “take last” function \\(L: \\CSeqT(X, Y) \\to \\Fn(\\Seq X, Y)\\) is defined the following way: given an \\(h\\in \\CSeqT(X,Y)\\), if \\(x_1 \\cdots x_t \\in \\Seq_t X\\) then \\(L(h)(x_1 \\cdots x_t) = [h(x_1 \\cdots x_t)]_t\\). In other words, if \\(y_1 \\cdots y_t = h(x_1 \\cdots x_t)\\) then \\(L(h)(x_1 \\cdots x_t) = y_t\\).\nResult \\(L\\) is a 1-1 mapping where the inverse “concatifies” a function \\(f \\in \\Fn(\\Seq X, Y)\\) by evaluating it on \\(x_1\\) and \\(x_1, x_2\\) etc.. Concretely: \\[\nL^{-1}(f)(x_1 \\cdots x_t) = \\left(f(x_1), f(x_1, x_2), \\cdots, f(x_1 \\cdots x_t) \\right)\n\\]\nProof First note that, given an \\(f\\in \\Fn(\\Seq X, Y)\\), if \\(h\\) is the concatification of \\(f\\), meaning that \\(h(x_1 \\cdots x_t) = \\left(f(x_1), f(x_1, x_2), \\cdots, f(x_1 \\cdots x_t) \\right)\\) then clearly, \\(L(h)(x_1 \\cdots x_t) = f(x_1 \\cdots x_t)\\).\nWe also need to check the other direction. Given an \\(h\\in \\CSeqT(X, Y)\\): \\[\\begin{align}\nh(x_1 \\cdots x_t) &= \\left([h(x_1 \\cdots x_t)]_1, [h(x_1 \\cdots x_t)]_2, \\cdots, [h(x_1 \\cdots x_t)]_t \\right)  \\\\\n&= \\left([h(x_1)]_1, [h(x_1, x_2)]_2, \\cdots, [h(x_1 \\cdots x_t)]_t \\right)   &  \\text{using the fact that $h$ is causal}  \\\\\n&=  \\left(L(h)(x_1), L(h)(x_1, x_2), \\cdots, L(h)(x_1 \\cdots x_t) \\right) &  \\text{using the definition of $L$}\n\\end{align}\\]\nso if we concatify \\(L(h)\\) we get \\(h\\) back. \\(\\blacksquare\\)\nIn summary, \nWe’ve established a 1-1 mapping between the two function spaces. But computationally there is a very big difference between applying \\(L\\) and applying \\(L^{-1}\\). One is expensive the other is cheap. The color of the arrows represent that.\nWe get another interesting diagram when we combine this result with the previous one we get the following picture (where horizontal lines indicate the function is invertible and vertical ones indicate the function is surjective)\n\n\n\nFile (6)\n\n\nWhen we are trying to construct our distribution over length \\(t\\) sequences we loose no expresivety if we start with an \\(h\\in \\CSeqT(X, \\Dist X)\\) and we apply \\(A_t\\circ L: \\CSeqT(X, \\Dist X) \\to \\Dist \\Seq_t X\\). For any \\(f\\in \\Dist \\Seq_t X\\) there will be an \\(h\\in \\CSeqT(X, \\Dist X)\\) s.t. \\(f = A_t \\circ L (h)\\). So mathematically it really doesn’t matter if our base architecture is \\(g\\in \\Fn(\\Seq X, \\Dist X)\\) or \\(h\\in \\CSeqT(X, \\Dist X)\\), but computationally it makes a massive differene!\nSuppose \\(g = L (h)\\). Then, to evaluate the loss on a single sequence \\(\\text{\"hat\"})\\) we required 3 invokations of the function \\(g\\) \\[\ng(\\text{\"h\"}), \\; g(\\text{\"ha\"}), \\; g(\\text{\"hat\"})\n\\]\nBut when we evaluate \\(h(\\text{\"hat\"})\\), since \\(h = L^{-1} \\circ L (h) = L^{-1} \\circ g\\) then \\[\nh(\\text{\"hat\"}) = L^{-1}(g)(\\text{\"hat\"})  = \\left (g(\\text{\"h\"}), \\; g(\\text{\"ha\"}), \\; g(\\text{\"hat\"})  \\right)\n\\]\nFor a sequence of length \\(t\\), just by evaluating \\(h\\) a single time we get the \\(t\\) evaluations of \\(f\\) that we need for the loss. Using a causal sequence transformation is an incredible bargain. You compute 1 and get \\(t-1\\) for free!"
  },
  {
    "objectID": "drafts/research/index.html#efficient-sampling",
    "href": "drafts/research/index.html#efficient-sampling",
    "title": "An Architecture For Neural Language Modeling",
    "section": "3.3) Efficient Sampling",
    "text": "3.3) Efficient Sampling\nCan we do even better? Let’s consider what would it would look like to follow the autoregressive sampling procedure for the RNN of EQX. First we call \\(g(x_1)\\) which requires \\[\ns_1, y_1 = r(s_0, x_1) \\;\\;\\;\\;\\; z_1 \\sim y_1 \\in \\Dist Z\n\\]\nThen we want to call \\(g(x_1, x_2)\\) which requires \\[\ns_1, y_1 = r(s_0, x_1) \\;\\;\\;\\;\\; s_2, y_2 = r(s_1, x_2) \\;\\;\\;\\;\\; z_2 \\sim y_2 \\in \\Dist Z\n\\]\nAnd we can already see the problem. The second invocation of \\(g\\) internally recomputes the term \\(s_1\\) which was already computed in the first call. This is a consequence that the way we are calling \\(g\\) is very structured. And often there is potential for cacheing values and reusing computation between the individual calls.\nIn the case of an RNN, the way you actually want to sample is to first compute \\(s_1, y_1 = r(s_0, x_1)\\) and sample \\(z_1 \\sim y_1\\). Then reuse \\(s_1\\) and compute \\(s_2, y_2 = r(s_1, x_2)\\) and sample \\(z_2 \\sim y_2\\) and so on. This is a big optimization because we are cacheing computation into the state."
  },
  {
    "objectID": "drafts/research/index.html#state-machines",
    "href": "drafts/research/index.html#state-machines",
    "title": "An Architecture For Neural Language Modeling",
    "section": "3.4) State Machines",
    "text": "3.4) State Machines\nTo optimize training we exchanged \\(t\\) calls to an expensive function \\(g\\) into \\(1\\) call to an equally expensive function \\(h\\). To optimize sampling we will echange \\(t\\) calls to an expensive function \\(g\\) into \\(t\\) calls to a function \\(r\\) which is cheaper because it reuses cached computation from the previous calls.\nDefinition: A state machine A state machine in \\(\\SM(X,Y)\\) is a tuple \\((S, s_0, r)\\) where \\(S\\) is a set called the state space, \\(s_0 \\in S\\) is a point we call the origin and \\(u: S, X \\to S, Y\\) is a step/tick function \\[\nr(s_{t-1}, x_t) = (s_t, y_t)\n\\]\nThere is no difference between these equations and the basic form of an RNN, but there are a few reasons we are choosing to call the object state machine instead of RNN. * We will apply this type of equation to things that in no way can be considered neural networks. A good example is a tape, which we will discuss shortly. Using the term NN (i.e. neural network) to refer to such objects feels silly. * Another reason is that for RNN it is usually assumed that \\(s_t \\in \\R^d\\) for some \\(d\\in \\N\\). In other words, the state has a “fixed size”. As we will now see, to be able to unify transformers and RNNs, it’s very important to think in terms of state machines with states of “growing size”.\nIt’s kind of obvious that, if the whole idea of state machines is to allow for the cacheing of information during the repreated invokations of \\(g\\), there will always be a “worst case state machine” for any \\(g\\): the state machine that caches no computations. We call this the tape state machine because the only thing it stores is the inputs.\nDefinition: The tape state machine \\(P(g)\\in \\SM(X,Y)\\) of a function \\(g\\in \\Fn(\\Seq X, Y)\\) is \\[\nP(g)=( \\underbrace{\\Seq X}_S, \\underbrace{[\\;]}_{s_0}, r_g )\n\\]\nwhere the tick funciton \\(r_g\\) is defined as \\[\nr_g \\bigg( \\underbrace{[x_1 \\cdots x_{t-1}]}_{s_{t-1}}, \\; x_t \\bigg)\n= \\bigg( \\underbrace{[x_1 \\cdots x_t]}_{s_t}, \\; \\underbrace{g(s_t)}_{y_t} \\bigg)\n\\]\nThe tape state machine might seem like a bit of a silly object,\n\nIt helps us see that all \\(g\\) can be thought of as state machiens. Once we have this unified framework to compare sampling costs of different models. We just compare the size of the sates and the cost of every tick. We are going to do that next.\nEven though you never want to run the tape state machine, it’s a useful starting point to start the analysys of imporovements. We can sample from autoregressive MLPs. It’s also useful to construct baselines.\nAnd is useful as a software engineering idea. You can see if the samples from a model are good and only then take the effort to implement a highly optimized state machine. Another aplication is testing. When you are implementing an optimized SM, it’s useful to have a (slow) reference that has the exact same interface and outputs as the funciton you are trying to implement.\n\nResult: The unrolling funciton \\(U: \\SM(X,Y) \\to \\CSeqT(X,Y)\\) is surjective. Given \\(m = (S, s_0, p) \\in \\SM(X,Y)\\) \\[\nU(m)(x_1 \\cdots x_t) = [y_1 \\cdots y_t] \\;\\;\\;\\; \\text{where} \\;\\;\\;\\;  (s_t, y_t) = p(s_{t-1}, x_t)\n\\]\nProof: Use the tape state machine… \\(\\blacksquare\\)\nThe following diagram summarizes the result\n\n\n\nFile (10)\n\n\nNote how \\(P\\circ L\\) behaves similarly to an inverse of \\(U\\) because \\(U \\circ P \\circ L = \\text{id} : \\Fn(X,Y) \\to \\Fn(X,Y)\\). But the other direction is not necessarily true. For example if we start with a classic RNN \\((\\R^d, 0, r)\\) and we \\(P \\circ L \\circ U\\) we end up with tape state machine that is different from the RNN.\nWe can also combine it with the autoregressive correspondance result to get the diagram: \n\nRNN State Machines\n\nTape SM: \\(O(t)\\) memory and \\(O(td^2)\\) compute\n\n\nEfficient SM: \\(O(d)\\) memory and \\(O(d^2)\\) compute\n\n\n\nSelf Attention State Machines\n\nTape SM: \\(O(t)\\) memory and \\(O(t^2d)\\) compute\n\n\nKV cache SM: \\(O(td)\\) memory and \\(O(td)\\) compute\nFunnily enough, this imporvement #### Efficient SM: \\(O(d^2)\\) memory and \\(O(d^2)\\) compute Does it exist for the transformer? Yes under the assumption that…"
  },
  {
    "objectID": "drafts/research/index.html#summary",
    "href": "drafts/research/index.html#summary",
    "title": "An Architecture For Neural Language Modeling",
    "section": "3.5) Summary",
    "text": "3.5) Summary\nOk, we’ve seen a lot of changes of perspectives. The following diagram sumamrizes what is the relationship between the objects we’ve been using, and what each perspective is useful for.\n\n\n\nFile (1)\n\n\nOur objective now is to construct good architectures with both, an efficient causal sequence transformation and state machine implementations. We will see how causal transformers can be seen as examples of causal sequence transformations, and then we will present our star architecture a small modification that we call linear self attention that retains all the advantages of the transformers but gives a big improvement on the causal sequence implementation\nBut it’s going to be easier to introduce the key ideas that allow for these optimizations in the simpler setting of sequence transformations.\nTransformers are amazing for two general reasons * generalize very well (better than RNNs?) * run very fast on our GPUs (way better than RNNs)\nbut suffer from some disadvantages: * Cost \\(O(t^2)\\) on train on datasets of length \\(t\\) as opposed to \\(O(t)\\) for RNNs * Sampling a sequence of length \\(t\\) has cost \\(O(t^2)\\) vs \\(O(t)\\) for RNNs\nOur empirical results will be focused on an architecture we implemented that we call Self Attention RNN (SARNN) / Self Attention State Machine / We think its a very succseful architecture at combining the best of both worlds because it has the properties: * generalize very well * run very fast on our GPUs * Cost \\(O(t)\\) to train on datasets of length \\(t\\) * Sampling a sequence of length \\(t\\) has cost \\(O(t)\\)"
  },
  {
    "objectID": "drafts/research/index.html#footnotes",
    "href": "drafts/research/index.html#footnotes",
    "title": "An Architecture For Neural Language Modeling",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSequence transformations form a category.↩︎"
  },
  {
    "objectID": "blogposts/mission/index.html",
    "href": "blogposts/mission/index.html",
    "title": "Our Mission",
    "section": "",
    "text": "A poem is a shadow of the act of writing poetry.\nHumanity casts many shadows. All literature, letters, recipes, and tweets. Mathematical proofs and git repositories. Laws, treaties, and declarations of war. Financial statements, employee performance reports, and bankruptcy filings. Podcasts, operas, accidental voicemails, YouTube videos and Hollywood blockbusters. Restaurant reviews and love letters and times tables. Individually, each of these pieces of information is nothing but the faintest shadow of the process that produced it. But collectively, these shadows tell a rich story about the world.\nSince the dawn of humanity, the brain alone could reconstruct the world from these shadows. But the last decade of deep learning has convinced us that this won’t be the case for much longer. Though significant challenges remain, we stand poised to solve them. If successful, it will become possible to synthesize every documentable aspect of humanity into the weights of a neural network.\nOur mission is to train a neural network to model all human output.\nThere are two primary challenges in our pursuit of this goal.\n\nCurrently, it’s not technically feasible to train a model that can ingest all the data that we can collect. Limitations around context length, modality, and throughput force us to use only a small subset of the data available to us.\nMuch of the data we will need has never been collected, curated, and organized into datasets that we can use for training.\n\nWe are building a world-class team of engineers and researchers to tackle these challenges, united around shared principles and a specific research agenda. We value clear thinking, sharing knowledge, and an extreme commitment to scientific honesty. Our research is guided by mathematical beauty and grounded in rigorous empiricism. We are committed to letting the quality of our work speak for itself. No hype, no fluff. All meat.\nIf this vision resonates, please reach out:          777b7a607577605479757a7d72716760757d3a777b79\n\n\n  \n  Or, subscribe to be notified of new posts:"
  },
  {
    "objectID": "blogposts/faster-after-all/index.html",
    "href": "blogposts/faster-after-all/index.html",
    "title": "Linear Transformers Are Faster After All",
    "section": "",
    "text": "\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\Z}{\\mathbb{Z}}\n\\newcommand{\\N}{\\mathbb{N}}\n\\newcommand{\\sft}{\\text{softmax}}\n\\newcommand{\\List}{\\text{List}}\n\\newcommand{\\Seq}{\\text{Seq}}\n\\newcommand{\\SeqT}{\\text{SeqT}}\n\\newcommand{\\CSeqT}{\\text{CSeqT}}\n\\newcommand{\\Dist}{\\text{Dist}}\n\\newcommand{\\SM}{\\text{SM}}\n\\newcommand{\\Fn}{\\text{Fn}}\n\\newcommand{\\Tok}{\\text{Tok}}\n\\newcommand{\\Aij}{ A_{[i,j]}}\n\\]\nIt is well-known that removing the exponential from the attention layer of a transformer allows for a recurrent reformulation, with computational cost that is linear instead of quadratic on context length [1]. One would expect that such an architecture would be far faster to train, especially when the context size is large. However, when training deep neural networks on hardware accelerators (e.g. GPUs), reductions in FLOPs do not always straightforwardly translate into practical speedups. Initial empirical experiments showed that large language models based on linear transformers train more slowly than classic transformers, and led many to dismiss the approach as nice-in-theory but unhelpful-in-practice [2].\nAt the moment, the conventional wisdom in the field is that a quadratic-cost algorithm with highly optimized hardware utilization (e.g. FlashAttention) gives the best training throughput [3]. But this is mistaken. In this post, we explain several different ways of implementing linear transformers and we show that, in fact, they can produce massive speed-ups.\nThe experiment below showcases the central takeaway. We compare the speed of an optimized transformer, a straightforward recurrent implementation of the linear transformer (as described in [1]), and an optimized linear transformer (described in Section 3). We see that, despite exhibiting better growth with respect to context size, the linear transformer trains much more slowly than the transformer in wall-clock time. In contrast, our algorithm is strictly faster than even the highly-optimized FlashAttention baseline, at all context and model sizes. At moderately large context sizes, this speedup has an larger impact than FlashAttention itself.\nBut speed is only one part of the picture. We also need to know whether linear transformers actually minimize the loss as well as standard transformers do. Unfortunately, as the next plot shows, directly converting GPT2 to use linear transformer layers hurts learning significantly.\nThese results are discussed in detail in Section 5, but in short: linear transformers seem to become increasingly unstable as the sequence length grows, negatively affecting learning. This means that our linear transformers are somewhat useless,1 as the positive impact from the speedup seen in long contexts is undermined by the negative impact of degraded learning.\nOther variants of linear transformers have been proposed that claim resolve these learning issues [5]–[11], but we do not survey them today. Since our main motivation in this post is to share our insights on how to implement efficient linear transformers we stick with the most natural variant, which is most closely analogous to standard transformers. In a future post, we will explain how to improve the learning of linear transformers, allowing us to efficiently train unprecedentedly-long-context language models with super-fast sampling."
  },
  {
    "objectID": "blogposts/faster-after-all/index.html#linear-transformers",
    "href": "blogposts/faster-after-all/index.html#linear-transformers",
    "title": "Linear Transformers Are Faster After All",
    "section": "1. Linear Transformers",
    "text": "1. Linear Transformers\nThe inputs to a transformer layer are sequences of \\(Q_i, K_i, V_i \\in \\R^d\\) of query, key and values, where \\(i\\) ranges from \\(1\\) to the sequence length \\(t\\). The outputs are a sequence \\(Y_i\\in \\R^d\\). The well-known formula for the transformer layer, first popularized by Vaswani et al [12], is: \\[\nY_i^\\text{Transformer} = \\sum_{j=1}^i e^{Q^T_i K_j} V_j\n\\] Here we are excluding the denominator of the softmax for simplicity of notation. Although the normalization provided by the denominator is important for good learning performance, it doesn’t have a major impact on the computational cost or implementation speed, which are our main focus here.\n\n\nEven though we omit the normalization for the mathematical exposition, it is included in our implementation for all experiments.\nThe formula for the linear transformer (LT) layer is quite similar: just change the term \\(e^{Q^T_i K_j} \\to  Q^T_i K_j\\) yielding \\[\nY_i^\\text{LinearTransformer} = \\sum_{j=1}^i Q^T_i K_j V_j\n\\]\n\n\nAll our experiments with linear transformers also include a normalization factor, which we empirically found to be important for learning performance. Drawing on [1], we divide each \\(Y_i\\) by \\(\\sum_{j=1}^i Q^T_i K_j\\) after eunsuring the sum is positive by making keys and queries live in the positive quadrant using softplus.\nThis layer is “linear” in that the outputs \\(Y\\) are linearly related to all of \\(Q\\), \\(K\\), and \\(V\\).2 From now on, we will omit the superscript of \\(Y_i^\\text{LinearTransformer}\\) and just write \\(Y_i\\). To begin our exploration of the computational cost of linear transformers, consider the following implementation.\ndef LT_attention(Q, K, V):\n    \"\"\"\n    Shapes of inputs are\n     Q: [t, d]  K: [t, d]  V: [t, d]\n    Shapes of outputs are\n     Y: [t, d]\n    \"\"\"\n    t, d = Q.shape\n    Y_list = []\n    for i in range(t):           # loop cost: O(t^2 d)\n        Y_i = zeros(d)\n        Q_i = Q[i]\n        for j in range(i):       # loop cost: O(id)\n            A_ij = inner(K[j], Q_i)  # cost: O(d)\n            Y_i += A_ij * V[j]   # cost: O(d)\n        Y_list.append(Y_i)\n    return stack(Y_list)\nAnyone who has worked with self-attention will recognize that this is a terrible implementation, since nested for-loops are not GPU-friendly. Don’t worry: in the next section, we will discuss implementations that parallelize computation, and thus run much faster on modern hardware. For now, the main point of this pseudocode is to highlight that first mode of computation, which we call attention formulation, has a FLOP cost of \\(O(t^2 d)\\).\nThe key to the massive speedups we saw in the introduction comes from leveraging linearity to restructure the computation. Consider the following factorization: \\[\nY_i = \\sum_{j=1}^i Q^T_i K_j V_j = \\underbrace{ \\left (  \\sum_{j=1}^i V_j  K_j^T\\right )}_{S_i} \\; \\; Q_i\n\\] Written in this form, we notice that the term labeled \\(S_i \\in \\R^{d\\times d}\\) can be thought of as a state summarizing all the relevant information up to time \\(i\\). It’s easy to rewrite into the following recurrent equations \\[\nY_{i} = S_i Q_i\n\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\nS_i = S_{i-1} + V_i K_i^T\n\\] where we assume \\(S_{0} = 0\\in \\R^{d\\times d}\\). Written in this form, we realize that a linear transformer is an RNN. We can also write pseudocode for this approach, which we call the state formulation, and analyze the cost:\ndef LT_state(Q, K, V):\n    \"\"\"\n    Shapes of inputs are\n     Q: [t, d]  K: [t, d]  V: [t, d]\n    Shapes of outputs are\n     Y: [t, d]\n    \"\"\"\n    t, d = Q.shape\n    S_i = zeros(d, d) # shape [d,d]\n    Y_list = []\n    for i in range(t):        # loop cost: O(t d^2)\n        S_i += outer(K[i], V[i]) # cost: O(d^2)\n        Y_i = S_i @ Q[i]      # cost: O(d^2)\n        Y_list.append(Y_i)\n    return stack(Y_list)\nWe see that the cost here is \\(O(t d^2)\\).\nSo, while a standard transformer layer always has cost \\(O(t^2 d)\\), linear transformers have two formulations with different costs. By switching from the attention formulation to the state formulation, we can change the cost from \\(O(t^2 d)\\) to \\(O(t d^2)\\), trading a \\(t\\) term for a \\(d\\) term."
  },
  {
    "objectID": "blogposts/faster-after-all/index.html#parallel-implementations",
    "href": "blogposts/faster-after-all/index.html#parallel-implementations",
    "title": "Linear Transformers Are Faster After All",
    "section": "2. Parallel Implementations",
    "text": "2. Parallel Implementations\nIn general, for-loops are a terrible way to implement anything that will run on a GPU. When using high-level frameworks like PyTorch or JAX, the easiest way to get high GPU utilization is to only use primitives that are already highly optimized for GPU: matrix multiplication, elementwise operations, etc. We can rewrite our attention and state algorithms in this style to make them efficient.\nFirst, let’s do this for attention. Our main technique is to compute the attention matrix \\(A\\), which contains all the terms outer(Q[i], K[j]) that appeared inside the for-loops of LT_attention, using a single heavyweight matrix multiply.\ndef LT_attention_parallel_no_flash(Q, K, V):\n    \"\"\"\n    Shapes of inputs are\n     Q: [t, d]  K: [t, d]  V: [t, d]\n    Shapes of outputs are\n     Y: [t, d]\n    \"\"\"\n    t = Q.shape[0]\n    M = causal_mask(t)\n    A_raw = Q @ K.T  # cost O(t^2 d)\n    A = A_raw * M    # cost O(t^2)\n    Y = A @ V        # cost O(t^2 d)\n    return Y\nThis implementation lies at the core of nearly every transformer of the past five years, powering all of the AI models that have recently exploded in popularity. Since it is composed of such a small number of ultra-parallelizable ops, it obtains high GPU utilization. Recently, specialized flash attention kernels [3] have been used to get even further speedups by avoiding explicitly storing the attention matrix \\(A\\), and thereby saving both memory and time spent on memory-transfers. Algorithmically, though, flash attention is a variant of parallel attention, and has the same computational cost (as measured in FLOPs). We use LT_attention_parallel to refer to the flash attention implementation.\nNext, let’s parallelize the recurrent formulation. This optimization is less well-known. The key is to compute all the terms \\(V_i K^T_i\\) in parallel, and then use a cumulative-sum, which can be parallelized, to combine them.\ndef LT_state_parallel(Q, K, V):\n    \"\"\"\n    Shapes of inputs are\n     Q: [t, d]  K: [t, d]  V: [t, d]\n    Shapes of outputs are\n     Y: [t, d]\n    \"\"\"\n    P = V[:,:,None] @ K[:,None,:]  # cost: O(t d^2)\n    S = cumsum(P, axis=0)          # cost: O(log_2(t) t d^2)\n    Y = S @ Q[:,:,None]            # cost: O(t d^2)\n    return Y[:,:0]\nThe cost in FLOPs of this algorithm is \\(O(\\log_2(t) t d^2)\\).3\nNow that we have four mathematically-equivalent implementations of a linear transformer, let’s time the training step of our GPT2 models and see how big of a difference it makes. For our LT_attention_parallel implementation, we use a custom linear self-attention flash kernel we implemented in Triton [13] based on OpenAI’s FlashAttention2 implementation.\n\n\n\n\nHere are some takeaways:\n\nAs expected, the attention variants all have a quadratic asymptotic cost (slope of 2 on a log-log plot4). The state variants all have linear asymptotic cost (slope 1). 5\nLT_state_parallel is an order-of-magnitude faster than LT_state.\nLT_attention_parallel_no_flash is two orders-of-magnitude faster than LT_attention.\nLT_attention_parallel seems to asymptotically stabilize into being an order-of-magnitude faster than LT_attention_parallel_no_flash.\nFor the majority of settings, LT_attention_parallel is the fastest. (This is the linear version of the algorithm used by the standard transformer.)\nParallel attention is the fastest algorithm for small context sizes. However, LT_state_parallel overcomes LT_attention_parallel_no_flash at around 13k context size, and overcomes LT_attention_parallel at around 100k.\n\nOverall, these results paint a clear picture: use the attention algorithm for small contexts and the state algorithm for large contexts. But do we really face a binary choice? Can we combine the state and attention ideas and get the best of both words?"
  },
  {
    "objectID": "blogposts/faster-after-all/index.html#chunked-formulation",
    "href": "blogposts/faster-after-all/index.html#chunked-formulation",
    "title": "Linear Transformers Are Faster After All",
    "section": "3. Chunked Formulation",
    "text": "3. Chunked Formulation\nIt’s evident that, for small context sizes, computing the \\(t\\) by \\(t\\) attention matrix is much more efficient than computing many \\(d\\) by \\(d\\) state matrices. But as \\(t\\) grows, there is a point where the quadratic cost of the attention matrix ends up dominating. Noticing that attention is extremely efficient for small \\(t\\) and that states are necessary for large \\(t\\) motivates doing one last reworking of the LT equation.\nLet \\(c \\in \\N\\) be a positive integer that we’ll call the chunk size. For any \\(i\\in \\N\\) find the unique \\(n\\in \\Z\\) s.t. \\(cn &lt; i \\le c(n+1)\\). We can easily see that the following equations are equivalent to the previous ones. \\[\nY_{i} = S_{cn}Q_i + \\sum_{j=cn+1}^i Q_i^T K_j V_j\n\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\nS_{c(n+1)} = S_{cn} + \\sum_{j=cn+1}^{c(n+1)} V_j K_j^T\n\\] The key idea is that we are only going to compute a subset of all states: \\(S_0, S_c, S_{2c}, \\cdots\\). Then, to compute each output \\(Y_i\\), we need only to take into account the contribution via the most recent state \\(S_{cn}\\), as well as the contribution (computed via attention) of all moments in time \\(j\\) in the range \\(cn &lt; j \\le i\\).\nAs pseudocode, this looks like:\ndef LT_attention_with_initial_state(S, Q, K, V):\n    \"\"\"\n    Shapes of inputs are\n     S: [d, d]  Q: [c, d]  K: [c, d]  V: [c, d]\n    Shapes of outputs are\n     Y: [c, d]\n    \"\"\"\n    Y_state = Q @ S                               # cost O(c d^2)\n    Y_attention = LT_attention_parallel(Q, K, V)  # cost O(c^2 d)\n    Y = Y_state + Y_attention                     # cost O(cd)\n    return Y\n\ndef LT_chunked(Q, K, V, c):\n    \"\"\"\n    Shapes of inputs are\n     Q: [t, d]  K: [t, d]  V: [t, d], c: int\n    Shapes of outputs are\n     Y: [t, d]\n    \"\"\"\n    t, d = Q.shape\n    assert t % c == 0\n    Q_, K_, V_ = [arr.reshape(t//c, c, d)\n    `               for arr in [Q,K,V]]\n    P_ = K_.transpose([0,2,1]) @ V_  # cost O(t d^2)\n    S_ = cumsum(P_, axis=0) - P_     # cost O(log_2(t/c)(t/c)d^2)\n    Y_ = vmap(LT_attention_with_initial_state, axis=0)(\n                S_, Q_, K_, V_)      # cost O(td^2 + tcd)\n    return Y_.reshape(t, d)\nThe cost is \\(O\\left(td^2 + tcd + \\log_2(t/c)(t/c)d^2\\right)\\), once again avoiding a quadratic dependency on \\(t\\). Also, note that this algorithm makes an inner call to LT_attention_parallel, so we can use a flash-attention kernel to do that part of the computation.\nThis algorithm has a hyperparameter, the chunk size, which must be set correctly for optimal performance. Fortunately, it is inexpensive to identify the best chunk size empirically, since measuring just a few steps of training at each chunk size is sufficient to identify which is the fastest. In the plot below, we plot the speed of the optimally-chunked algorithm in each setting.\n\n\n\n\nWe see LT_chunked gives the desired best-of-both-worlds behavior, as it is equal-or-better than all other approaches in all settings. And we now see the massive (& rapidly growing) speedup relative to standard self-attention, finally unlocking the true power of linear transformers."
  },
  {
    "objectID": "blogposts/faster-after-all/index.html#sampling",
    "href": "blogposts/faster-after-all/index.html#sampling",
    "title": "Linear Transformers Are Faster After All",
    "section": "4. Sampling",
    "text": "4. Sampling\nWhen working with language models, efficient training is not the only performance that deserves consideration. Once the model is trained, how expensive is it to utilize? When we are sampling we have a sequence of tokens, \\(z_1 \\cdots z_t\\), and we want to sample the next token, \\(z_{t+1}\\).\nThe most efficient algorithm to sample from traditional transformers is called the KV-cache algorithm [14]. This algorithm assumes that when we generate token \\(z_{t+1}\\), we will have already computed and cached all the \\(K_i, V_i\\) for all \\(0 \\le i \\le t\\). In order to compute the output of the attention layer at time \\(t+1\\) given this cached information, we can use \\[\nY_{t+1}^\\text{Transformer} = \\sum_{j=1}^{t+1} e^{Q^T_i K_j} V_j\n\\] It is easy to see that this is an \\(O(td)\\) operation. In other words, as the sequence length grows, sampling each subsequent token becomes more computationally expensive.6 This is one of the major limitations of the classic transformer architecture.\nWith linear transformers, however, we have access to the recurrent formulation. A linear transformer is an RNN where the state has size \\(O(d^2)\\). \\[\nY_{i} = S_i Q_i\n\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\nS_i = S_{i-1} + V_i K_i^T\n\\] We can compare the time it takes to generate any particular token when sampling a sequence:\n\n\n\n\nAs expected, we see that the time to sample of the linear transformer is independent of context length, whereas that of the KV-cache transformer grows linearly. This leads to a large gap in inference speed at large contexts.7"
  },
  {
    "objectID": "blogposts/faster-after-all/index.html#learning-performance",
    "href": "blogposts/faster-after-all/index.html#learning-performance",
    "title": "Linear Transformers Are Faster After All",
    "section": "5. Learning Performance",
    "text": "5. Learning Performance\nUntil now, our focus has been on how to implement linear transformers efficiently. But an architecture that runs really fast is useless unless it also learns well. We will now compare the learning curves of the GPT2 baselines with their linear transformer counterparts, at various context sizes.\nIn order to control for the fact that longer contexts help learning by introducing more tokens per update, we hold the number of tokens-per-update constant across context sizes by decreasing batch size. At all context-lengths, each update sees \\(2^{19}\\) tokens.8 Importantly, for this set of experiments, we have used the dataset c4 [4], which does not have much long-term structure: it consists of a shuffled collection of short articles, of average length ~500 tokens.\nFirst, let’s look at the parameter scaling law of GPT2 and our modified Linear-GPT2. The following plot shows the final performance after 24h of training on our 8xH100 server for each scale and each context size. Click the second tab if you want to see the full learning curves.\n\n\n\n\nBoth architectures scale similarly with respect to model size, but there is a gap in performance. This gap seems to grow as context size increases, together with more loss spikes and general instability. It’s possible that the gap can be explained by the linear transformer not effectively utilizing its context. To explore whether or not this is the case, we can use these same experiments, but visualize all context-lengths together.\n\n\n\n\nWe see a dramatically different trend between the two architectures. For GPT2, each increase in context length slows down the initial pace of learning, but ultimately all context-lengths (beyond at least 1024) converge to a similar final loss. Whereas for Linear-GPT2, not only does increasing the context slow down learning in the beginning, but it also causes convergence to a worse final performance and nasty-looking loss spikes.\nThe results for GPT2 are what we would expect from a healthy model, given the setting. Note that since our dataset consists of short articles, of average length ~500 tokens, we can assume that even a context window of 1024 would include nearly everything relevant. Thus, we wouldn’t expect that increasing the context length beyond this point would decrease the loss the model ultimately converges to. Longer-context models do however need to learn to ignore many more irrelevant tokens, explaining the slowed initial learning.9\nIn contrast, the results for Linear-GPT2 seem to indicate some underlying instability of the linear transformer architecture. It doesn’t even seem capable of ignoring irrelevant information, let alone exploiting useful long-term structure.\nRemedying these learning issues will be the main focus of our future work in linear transformers. Our current architecture is essentially a one-to-one clone of GPT2, sticking as architecturally close to the original as possible; aspects such as initializations, normalizations and hyperparameters were directly copied. It is well-known that these decisions can have a large impact on scaling and stability and often need to be tuned in an architecture-specific way. In the literature, various other linear variants of self-attention have been proposed, that incorporate techniques such as gating mechanisms in order to improve stability and learning [5]–[11]. A future post will include a thorough study of the impact of all of these choices.\nUltimately, it may be impossible for any linear transformer to perfectly match the context scaling laws of the classic transformer baseline. Although removing the exponential seems like a minor change, it represents a meaningful decrease in expressivity.10 But as we’ve shown, linear transformers can be trained orders-of-magnitude more efficiently. On equivalent compute, linear transformers can be trained for many more steps; or, keeping steps constant as well, we can train a much larger model. If this additional scaling is sufficient to compensate for the reduced expressivity, linear transformers will be the more efficient architecture overall.\n\n  \n  Subscribe to be notified of new posts:"
  },
  {
    "objectID": "blogposts/faster-after-all/index.html#footnotes",
    "href": "blogposts/faster-after-all/index.html#footnotes",
    "title": "Linear Transformers Are Faster After All",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs discussed in Section 4, a second benefit of linear transformers is that the cost to sample a token does not grow with context size. Perhaps one could argue that this improvement in sampling speed could, on its own, justify using linear transformers for applications where the inference costs vastly exceed training costs. But it is evident to us that, for linear transformers to become actually useful, we need to address these instability issues.↩︎\nIt is not named for the fact that the computational cost is linear with respect to \\(t\\)! That is just a coincidence. (And is not even always true, as we will see.)↩︎\nInterestingly, LT_state_parallel is actually more expensive than LT_state. (This is in contrast with the attention formulation, where LT_attention and LT_attention_parallel share the same \\(O(t^2 d)\\) cost.) As we will see in a moment, this extra \\(\\log_2(t)\\) factor is well worth the parallelization benefits.↩︎\nIf \\(y=x^2\\), a log-log plot where \\(y'=\\log_a(y)\\) and \\(x'=\\log_a(x)\\) for any base \\(a\\), then \\(y'=\\log_a(y) = \\log_a(x^2) = 2 \\log_a(x) = 2 x'\\). So the graph will be a line with slope 2.↩︎\nThe reason we see the expected slopes asymptotically is that we are timing a full GPT2 architecture which has many other components besideds the attention layer. If we were only timing the attention layer, the plots would all be straight lines.↩︎\nAn interesting connection is that the KV-cache can be understood as the state of an RNN with non-constant state size; namely, one whose state-size is \\(O(td)\\).↩︎\nThis comparison may not be completely fair. In these experiments, our implementation of neither sampling algorithm makes use of specialized kernels. A lot of the ideas of flash attention can be used to write a much faster KV cache sampling algorithm; on the other hand, it’s unclear if much improvement is possible on the recurrent sampling. Thus, it’s possible that with engineering effort the gap between the two algorithms could become smaller. However, the overall pattern will certainly remain the same.↩︎\ne.g. runs with context-size 1024 would have batch-size of \\(2^{19} / 2^{10} = 2^{9} = 512\\).↩︎\nPut another way: doubling the size of the input vastly increases the size of the function space over which gradient descent must search, and it’s intuitive that in a larger space it takes somewhat longer to find a good solution.↩︎\nWe plan to elaborate on this topic in a future blog post.↩︎"
  },
  {
    "objectID": "drafts/research.html",
    "href": "drafts/research.html",
    "title": "Research",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\n\n\n\n\nDec 27, 2023\n\n\nAn Architecture For Neural Language Modeling\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "articles/longcrawl64/index.html",
    "href": "articles/longcrawl64/index.html",
    "title": "LongCrawl64: A Long-Context Natural-Language Dataset",
    "section": "",
    "text": "This article is a pre-release WIP, and will be updated soon.\nAs part of our broader mission of training language models with ultra-long context, we are releasing a dataset for use in research on architectures and algorithms for long-context modeling. This dataset, which we call LongCrawl64, is available on GitHub. It consists of 6,661,465 pre-tokenized documents, each of which is 65,536 tokens long, for a total token count of 435 billion."
  },
  {
    "objectID": "articles/longcrawl64/index.html#footnotes",
    "href": "articles/longcrawl64/index.html#footnotes",
    "title": "LongCrawl64: A Long-Context Natural-Language Dataset",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe use the same criteria as [1].↩︎\nThe SI prefix “kibi” means 1024, so 64 KiT = 65536 tokens.↩︎\nBy “roll”, we mean in the numpy.roll sense. For example, rolling [12, 5, 7, 4, 21] by 3 would yield [7, 4, 21, 12, 5]. This preprocessing step causes us to sometimes be predicting tokens from the start of a document, conditional on tokens from its end. This is atypical, but completely legitimate; we invite any skeptics to watch the Star Wars movies in release order, beginning with Episode IV.↩︎\nWe split this into a train set of 6,609,334 documents and a heldout set of 52,131 documents.↩︎"
  },
  {
    "objectID": "articles/compute-optimal-context-size/index.html",
    "href": "articles/compute-optimal-context-size/index.html",
    "title": "Compute-Optimal Context Size",
    "section": "",
    "text": "The objective of language modeling is to predict each token in a sequence. Each prediction is conditional on a subset of previous tokens, which are called the context for the prediction. Intuitively, expanding the context should make prediction task strictly easier. If an extra token provides relevant information, the model can learn to use it; otherwise, the model can simply ignore it. Therefore, given any well-trained language model, we expect the average loss to be lower on longer-context predictions. To verify this, we trained a 772-million-parameter transformer1 with context size 32 kibitokens (KiT).\nWe refer to this as a contextwise loss curve, and the general phenomenon of the loss improving on longer contexts as inference-time context scaling. This trend is not specific to our training setup, and has been observed elsewhere in the literature. For example, below is a plot from Gemini [1], illustrating the same effect.\nInference-time context scaling provides a quantitative justification for increasing the training context. Intuitively, training on longer contexts increases the extent to which we can leverage inference-time context scaling, ultimately decreasing loss. This motivates an approach to selecting the size of the training context: choose the context size that optimizes loss given training budget.2 This is a natural complement to existing research on scaling laws. For example, Kaplan et al [2] and Hoffmann et al [3] investigated the optimal way to scale the model size & amount of tokens seen, but both works held context length fixed. To complete this analysis, one must optimize over context length as well.\nIn Section 3, we will do exactly this, using GPT-2-style transformers at scales ranging from 124 million to 1.6 billion parameters. The results show that the optimal training-context length increases with larger training budgets. But devising a proper experimental setting to compare between train-context lengths is surprisingly tricky. It turns out that popular datasets (such as openwebtext or C4) and standard metrics (average train loss) are inappropriate. We begin by discussing these two subtle but important details: in Section 1, we address the choice of dataset, and in Section 2, we address the choice of evaluation metric.\nWe conclude with Section 4, a discussion of some applications that are unlocked by models with ultra-long contexts, from kilotokens up to petatokens. But the vast potential of models with ultra-long contexts cannot be realized if they are trained in a setting that is far from compute-optimal. And so, we need research focused on increasing the optimal training-context size. We believe that careful evaluation of context scaling will be an essential ingredient in progress, and hope that the dataset, ideas, and evaluations presented in this article will prove useful towards that objective."
  },
  {
    "objectID": "articles/compute-optimal-context-size/index.html#data-with-long-term-structure",
    "href": "articles/compute-optimal-context-size/index.html#data-with-long-term-structure",
    "title": "Compute-Optimal Context Size",
    "section": "1. Data with Long-term Structure",
    "text": "1. Data with Long-term Structure\nBelow is a contextwise loss curve similar to the ones in the introduction. It shows the average loss at every context length for a 1.6-billion-parameter model trained using 8 KiT of context on openwebtext. In the first part of the curve, this plot shows contextwise scaling, with performance improving as more tokens are seen. But the trend of improvement tapers off. After around 2 KiT, additional tokens no longer improve the loss.\n\n\n\n\nTo understand the reason for this, one only need look at the document-length distribution of the openwebtext dataset.\n\n\n\n\nOver 90% of the documents are less than 2 KiT long. In order to train train 8-KiT-context models on this dataset, somehow longer documents must be constructed out of smaller ones (in our experiments, we simply concatenated multiple documents). But the resulting “long” documents do not truly contain any long-term structure, and so there is no benefit to seeing additional tokens at inference-time.\nThis problem is not restricted to openwebtext. Many other popular datasets, such as C4 and RedPajama, have similar document-length distributions. This is insufficient for our goals, because it does not allow one to thoroughly evaluate contextwise scaling properties.\nTo solve this issue, we created LongCrawl64, a large natural langauge dataset composed entirely of documents of length 64 KiT. This data is a subset of Common Crawl, tokenized using OpenAI’s TikToken and with short documents filtered out. The end result is a 6661465 x 65336 Zarr array of uint16s, representing 6,661,465 documents each of size 64 KiT. The total token count is 435 billion, two orders of magnitude larger than openwebtext (6 billion). Read our release for the details around the construction and usage of the dataset; for example, how to efficiently load documents when training at context lengths shorter than 64 KiT.\nArmed with this new dataset, we can repeat our experiment and again compute the contextwise loss curve of a 1.6-billion-parameter transformer with context size 8 KiT:\n\n\n\n\nOn LongCrawl64, we see consistent contextwise scaling throughout the train context. With this first issue resolved, let’s move on to the second."
  },
  {
    "objectID": "articles/compute-optimal-context-size/index.html#the-training-loss-is-misleading",
    "href": "articles/compute-optimal-context-size/index.html#the-training-loss-is-misleading",
    "title": "Compute-Optimal Context Size",
    "section": "2. The Training Loss is Misleading",
    "text": "2. The Training Loss is Misleading\nBelow, we show the contextwise loss curves for two trained transformers. The average training loss of each model is given by a dotted line. The details of training are not relevant for this section3, so we will simply call them Model A and Model B. But an important difference between the two is that Model A is trained with 4 KiT of context, and Model B with 16 KiT.\n\n\n\n\nModel B has better training loss (2.244) than Model A (2.253). But do we truly prefer Model B? Note that Model A makes better predictions than Model B at every context length where they can be compared. Furthermore, Model A with a 4 KiT context reaches a lower loss than the 16 KiT model ever does. This means that at inference time, if we had 16 KiT of context available, we would be better off throwing away the first 12 KiT of context and feeding the remainder to Model A, instead of feeding all 16 KiT to Model B. Doing so would result in better predictions. In fact, there is no situation where we prefer Model B.\nWhy does the training loss mislead us? The training loss can be computed as the average of the contextwise loss curve, where the x-axis ranges from 1 to the training-context size. For a 16 KiT model, a much larger component of the training loss comes from situations where the model has a large amount of information in its context. For example, if we look at the proportion of the training loss coming from predictions with at least 3 KiT of context, we see that for Model A this is only 25%, whereas for Model B it is over 80%.\nThe upshot is: when comparing models trained with different context sizes, the training loss inaccurately ranks their performance. In order to select the optimal training-context size, we must find a more reliable metric to optimize.\nIntuitively, we want our metric to reflect the model’s ability to predict the next token at inference time. If we make the assumption that the users of the model have access to arbitrarily many tokens to put in the context, then a natural metric would be the lowest loss that the model attains at any context size. We refer to this as the best-context loss. To measure the best-context loss, compute the contextwise loss curve, and take its minimum.\n\n\nConsider, for example, the common practice of “prompting” a chatbot: pre-placing tokens into the context ahead of the user’s query. Conventional wisdom holds that longer and more thorough prompts improve final performance. If a maximum-length prompt is always utilized, our assumption is fulfilled, and best-context loss drives performance.\nIn fact, since the transformer we’ve been working with uses rotary embeddings, we can evaluate it beyond its training context. And, with the LongCrawl64 dataset, we have data with long-term structure up to 64 KiT. Thus, we can extend the contextwise scaling plots up to 64 KiT:\n\n\n\n\nBeyond the context size used during training, there is a rapid deterioration of prediction ability. Clearly, this model does not generalize well to the beyond-train-context regime. We’ve observed this exact same phenomenon for transformers of all sizes trained on all sorts of context sizes.\n\n\nEven though there have many claims that language models can generalize beyond their training context [4]–[6], to the best of our knowledge, nobody has shown a model for which the loss on natural-language text monotonically decreases with the context size. We consider this to be the true criterion for “sequence length generalization”.\nThis empirical fact is unfortunate, but has a silver lining: it simplifies measurement of the best-context loss. For models that do not generalize beyond their training context, we can measure the best-context loss by simply reporting the loss at the largest context size seen during training.4 This is the approach that we take in this article. But note that it is merely a convenient heuristic, and is valid only when working with models that fail to generalize in this way."
  },
  {
    "objectID": "articles/compute-optimal-context-size/index.html#context-scaling-experiments",
    "href": "articles/compute-optimal-context-size/index.html#context-scaling-experiments",
    "title": "Compute-Optimal Context Size",
    "section": "3. Context Scaling Experiments",
    "text": "3. Context Scaling Experiments\nWith our experimental setting established, it is time to evaluate scaling trends for the train-context size of transformers. The basic experiment we conducted is: train GPT-2 + rotary embeddings + flash attention, for a variety of parameter counts (124 million, 354 million, 772 million, 1.6 billion) and a variety of train-context sizes (128 tokens, 256 tokens, 512 tokens, …, 64 KiT). Each training run used 8 H100 GPUs with data parallel, and ran for 160 GPU-hours. We kept the batch size (i.e. number of tokens per gradient step) constant, so that, as the context size ranged from 128 to 64KiT, the number of documents per update varied from 2048 to 8.\nThe results of this experiment are visualized in the plot below. The x-axis of is the context size used during training. The y-axis is the best-context loss. Every line corresponds to a different model size. The training resources (in terms of GPU hours) can be interactively controlled via the slider. The colored circles show the optimal train context at each model size, and the dashed line shows the overall optimum.\n\n\n\n\nYou can see that varying the context size tends draw a U-shaped curve at all resource levels. Picking too small or too large a context size results in severely degraded performance. By playing with the slider you can adjust amount of training resources and confirm that this trend holds generally.\nIt is clear from this data that for any model size we should grow the context size with the training resources. We can directly visualize this trend with a second plot. For each model size, we plot a line with the hours of training on the x-axis, and the optimal context size on the y-axis.5\n\n\n\n\nClearly, as more resources become available we should train our models using longer context sizes. Also, an interesting observation is that the optimal context size grows more slowly for larger models.\nSo far, we’ve just been looking at the optimal context size for a given model scale. What if we select for the optimal combination of model size and context size?\n\n\n\n\n\n\nIdeally, we would quantify these trends and propose scaling laws. This would merely require extending our methodology by a few additional orders of magnitude of model scale, and to sweep over a few other hyperparamters (e.g. learning rate). This is beyond our current capacity, and we cannot meaningfully extrapolate from existing experiments, so we leave quantitative context-size scaling laws to future work.\nAs we expected, we see that as resources grow one wants to increase both model size and train-context size. But, relative to the previous plot (where we held model size fixed), the growth of the optimal context size noticeably slows down. This seems to be a consequence of the fact that, with a larger GPU hour budget, we want to use larger model sizes, and the optimal context size for those larger models tends to grow slower."
  },
  {
    "objectID": "articles/compute-optimal-context-size/index.html#final-thoughts-on-context-scaling",
    "href": "articles/compute-optimal-context-size/index.html#final-thoughts-on-context-scaling",
    "title": "Compute-Optimal Context Size",
    "section": "4. Final Thoughts On Context Scaling",
    "text": "4. Final Thoughts On Context Scaling\nAt Manifest AI, we believe that context size is one of the most important bottlenecks the field of AI is facing. Many of the most important applications of the technology are just waiting to be unlocked once sufficiently-long-context models become available. Most likely, we will be surprised by which applications end up being most important, but here are some guesses as to what type of use cases will become possible at every context-size scale:\n\nkilotoken scale: Read & write emails. Hold a short chatbot-style conversation. Customize behavior with a prompt. Few-shot learning with a small number of examples.\nmegatoken scale: Write books. Review news articles. Read & edit code. Answer questions from a large scientific literature. Navigate web interfaces.\ngigatoken scale: Read everything tweeted in a day and summarize global opinion. Execute a full software engineering workflow. In-context learning of entire datasets (replacing fine-tuning). Solve complex mathematical problems by iteratively improving over many proof attempts.\nteratoken scale: Manipulate all the data created by a corporation (contracts, documents, emails, etc).\npetatoken scale: Coordinate the affairs of an entire society by integrating all information it produces.\n\nIn light of this astonishing potential, it is tempting to simply always train on the longest context that is computationally feasible. But, as our experiments indicate, naively increasing the train context merely leads to models which are massively under-performant – able to ingest long contexts but unable to use their contents to make good predictions. The goal is not merely to train on long contexts, but to efficiently train on long contexts, by finding a setting where long contexts are compute-optimal. This is what it will take to truly leverage vast context sizes.\nSuch a setting will likely require radical algorithmic and architectural changes. An example of research that has successfully pushed the context size frontier is flash attention [7]. The reason is that it can decrease the cost of training with long contexts. That is also why we are excited about linear transformers, which reduce the cost of training on a context of length \\(t\\) from \\(O(t^2)\\) to \\(O(t)\\). Another angle that seems important is to develop models that generalize beyond the training context, in the specific sense that the contextwise loss curve keeps improving beyond the context size used for training.\nWe hope that the mindset, methodology, and dataset introduced in this article will be helpful in progressing to the petatoken scale and beyond.\n\n  \n  Subscribe to be notified of new posts:"
  },
  {
    "objectID": "articles/compute-optimal-context-size/index.html#footnotes",
    "href": "articles/compute-optimal-context-size/index.html#footnotes",
    "title": "Compute-Optimal Context Size",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor full experimental details, see Section 3.↩︎\nOur approach can be contrasted with the common mindset of train models with the largest context that the training budget will permit.↩︎\nFor those who are curious: both models are 124 million parameters and were trained on LongCrawl64 for 50,000 steps.↩︎\nIn practice, we take the average loss for the final 10% of the training context, which is less noisy.↩︎\nThe optimal context size tends to jump around due to noise in the loss, so this plot is smoothed by taking the most common context size in any given window.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Manifest AI",
    "section": "",
    "text": "writing\n\n\n\nDate\nTitle\n\n\n\n\nJan 5, 2024\nOur Mission\n\n\nJan 5, 2024\nLinear Transformers Are Faster\n\n\nMay 16, 2024\nCompute-Optimal Context Size\n\n\n\n\n  \n  Subscribe to posts:\n  \n  \n  \n  \n\n\n\nabout us\nCarles Gelada and Jacob Buckman. We each have 8+ years of deep learning research at institutions like OpenAI, Google Brain, and Mila. Our research has been published at NeurIPS, ICLR, ICML, etc., and been cited 1000+ times. After five years of academic collaboration, we founded Manifest AI in March 2023, backed by Decibel.\n\n\njoin us\nWe are hiring core technical team members.\nThe role has elements of both software engineering and research. Responsibilities include implementing deep learning architectures, deriving algorithms, developing research infrastructure, running large-scale experiments, and interpreting and communicating results.\nWe will work well together if you are independent-minded, capable of self-teaching, and value thinking from first principles. Skills we are looking for include comfort with mathematics, strong communication, deep knowledge in areas like CUDA, XLA/MLIR, Jax, or distributed systems/HPC, and experience training large-scale deep learning models.\nWe do not care about formal credentials. If you share our vision and would like to get involved, please send an example of some technical work that you are proud of to 777b7a607577605479757a7d72716760757d3a777b79"
  }
]