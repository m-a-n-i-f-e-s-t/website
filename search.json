[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJan 5, 2024\n\n\nLinear Transformers Are Faster After All\n\n\nJacob Buckman, Carles Gelada\n\n\n\n\nJan 4, 2024\n\n\nOur Mission\n\n\nJacob Buckman, Carles Gelada\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "articles/linear-transformers-are-faster/index.html",
    "href": "articles/linear-transformers-are-faster/index.html",
    "title": "Linear Transformers Are Faster",
    "section": "",
    "text": "\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\Z}{\\mathbb{Z}}\n\\newcommand{\\N}{\\mathbb{N}}\n\\newcommand{\\sft}{\\text{softmax}}\n\\newcommand{\\List}{\\text{List}}\n\\newcommand{\\Seq}{\\text{Seq}}\n\\newcommand{\\SeqT}{\\text{SeqT}}\n\\newcommand{\\CSeqT}{\\text{CSeqT}}\n\\newcommand{\\Dist}{\\text{Dist}}\n\\newcommand{\\SM}{\\text{SM}}\n\\newcommand{\\Fn}{\\text{Fn}}\n\\newcommand{\\Tok}{\\text{Tok}}\n\\newcommand{\\Aij}{ A_{[i,j]}}\n\\]\nIt is well-known that removing the exponential from the attention layer of a transformer allows for a recurrent reformulation, with computational cost that is linear instead of quadratic on context length [1]. One would expect that such an architecture would be far faster to train, especially when the context size is large. However, when training deep neural networks on hardware accelerators (e.g. GPUs), reductions in FLOPs do not always straightforwardly translate into practical speedups. Initial empirical experiments showed that large language models based on linear transformers train more slowly than classic transformers, and led many to dismiss the approach as nice-in-theory but unhelpful-in-practice [2].\nAt the moment, the conventional wisdom in the field is that a quadratic-cost algorithm with highly optimized hardware utilization (e.g. FlashAttention) gives the best training throughput [3]. But this is mistaken. In this post, we explain several different ways of implementing linear transformers and we show that, in fact, they can produce massive speed-ups.\nThe experiment below showcases the central takeaway. We compare the speed of an optimized transformer, a straightforward recurrent implementation of the linear transformer (as described in [1]), and an optimized linear transformer (described in Section 3). We see that, despite exhibiting better growth with respect to context size, the linear transformer trains much more slowly than the transformer in wall-clock time. In contrast, our algorithm is strictly faster than even the highly-optimized FlashAttention baseline, at all context and model sizes. At moderately large context sizes, this speedup has an larger impact than FlashAttention itself.\nBut speed is only one part of the picture. We also need to know whether linear transformers actually minimize the loss as well as standard transformers do. Unfortunately, as the next plot shows, directly converting GPT2 to use linear transformer layers hurts learning significantly.\nThese results are discussed in detail in Section 5, but in short: linear transformers seem to become increasingly unstable as the sequence length grows, negatively affecting learning. This means that our linear transformers are somewhat useless,1 as the positive impact from the speedup seen in long contexts is undermined by the negative impact of degraded learning.\nOther variants of linear transformers have been proposed that claim resolve these learning issues [5]–[11], but we do not survey them today. Since our main motivation in this post is to share our insights on how to implement efficient linear transformers we stick with the most natural variant, which is most closely analogous to standard transformers. In a future post, we will explain how to improve the learning of linear transformers, allowing us to efficiently train unprecedentedly-long-context language models with super-fast sampling."
  },
  {
    "objectID": "articles/linear-transformers-are-faster/index.html#linear-transformers",
    "href": "articles/linear-transformers-are-faster/index.html#linear-transformers",
    "title": "Linear Transformers Are Faster",
    "section": "1. Linear Transformers",
    "text": "1. Linear Transformers\nThe inputs to a transformer layer are sequences of \\(Q_i, K_i, V_i \\in \\R^d\\) of query, key and values, where \\(i\\) ranges from \\(1\\) to the sequence length \\(t\\). The outputs are a sequence \\(Y_i\\in \\R^d\\). The well-known formula for the transformer layer, first popularized by Vaswani et al [12], is: \\[\nY_i^\\text{Transformer} = \\sum_{j=1}^i e^{Q^T_i K_j} V_j\n\\] Here we are excluding the denominator of the softmax for simplicity of notation. Although the normalization provided by the denominator is important for good learning performance, it doesn’t have a major impact on the computational cost or implementation speed, which are our main focus here.\n\n\nEven though we omit the normalization for the mathematical exposition, it is included in our implementation for all experiments.\nThe formula for the linear transformer (LT) layer is quite similar: just change the term \\(e^{Q^T_i K_j} \\to  Q^T_i K_j\\) yielding \\[\nY_i^\\text{LinearTransformer} = \\sum_{j=1}^i Q^T_i K_j V_j\n\\]\n\n\nAll our experiments with linear transformers also include a normalization factor, which we empirically found to be important for learning performance. Drawing on [1], we divide each \\(Y_i\\) by \\(\\sum_{j=1}^i Q^T_i K_j\\) after eunsuring the sum is positive by making keys and queries live in the positive quadrant using softplus.\nThis layer is “linear” in that the outputs \\(Y\\) are linearly related to all of \\(Q\\), \\(K\\), and \\(V\\).2 From now on, we will omit the superscript of \\(Y_i^\\text{LinearTransformer}\\) and just write \\(Y_i\\). To begin our exploration of the computational cost of linear transformers, consider the following implementation.\ndef LT_attention(Q, K, V):\n    \"\"\"\n    Shapes of inputs are\n     Q: [t, d]  K: [t, d]  V: [t, d]\n    Shapes of outputs are\n     Y: [t, d]\n    \"\"\"\n    t, d = Q.shape\n    Y_list = []\n    for i in range(t):           # loop cost: O(t^2 d)\n        Y_i = zeros(d)\n        Q_i = Q[i]\n        for j in range(i):       # loop cost: O(id)\n            A_ij = inner(K[j], Q_i)  # cost: O(d)\n            Y_i += A_ij * V[j]   # cost: O(d)\n        Y_list.append(Y_i)\n    return stack(Y_list)\nAnyone who has worked with self-attention will recognize that this is a terrible implementation, since nested for-loops are not GPU-friendly. Don’t worry: in the next section, we will discuss implementations that parallelize computation, and thus run much faster on modern hardware. For now, the main point of this pseudocode is to highlight that first mode of computation, which we call attention formulation, has a FLOP cost of \\(O(t^2 d)\\).\nThe key to the massive speedups we saw in the introduction comes from leveraging linearity to restructure the computation. Consider the following factorization: \\[\nY_i = \\sum_{j=1}^i Q^T_i K_j V_j = \\underbrace{ \\left (  \\sum_{j=1}^i V_j  K_j^T\\right )}_{S_i} \\; \\; Q_i\n\\] Written in this form, we notice that the term labeled \\(S_i \\in \\R^{d\\times d}\\) can be thought of as a state summarizing all the relevant information up to time \\(i\\). It’s easy to rewrite into the following recurrent equations \\[\nY_{i} = S_i Q_i\n\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\nS_i = S_{i-1} + V_i K_i^T\n\\] where we assume \\(S_{0} = 0\\in \\R^{d\\times d}\\). Written in this form, we realize that a linear transformer is an RNN. We can also write pseudocode for this approach, which we call the state formulation, and analyze the cost:\ndef LT_state(Q, K, V):\n    \"\"\"\n    Shapes of inputs are\n     Q: [t, d]  K: [t, d]  V: [t, d]\n    Shapes of outputs are\n     Y: [t, d]\n    \"\"\"\n    t, d = Q.shape\n    S_i = zeros(d, d) # shape [d,d]\n    Y_list = []\n    for i in range(t):        # loop cost: O(t d^2)\n        S_i += outer(K[i], V[i]) # cost: O(d^2)\n        Y_i = S_i @ Q[i]      # cost: O(d^2)\n        Y_list.append(Y_i)\n    return stack(Y_list)\nWe see that the cost here is \\(O(t d^2)\\).\nSo, while a standard transformer layer always has cost \\(O(t^2 d)\\), linear transformers have two formulations with different costs. By switching from the attention formulation to the state formulation, we can change the cost from \\(O(t^2 d)\\) to \\(O(t d^2)\\), trading a \\(t\\) term for a \\(d\\) term."
  },
  {
    "objectID": "articles/linear-transformers-are-faster/index.html#parallel-implementations",
    "href": "articles/linear-transformers-are-faster/index.html#parallel-implementations",
    "title": "Linear Transformers Are Faster",
    "section": "2. Parallel Implementations",
    "text": "2. Parallel Implementations\nIn general, for-loops are a terrible way to implement anything that will run on a GPU. When using high-level frameworks like PyTorch or JAX, the easiest way to get high GPU utilization is to only use primitives that are already highly optimized for GPU: matrix multiplication, elementwise operations, etc. We can rewrite our attention and state algorithms in this style to make them efficient.\nFirst, let’s do this for attention. Our main technique is to compute the attention matrix \\(A\\), which contains all the terms outer(Q[i], K[j]) that appeared inside the for-loops of LT_attention, using a single heavyweight matrix multiply.\ndef LT_attention_parallel_no_flash(Q, K, V):\n    \"\"\"\n    Shapes of inputs are\n     Q: [t, d]  K: [t, d]  V: [t, d]\n    Shapes of outputs are\n     Y: [t, d]\n    \"\"\"\n    t = Q.shape[0]\n    M = causal_mask(t)\n    A_raw = Q @ K.T  # cost O(t^2 d)\n    A = A_raw * M    # cost O(t^2)\n    Y = A @ V        # cost O(t^2 d)\n    return Y\nThis implementation lies at the core of nearly every transformer of the past five years, powering all of the AI models that have recently exploded in popularity. Since it is composed of such a small number of ultra-parallelizable ops, it obtains high GPU utilization. Recently, specialized flash attention kernels [3] have been used to get even further speedups by avoiding explicitly storing the attention matrix \\(A\\), and thereby saving both memory and time spent on memory-transfers. Algorithmically, though, flash attention is a variant of parallel attention, and has the same computational cost (as measured in FLOPs). We use LT_attention_parallel to refer to the flash attention implementation.\nNext, let’s parallelize the recurrent formulation. This optimization is less well-known. The key is to compute all the terms \\(V_i K^T_i\\) in parallel, and then use a cumulative-sum, which can be parallelized, to combine them.\ndef LT_state_parallel(Q, K, V):\n    \"\"\"\n    Shapes of inputs are\n     Q: [t, d]  K: [t, d]  V: [t, d]\n    Shapes of outputs are\n     Y: [t, d]\n    \"\"\"\n    P = V[:,:,None] @ K[:,None,:]  # cost: O(t d^2)\n    S = cumsum(P, axis=0)          # cost: O(t d^2)\n    Y = S @ Q[:,:,None]            # cost: O(t d^2)\n    return Y[:,:0]\nThe cost in FLOPs of this algorithm is \\(O(t d^2)\\).\nNow that we have four mathematically-equivalent implementations of a linear transformer, let’s time the training step of our GPT2 models and see how big of a difference it makes. For our LT_attention_parallel implementation, we use a custom linear self-attention flash kernel we implemented in Triton [13] based on OpenAI’s FlashAttention2 implementation.\n\n\n\n\nHere are some takeaways:\n\nAs expected, the attention variants all have a quadratic asymptotic cost (slope of 2 on a log-log plot3). The state variants all have linear asymptotic cost (slope 1). 4\nLT_state_parallel is an order-of-magnitude faster than LT_state.\nLT_attention_parallel_no_flash is two orders-of-magnitude faster than LT_attention.\nLT_attention_parallel seems to asymptotically stabilize into being an order-of-magnitude faster than LT_attention_parallel_no_flash.\nFor the majority of settings, LT_attention_parallel is the fastest. (This is the linear version of the algorithm used by the standard transformer.)\nParallel attention is the fastest algorithm for small context sizes. However, LT_state_parallel overcomes LT_attention_parallel_no_flash at around 13k context size, and overcomes LT_attention_parallel at around 100k.\n\nOverall, these results paint a clear picture: use the attention algorithm for small contexts and the state algorithm for large contexts. But do we really face a binary choice? Can we combine the state and attention ideas and get the best of both words?"
  },
  {
    "objectID": "articles/linear-transformers-are-faster/index.html#chunked-formulation",
    "href": "articles/linear-transformers-are-faster/index.html#chunked-formulation",
    "title": "Linear Transformers Are Faster",
    "section": "3. Chunked Formulation",
    "text": "3. Chunked Formulation\nIt’s evident that, for small context sizes, computing the \\(t\\) by \\(t\\) attention matrix is much more efficient than computing many \\(d\\) by \\(d\\) state matrices. But as \\(t\\) grows, there is a point where the quadratic cost of the attention matrix ends up dominating. Noticing that attention is extremely efficient for small \\(t\\) and that states are necessary for large \\(t\\) motivates doing one last reworking of the LT equation.\nLet \\(c \\in \\N\\) be a positive integer that we’ll call the chunk size. For any \\(i\\in \\N\\) find the unique \\(n\\in \\Z\\) s.t. \\(cn &lt; i \\le c(n+1)\\). We can easily see that the following equations are equivalent to the previous ones. \\[\nY_{i} = S_{cn}Q_i + \\sum_{j=cn+1}^i Q_i^T K_j V_j\n\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\nS_{c(n+1)} = S_{cn} + \\sum_{j=cn+1}^{c(n+1)} V_j K_j^T\n\\] The key idea is that we are only going to compute a subset of all states: \\(S_0, S_c, S_{2c}, \\cdots\\). Then, to compute each output \\(Y_i\\), we need only to take into account the contribution via the most recent state \\(S_{cn}\\), as well as the contribution (computed via attention) of all moments in time \\(j\\) in the range \\(cn &lt; j \\le i\\).\nAs pseudocode, this looks like:\ndef LT_attention_with_initial_state(S, Q, K, V):\n    \"\"\"\n    Shapes of inputs are\n     S: [d, d]  Q: [c, d]  K: [c, d]  V: [c, d]\n    Shapes of outputs are\n     Y: [c, d]\n    \"\"\"\n    Y_state = Q @ S                               # cost O(c d^2)\n    Y_attention = LT_attention_parallel(Q, K, V)  # cost O(c^2 d)\n    Y = Y_state + Y_attention                     # cost O(cd)\n    return Y\n\ndef LT_chunked(Q, K, V, c):\n    \"\"\"\n    Shapes of inputs are\n     Q: [t, d]  K: [t, d]  V: [t, d], c: int\n    Shapes of outputs are\n     Y: [t, d]\n    \"\"\"\n    t, d = Q.shape\n    assert t % c == 0\n    Q_, K_, V_ = [arr.reshape(t//c, c, d)\n    `               for arr in [Q,K,V]]\n    P_ = K_.transpose([0,2,1]) @ V_  # cost O(t d^2)\n    S_ = cumsum(P_, axis=0) - P_     # cost O((t/c)d^2)\n    Y_ = vmap(LT_attention_with_initial_state, axis=0)(\n                S_, Q_, K_, V_)      # cost O(td^2 + tcd)\n    return Y_.reshape(t, d)\nThe cost is \\(O\\left(td^2 + tcd\\right)\\), once again avoiding a quadratic dependency on \\(t\\). Also, note that this algorithm makes an inner call to LT_attention_parallel, so we can use a flash-attention kernel to do that part of the computation.\nThis algorithm has a hyperparameter, the chunk size, which must be set correctly for optimal performance. Fortunately, it is inexpensive to identify the best chunk size empirically, since measuring just a few steps of training at each chunk size is sufficient to identify which is the fastest. In the plot below, we plot the speed of the optimally-chunked algorithm in each setting.\n\n\n\n\nWe see LT_chunked gives the desired best-of-both-worlds behavior, as it is equal-or-better than all other approaches in all settings. And we now see the massive (& rapidly growing) speedup relative to standard self-attention, finally unlocking the true power of linear transformers."
  },
  {
    "objectID": "articles/linear-transformers-are-faster/index.html#sampling",
    "href": "articles/linear-transformers-are-faster/index.html#sampling",
    "title": "Linear Transformers Are Faster",
    "section": "4. Sampling",
    "text": "4. Sampling\nWhen working with language models, efficient training is not the only performance that deserves consideration. Once the model is trained, how expensive is it to utilize? When we are sampling we have a sequence of tokens, \\(z_1 \\cdots z_t\\), and we want to sample the next token, \\(z_{t+1}\\).\nThe most efficient algorithm to sample from traditional transformers is called the KV-cache algorithm [14]. This algorithm assumes that when we generate token \\(z_{t+1}\\), we will have already computed and cached all the \\(K_i, V_i\\) for all \\(0 \\le i \\le t\\). In order to compute the output of the attention layer at time \\(t+1\\) given this cached information, we can use \\[\nY_{t+1}^\\text{Transformer} = \\sum_{j=1}^{t+1} e^{Q^T_i K_j} V_j\n\\] It is easy to see that this is an \\(O(td)\\) operation. In other words, as the sequence length grows, sampling each subsequent token becomes more computationally expensive.5 This is one of the major limitations of the classic transformer architecture.\nWith linear transformers, however, we have access to the recurrent formulation. A linear transformer is an RNN where the state has size \\(O(d^2)\\). \\[\nY_{i} = S_i Q_i\n\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\nS_i = S_{i-1} + V_i K_i^T\n\\] We can compare the time it takes to generate any particular token when sampling a sequence:\n\n\n\n\nAs expected, we see that the time to sample of the linear transformer is independent of context length, whereas that of the KV-cache transformer grows linearly. This leads to a large gap in inference speed at large contexts.6"
  },
  {
    "objectID": "articles/linear-transformers-are-faster/index.html#learning-performance",
    "href": "articles/linear-transformers-are-faster/index.html#learning-performance",
    "title": "Linear Transformers Are Faster",
    "section": "5. Learning Performance",
    "text": "5. Learning Performance\nUntil now, our focus has been on how to implement linear transformers efficiently. But an architecture that runs really fast is useless unless it also learns well. We will now compare the learning curves of the GPT2 baselines with their linear transformer counterparts, at various context sizes.\nIn order to control for the fact that longer contexts help learning by introducing more tokens per update, we hold the number of tokens-per-update constant across context sizes by decreasing batch size. At all context-lengths, each update sees \\(2^{19}\\) tokens.7 Importantly, for this set of experiments, we have used the dataset c4 [4], which does not have much long-term structure: it consists of a shuffled collection of short articles, of average length ~500 tokens.\nFirst, let’s look at the parameter scaling law of GPT2 and our modified Linear-GPT2. The following plot shows the final performance after 24h of training on our 8xH100 server for each scale and each context size. Click the second tab if you want to see the full learning curves.\n\n\n\n\nBoth architectures scale similarly with respect to model size, but there is a gap in performance. This gap seems to grow as context size increases, together with more loss spikes and general instability. It’s possible that the gap can be explained by the linear transformer not effectively utilizing its context. To explore whether or not this is the case, we can use these same experiments, but visualize all context-lengths together.\n\n\n\n\nWe see a dramatically different trend between the two architectures. For GPT2, each increase in context length slows down the initial pace of learning, but ultimately all context-lengths (beyond at least 1024) converge to a similar final loss. Whereas for Linear-GPT2, not only does increasing the context slow down learning in the beginning, but it also causes convergence to a worse final performance and nasty-looking loss spikes.\nThe results for GPT2 are what we would expect from a healthy model, given the setting. Note that since our dataset consists of short articles, of average length ~500 tokens, we can assume that even a context window of 1024 would include nearly everything relevant. Thus, we wouldn’t expect that increasing the context length beyond this point would decrease the loss the model ultimately converges to. Longer-context models do however need to learn to ignore many more irrelevant tokens, explaining the slowed initial learning.8\nIn contrast, the results for Linear-GPT2 seem to indicate some underlying instability of the linear transformer architecture. It doesn’t even seem capable of ignoring irrelevant information, let alone exploiting useful long-term structure.\nRemedying these learning issues will be the main focus of our future work in linear transformers. Our current architecture is essentially a one-to-one clone of GPT2, sticking as architecturally close to the original as possible; aspects such as initializations, normalizations and hyperparameters were directly copied. It is well-known that these decisions can have a large impact on scaling and stability and often need to be tuned in an architecture-specific way. In the literature, various other linear variants of self-attention have been proposed, that incorporate techniques such as gating mechanisms in order to improve stability and learning [5]–[11]. A future post will include a thorough study of the impact of all of these choices.\nUltimately, it may be impossible for any linear transformer to perfectly match the context scaling laws of the classic transformer baseline. Although removing the exponential seems like a minor change, it represents a meaningful decrease in expressivity.9 But as we’ve shown, linear transformers can be trained orders-of-magnitude more efficiently. On equivalent compute, linear transformers can be trained for many more steps; or, keeping steps constant as well, we can train a much larger model. If this additional scaling is sufficient to compensate for the reduced expressivity, linear transformers will be the more efficient architecture overall.\n\n  \n  Subscribe to be notified of new posts:"
  },
  {
    "objectID": "articles/linear-transformers-are-faster/index.html#footnotes",
    "href": "articles/linear-transformers-are-faster/index.html#footnotes",
    "title": "Linear Transformers Are Faster",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs discussed in Section 4, a second benefit of linear transformers is that the cost to sample a token does not grow with context size. Perhaps one could argue that this improvement in sampling speed could, on its own, justify using linear transformers for applications where the inference costs vastly exceed training costs. But it is evident to us that, for linear transformers to become actually useful, we need to address these instability issues.↩︎\nIt is not named for the fact that the computational cost is linear with respect to \\(t\\)! That is just a coincidence. (And is not even always true, as we will see.)↩︎\nIf \\(y=x^2\\), a log-log plot where \\(y'=\\log_a(y)\\) and \\(x'=\\log_a(x)\\) for any base \\(a\\), then \\(y'=\\log_a(y) = \\log_a(x^2) = 2 \\log_a(x) = 2 x'\\). So the graph will be a line with slope 2.↩︎\nThe reason we see the expected slopes asymptotically is that we are timing a full GPT2 architecture which has many other components besideds the attention layer. If we were only timing the attention layer, the plots would all be straight lines.↩︎\nAn interesting connection is that the KV-cache can be understood as the state of an RNN with non-constant state size; namely, one whose state-size is \\(O(td)\\).↩︎\nThis comparison may not be completely fair. In these experiments, our implementation of neither sampling algorithm makes use of specialized kernels. A lot of the ideas of flash attention can be used to write a much faster KV cache sampling algorithm; on the other hand, it’s unclear if much improvement is possible on the recurrent sampling. Thus, it’s possible that with engineering effort the gap between the two algorithms could become smaller. However, the overall pattern will certainly remain the same.↩︎\ne.g. runs with context-size 1024 would have batch-size of \\(2^{19} / 2^{10} = 2^{9} = 512\\).↩︎\nPut another way: doubling the size of the input vastly increases the size of the function space over which gradient descent must search, and it’s intuitive that in a larger space it takes somewhat longer to find a good solution.↩︎\nWe plan to elaborate on this topic in a future blog post.↩︎"
  },
  {
    "objectID": "articles/rnn-state-scaling/index.html",
    "href": "articles/rnn-state-scaling/index.html",
    "title": "RNN State Scaling",
    "section": "",
    "text": "TODO write intro\nIn this post we go over how to think about RNN state size. We find an architecture, symmetric power, that looks very promising."
  },
  {
    "objectID": "articles/rnn-state-scaling/index.html#background",
    "href": "articles/rnn-state-scaling/index.html#background",
    "title": "RNN State Scaling",
    "section": "1. Background",
    "text": "1. Background\nWe begin by introducing some conceptual scaffolding to connect important ideas around transformers and RNNs.\nAll auto-regressive sequence models can be thought of as state machines (SMs). A state machine is a generic abstract model for any system that takes in an input and a state, and gives an output and updated state. In the case of neural language modeling, the inputs are typically tokens, the outputs are typically next-token distribution predictions, and the states are typically arrays of floating-point numbers.\nLet \\(\\mathcal{X}, \\mathcal{Y}, \\mathcal{S}\\) denote the input space, output space, and state space, respectively. A state machine is a triplet \\((S_0, \\delta, \\rho)\\), where \\(S_0 \\in \\mathcal{S}\\) denotes the initial state, \\(\\delta \\in (\\mathcal{S} \\times \\mathcal{X}) \\to \\mathcal{S}\\) denotes the state transition function, \\(\\rho \\in (\\mathcal{S} \\times \\mathcal{X}) \\to \\mathcal{Y}\\) denotes the output function, and \\[\n\\begin{aligned}\nS_t &:= \\delta(S_{t-1}, X_t)\\\\\nY_t &:= \\rho(S_{t}, X_t)\\\\\n\\end{aligned}\n\\] where \\(X_t\\) and \\(Y_t\\) denote the input and output at time \\(t\\), respectively.1 This simple but powerful abstraction encapsulates every modern language model, including models based on the standard softmax-based transformer, RNNs (e.g. LSTMs, GRUs, linear transformers), and state-space models like Mamba. At first glance, it may be surprising that transformers are included in this list; transformer language models are not typically presented as state machines. See Appendix A.1. for a constructive proof.\nWe are further interested in a more specific concept: a finite state machine (FSM). We use \\(\\operatorname{size}\\) to denote the function which returns the dimensionality of an object. In the context of this article, we define a finite state machine as a state machine with the property that there exists \\(\\varsigma \\in \\mathbb{N}\\) such that \\(\\operatorname{size}(S) \\leq \\varsigma\\) for all \\(S \\in \\mathcal{S}\\). We call \\(\\varsigma\\) the state size.2\nIt is easy to see that the prototypical RNN is a finite state machine: each state \\(S \\in \\mathcal{S}\\) is a fixed-and-finite-dimensional array.3 Furthermore, it can be shown that transformers are not finite state machines (see Appendix A.2). Intuitively, this is because the state size of a transformer grows with the length of its input history, so no fixed \\(\\varsigma\\) is larger than every possible states.\nThis paints a clear picture of the relationship between transformers and RNNs. The two approaches can be unified under the umbrella of state machines, but the crucial distinction between them is that transformers do not have a finite-sized state. From this perspective, it becomes clear that increasing the state size of an RNN will make it more similar to a transformer (and in fact, in Sections 2.2 and 2.5 we give families of RNNs that converge exactly to the transformer in the limit of increasing state size). Noting the performance gap between transformers and RNNs motivates a research question: can the performance of an RNN be improved by increasing its state size?\nThe remaining sections of article will answer that question in the affirmative, showing that the performance of RNNs with large states matches, and sometimes surpasses, that of transformers."
  },
  {
    "objectID": "articles/rnn-state-scaling/index.html#state-scaling-of-rnns",
    "href": "articles/rnn-state-scaling/index.html#state-scaling-of-rnns",
    "title": "RNN State Scaling",
    "section": "2. State Scaling Of RNNs",
    "text": "2. State Scaling Of RNNs\nLet’s take a look at some sequence modeling architectures through the lens of state size scaling. The table below gives an overview of what we will discover.\n\n\n\n\n\n\n\n\n\n\nRNN Architecture\nAdjustable State-Weight Ratio\nUnlimited Effective Context Window\nConverges To Transformer\nState-Weight Ratio\n\n\n\n\nFeedforward RNN\n✗\n✓\n✗\n\\(\\frac{1}{2d}\\)\n\n\nWindowed Transformer\n✓\n✗\n✓\n\\(\\frac{w}{6d + 1}\\)\n\n\nLinear Transformer\n✗\n✓\n✗\n\\(\\frac{d/head}{12d + 2}\\)\n\n\nSplit-Product Transformer\n✓\n✓\n✗\n\\(\\frac{(d/(head\\cdot s))^s}{12d + 2}\\)\n\n\nTaylor Transformer\n✓\n✓\n✓\n\\(\\frac{\\sum_{i=0}^{D}\\binom{\\frac{d}{head} + i - 1}{i}}{12d + 2}\\)\n\n\nSymmetric Power Transformer\n✓\n✓\n✗\n\\(\\frac{\\binom{\\frac{d}{head} + D - 1}{D}}{12d + 2}\\)\n\n\n\nAll architectures we discuss in this article have the same high-level structure. Letting \\(d \\in \\mathbb{N}\\) denote the width of a network and \\(\\ell \\in \\mathbb{N}\\) to denote the depth, each architecture has an embedding layer mapping tokens to \\(\\mathbb{R}^d\\), followed by \\(\\ell\\) copies of an RNN with \\(\\mathcal{X} = \\mathcal{Y} = \\mathbb{R}^d\\), and finally an outbedding layer mapping \\(\\mathbb{R}^d\\) to next-token distributions. Since this structure is shared, the section studying each article focuses only on the inner RNN, where we use \\(s\\) to denote the number of splits in Split-Product Transformer, and use \\(D\\) to denote the degrees of power in Taylor Transformer and Symmetric Power Transformer.\n\n2.1. Feedforward RNN\nA natural place to begin is with the classic feedforward RNN layer, defined as:\n\\[\n\\begin{aligned}\nS_0 &:= \\textbf{0} \\in \\mathbb{R}^d\\\\\n\\delta(S_{t-1}, X_t) &:= \\sigma(W_{\\delta}(S_{t-1} + X_t))\\\\\n\\rho(S_{t}, X_{t}) &:= W_{\\rho}S_t\n\\end{aligned}\n\\]\nwhere weights \\(W_{\\delta}, W_{\\rho} \\in \\mathbb{R}^{d \\times d}\\) and \\(\\sigma\\) denotes a nonlinearity like ReLU.\nSince \\(S \\in \\mathbb{R}^d\\) for all \\(S \\in \\mathcal{S}\\), the state size of each layer of this network is \\(d\\). The overall state size is therefore \\(d\\ell\\). If we want to increase the state size, we need to increase the width \\(d\\) or the depth \\(\\ell\\). But notice that, since the parameter count is \\(2d^2\\ell\\), increasing the width or depth also increases the number of parameters. This is problematic for two reasons, one scientific and one practical.\nScientifially, we were hoping to answer the question of does increasing the state size improve performance? This motivates an experimental design where we increase the state size and measure the change in performance. But if, when we increase state size, we also increase parameter count, our experiment becomes unable to disentagle the impact of increasing the size of the state from the familiar performance boost that comes with increasing the model scale.\nOn the practical side, the concern is best illustrated by highlighting a quantity we call the state-weight ratio. For the classic RNN, this is \\(\\frac{d\\ell}{2d^2\\ell} = \\frac{1}{2d}\\). This ratio is small and we have no way of increasing it. As we increase the state size by increasing \\(d\\), this architecture will actually spend a smaller fraction of its memory on its state. In order to design an RNN to have a similar parameter scaling law to that of a transformer, we need it to have not only large states, but a large state-weight ratio.\n\n\n2.2. Windowed Transformer\nIn the this section, we study an RNN architecture that allows us to freely adjust the state-weight ratio. The formula for a transformer layer is:\n\\[\nY^\\text{Transformer}_t = \\sum_{i=1}^t e^{Q^T_t K_i} V_i\n\\]\nIn Appendix A.1, we construct an equivalent state machine with \\(S_t = (K_{i=1}^t, V_{i=1}^t)\\). This means that \\(\\operatorname{size}(S_t) = 2td\\ell\\), which, being dependent on \\(t\\), has no upper bound.\nHowever, simply truncating the state beyond a certain size straightforwardly leads to a transformer-like RNN, which we call a windowed transformer. The truncation is given by a window size hyperparameter, which we denote \\(w \\in \\mathbb{N}\\). Its formula is:\n\\[\nY^\\text{WindowedTransformer}_t = \\sum_{i=t-w}^t e^{Q^T_t K_i} V_i\n\\]\nIt is easy to see that this is a state machine with \\(S_t = (K_{i=t-w}^t, V_{i=t-w}^t)\\), and is therefore a finite state machine with state size \\(2wd\\ell\\). The implementation is almost exactly the same as for a regular transformer, but in addition to a causal mask, it also includes an “ancient history mask”.\n[Visualization]\n\n\nIt is interesting to note that a naive implementation of windowed transformer attention (one which materializes the entire attention matrix) has cost \\(O(T^2)\\) to process a sequence of length \\(T\\) – just like a regular transformer. Running this architecture in \\(O(T)\\) requires an efficient fused implementation which avoids materializing (or even computing) segments of the attention matrix that are entirely masked out.\nBy adjusting \\(w\\), we can control the state size without affecting the parameter count. Using a GPT-2-style architecture, the (non-embedding) parameter count for a transformer is \\((2d + 12d^2)\\ell\\) (see Appendix A.3 for a breakdown). The state-weight ratio is therefore \\(\\frac{2wd\\ell}{(2d + 12d^2)\\ell} = \\frac{w}{6d + 1}\\). Any desired state-weight ratio can be achieved.\nThis gives us the ability to empirically observe the state scaling behavior of the windowed transformer. All experiments in this article were performed using a 124M parameter GPT-2 architecture, on LongCrawl64 with a context length of 4096.\n[PLOT]\n[JAX code]\nAs expected, we see that state scaling improves performance.\nSince the documents in our training set have context length of 4096 tokens, setting \\(w = 4096\\) causes a windowed transformer to exactly match the performance of the baseline transformer. Also, there is no value in increasing the state size beyond \\(w &gt; 4096\\). (On a longer-context dataset, there would be.)\nOne important perspective on windowed transformers is that they are a family of RNNs that converge to the regular transformer, in the limit as \\(w \\to \\infty\\). In Section 2.5, we will see another such family, which approaches the regular transformer but in a very different way.\n\n\n2.3. Split-Product Transformer\nIt is intuitively clear that a limitation of the windowed transformer architecture discussed in the previous section is that it has a limited effective context window, meaning that there are some tokens in the context which do not in any way impact the model’s prediction. A windowed transformer with \\(\\ell\\) layers and a window size \\(w\\) takes into account only the most recent \\(\\ell(w - 1)\\) tokens. We will show empirically in Section 3.2 how this limitation can impact performance. This motivates us to continue exploring the space of architectures, to find one that has both an unlimited effective context window (as the feedforward RNN has) and an adjustable weight-state ratio (as the windowed transformer has).\nThe linear transformer from our previous article is not that architecture, but it gives us a good starting point. The formula for the linear transformer is:\n\\[\nY_t^\\text{LT} = \\sum_{i=1}^t Q^T_t K_i V_i\n\\]\nThis can be generalized slightly by introducing \\(\\phi : \\mathbb{R}^d \\to \\mathcal{Z}\\), an embedding function which maps keys/queries into an inner product space \\(\\mathcal{Z}\\). The nice properties of linear transformers that we demonstrated in our previous article (specifically, the existence of an \\(O(t)\\) state algorithm) remain universally present for architectures in this more general family (i.e. for any choice of \\(\\phi\\)).\n\\[\nY_t^\\text{GenericLT} = \\sum_{i=1}^t \\langle \\phi(Q_t),  \\phi(K_i) \\rangle V_i \\\\\nY_{t}^\\text{GenericLT} = S_t \\phi(Q_t) \\qquad S_t = S_{t-1} + V_t  \\phi(K_t)^T\n\\]\nOf course, this reformulation is superficial.4 If we consider \\(\\phi(K_i)\\) to be part of the broader layer rather than the attention calculation, this reduces back to the original equations of the linear transformer. The advantage of thinking of \\(\\phi\\) as part of the transformer itself is that it allows us to study variants of \\(\\phi\\) while keeping the rest of the network exactly the same. In particular, we will be looking at choices of \\(\\phi\\) where \\(z \\gg d\\), which we call state expansions. State expansions allow us to freely adjust the state-weight ratio, and we will see that larger state expansions generally improve performance.\nWhat state expansion should we pick? It’s wide open, and many things have been tried [TODO cite stuff]. In this article, we focus on state expansions that leverage the tensor product, an approach that we consider to be particularly mathematically elegant. Intuitively, the tensor product can be seen as a generalization of the outer product to higher-order objects. (See Appendex A.4 for a more complete introduction to the topic.)\nHere is a straightforward way to use the tensor product to define a state expansion. Given a vector-valued key (or, wlog, query), first split the key into \\(p \\in \\mathbb{N}\\) evenly-sized pieces, and then take the tensor product of those pieces. Formally, we define the split-product \\(\\phi\\) as:\n\\[\n\\chi_i(K_t) = \\text{softplus}(K_t)_{\\left[\\frac{(i-1)d}{p} : \\frac{id}{p}\\right]} \\qquad \\phi^\\text{SplitProduct}(K_t) = \\bigotimes_{i=1}^p \\chi_i(K_t)\n\\]\nWith this embedding function, \\(\\phi^\\text{SplitProduct}(K_t) \\in \\mathbb{R}^{\\left(\\frac{d}{p}\\right)^p}\\). There is no way to represent this besides representing each component of \\(\\phi^\\text{SplitProduct}(K_t)\\) so the state size is \\(\\left(\\frac{d}{p}\\right)^p\\). (PROOF!?!!) As we increase \\(p\\), we get exponential growth in state size.\nTo implement the attention algorithm for \\(\\phi^\\text{SplitProduct}\\), there is no need to materialize the expanded state. Simply split both the key and the query, do a dot product on each piece, and then multiplying the results together. (PROOOF?)\n\\[\nY_t^\\text{SplitProduct} = \\sum_{i=1}^t \\left( \\prod_{i=1}^p \\chi_i(Q_t)^T \\chi_i(K_t) \\right) V_t\n\\]\nLet’s run some experiments to investigate the effect of the split-product state expansion on performance.\n[PLOT]\n[JAX code]\nAs expected, we see that state scaling improves performance.\n\n\n2.4. Symmetric Power Transformer\nThe split-product approach is a straightforward enough application of the tensor power, but in some ways it is a bit awkward: splitting up the keys means that \\(p\\) is required to divide \\(d\\) evenly, and the possibility of negative dot-products means we still need to use the softplus to push everything into the positive quadrant in order to normalize later.\nThere is a somewhat nicer variation, which we call this the symmetric power expansion. In this approach, we simply tensor the key with itself an even number of times. This obviates the need for awkward splitting, and guarantees that each dot-product term will be positive even when keys are not projected into the positive quadrant. The symmetric power formula is:\n\\[\n\\phi^\\text{SymmetricPower}(K_t) = \\bigotimes_{i=1}^p K_t\n\\]\nNaively, we might assume that the size of this state is \\(d^p\\). But actually, we can exploit symmetry: many terms are repeated. The real state size is \\(\\binom{d + p - 1}{p}\\). (PROOF!!!!!)\nThe attention algorithm for the symmetric power is pleasingly straightforward. Simply replace exponentiation operation with raising-to-\\(p\\):\n\\[\nY_t^\\text{SymmetricPower} = \\sum_{i=1}^t \\left( \\chi_i(Q_t)^T \\chi_i(K_t)^p \\right) V_t\n\\]\nLet’s run some experiments to investigate the effect of the symmetric power state expansion on performance.\n[PLOT]\n[JAX code]\nAs expected, we see that state scaling improves performance. Impressively, it matches the performance of the softmax baseline by \\(p=4\\), and surpasses it slightly at \\(p=6\\).\n\n\n2.5. Transformer (Taylor’s Version)\nThe previous two approaches each have an adjustable state-weight ratio and no maximum effective context size. But both are still missing one cool aspect of the windowed transformer: the guarantee that, in the limit of large state, it approaches the transformer. This third property is not needed for good performance, so a purely-practical-minded person might not care. But it is deeply insightful to think about what such an architecture would look like.\nStart off with a linear transformer, and let \\(\\phi\\) equal the first \\(n\\) terms of the Taylor series approximation of the exponential. This is the the sum of \\(n\\) different symmetric powers, so the state size is \\(\\sum_{p=1}^{n} \\binom{d + p - 1}{p}\\). (PROOF!!!!)\nAs we increase \\(n\\), the state size of the model grows, and we approach the original transformer.\nThis gives a second perspective on the idea that a transformer is a state machine, but not a finite state machine. Rather than thinking about the transformers as having a growing state (as in the KV cache perspective), we can also think about transformers as having an infinite-dimensional state."
  },
  {
    "objectID": "articles/rnn-state-scaling/index.html#comparing-state-scaling-curves",
    "href": "articles/rnn-state-scaling/index.html#comparing-state-scaling-curves",
    "title": "RNN State Scaling",
    "section": "3. Comparing State Scaling Curves",
    "text": "3. Comparing State Scaling Curves\nWe’ve gone on a wonderful journey though architecture land, looking at lots of qualitative differences between them. But now it is almost time to answer the question on everyone’s mind: which architecture is the best?\nBefore diving in, let’s take one more moment to reflect on what we are looking for in an architecture.\n\n3.1. Why State Scaling?\nUltimately, the goal of any training run is to reach the best performance given training budget. The most relevant metric would therefore be to directly measure the impact of this increased cost by comparing models on the basis of their performance-per-GPU-hour. But this sort of comparison is difficult to get right. Firstly, performance-per-GPU-hour is highly implementation-dependent. Without an efficient fused kernel for the chunked algorithm, any RNN will inevitably fall far short of its potential; but it is challenging and time-consuming to implement these kernels. Secondly, compute-optimal performance relies on jointly choosing the state size with the model size, context length, etc., a process which requires an expensive sweep over various combinations.\nIn light of these difficulties, it is convenient to have a simpler experimental design. As we have seen, increasing the state size improves performance per step of training. It also increases the cost, since a larger state means more computation per step. The computation per step is roughly proportional to the size of the state for most relevant architecutures, since each dimension of the state needs to be read, processed, and written at least once.5 State size is therefore a reasonable heuristic to use in place of wall-clock time, and so it makes sense to focus on architectures which have high growth in performance per dimension of state.\nSince the size of the state for any architecture can be computed in closed form, we do not even need to implement the state or chunked algorithms in order to assess quality on the basis of state size scaling laws. A simple attention implementation is all that is needed. This makes it easy to quickly iterate on architectural chocies, and only implement the difficult chunked algorithm for architectures that seem promising.\nTo summarize: our goal in this section is to identify which architectures (if any) we want to spend our time implementing efficiently, by comparing their state scaliing laws.\n\n\n3.2. Empirical Results\nBelow on the left you can see the state size scaling curves for all the architectures we described today. On the right, click the names of individual architectures to see their training curves.\n[PLOT]\nThere are two clear winners here: the windowed transformer and the symmetric power transformer.\n\n\n3.3. Data Dependence\nAn important consideration for these experiments is the choice of dataset. In theory this is always the case: for example, when Chinchilla computes its optimal model sizes, it’s doing so on the basis of scaling laws extrapolated for one particular dataset, and these may or may not hold on other datasets. Fortunately, for the vast majority of deep learning work, it really does not seem to matter. Evidence seems to indicate that the ordering of architectures is pretty consistent accross datasets (TODO CITE THIS), and it’s difficult to see how one might intentionally construct datasets that are more advantageous to one architecture vs another.\nWhen it comes to RNNs, this is not the case. There are some strong and easily visible data dependence effects. An obvious one in this case is the fact that the windowed transformer has a limited effective context length, whereas other architectures do not. When most of the information needed to make a prediction is local, the windowed transformer is great. But when the information is further away in the sequence, it does worse.\nIt seems natural language is, for the most part, pretty local, and so the windowed transformer is mostly able to crush other architectures on LongCrawl64. But we can construct an aritificial dataset where this is not the case. Simply take LongCrawl64 and insert dummy characters between each pair of consecutive tokens (increasing the overall context length by the commensurate amount). Intuitively, this will preserve the information in the sequence, so no prediction is made more challenging for models with no limitation on maximum effective context size. But it will make the information less dense, meaning less information will be accessible within the effective context window of the windowed transformer.\nWe can run this experiment to validate empirically. We train a windowed transformer and symmetric power transformer on a sequence of datasets transformed in this way.\n[PLOT]\nAs expected, we see the windowed transformer get worse and worse, but the symmetric power transformer stays strong.\nThe takeaway here is that you should be careful before generalizing a ranking of architectures across all datasets. Different RNNs can be more or less suitable for different datasets."
  },
  {
    "objectID": "articles/rnn-state-scaling/index.html#conclusion",
    "href": "articles/rnn-state-scaling/index.html#conclusion",
    "title": "RNN State Scaling",
    "section": "4. Conclusion",
    "text": "4. Conclusion\nTODO write conclusion\nThanks for reading idk we might not need this. Plz subscribe\n\n  \n  Subscribe to be notified of new posts:"
  },
  {
    "objectID": "articles/rnn-state-scaling/index.html#footnotes",
    "href": "articles/rnn-state-scaling/index.html#footnotes",
    "title": "RNN State Scaling",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAll of our architectures grow in FLOPs linearly with state size. One could imagine RNN architectures that process each element of their state \\(n^2\\) or more, in which case it might make more sense to modify the state scaling setup accordingly.↩︎\nThe more typical definition, \\(|\\mathcal{S}| &lt; \\infty\\), follows directly from ours if it is assumed that all elements of each \\(S \\in \\mathcal{S}\\) are themselves finite. Since we are only really interested in algorithms that we can implement on computers, and floating point values have finite precision, we are happy to make this simplifying assumption.↩︎\nRNN is a term that is widely used but rarely rigorously defined, and so there is not complete agreement on its definition. In some obscure parts of the literature people may have used the term “RNN” to describe an architecture that is not a finite state machine (as defined here). But we think that the definition we use here is the most widely understood. In any case, we aim only to describe a internally-consistent set of concepts, it is OK if we are not consistent with the whole of the external literature.↩︎\nIn fact, a careful read of our previous article will reveal that we already used this more general form without explicitly commenting on it, by setting \\(\\phi(Q_i) = \\text{softplus}(Q_i)\\).↩︎\nAll of our architectures grow in FLOPs linearly with state size. One could imagine RNN architectures that process each element of their state \\(n^2\\) or more, in which case it might make more sense to modify the state scaling setup accordingly.↩︎"
  },
  {
    "objectID": "articles/symmetric-power-transformers/index.html",
    "href": "articles/symmetric-power-transformers/index.html",
    "title": "Symmetric Power Transformers",
    "section": "",
    "text": "\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\Z}{\\mathbb{Z}}\n\\newcommand{\\N}{\\mathbb{N}}\n\\newcommand{\\sft}{\\text{softmax}}\n\\newcommand{\\List}{\\text{List}}\n\\newcommand{\\Seq}{\\text{Seq}}\n\\newcommand{\\SeqT}{\\text{SeqT}}\n\\newcommand{\\CSeqT}{\\text{CSeqT}}\n\\newcommand{\\Dist}{\\text{Dist}}\n\\newcommand{\\SM}{\\text{SM}}\n\\newcommand{\\Fn}{\\text{Fn}}\n\\newcommand{\\Tok}{\\text{Tok}}\n\\newcommand{\\Aij}{ A_{[i,j]}}\n\\newcommand{\\ten}{\\small\\text{tensor}}\n\\newcommand{\\sym}{\\small\\text{symmetric}}\n\\newcommand{\\flat}[1]{\\text{flat}\\left(#1\\right)}\n\\newcommand{\\stab}{\\text{stab}}\n\\]\nLinear transformers [1] can be formulated as linear-cost RNNs, and so have better theoretical FLOP scaling than ordinary transformers. In our previous article, we presented an efficient chunked algorithm that turns this theoretical advantage into practical speedups when the context is long: 10x faster training for a 100k-token context. Unfortunately, we also found that vanilla linear transformers suffer from degraded performance, especially at long contexts, rendering any benefits from the speedup useless. This article advances our previous discussion by introducing a linear transformer variant that solves the degraded performance issue while still enabling an efficient linear-cost implementation.\nThe central idea behind all the algorithms explored in this post is that for RNNs, the size of the model should be understood not just in terms of parameters, but also in terms of the size of the state. The state of an RNN encodes all the information in its context into a finite-dimensional list of numbers. If we look at the basic RNN equation,\n\\[\nY_t = f(S_t, X_t) \\qquad S_{t+1} = g(S_t, X_t)\n\\]\nit is clear that the only path by which information about past input \\(X_1,...,X_{t-1}\\) is propagated to prediction \\(Y_{t}\\) is via the states \\(S_1,...,S_t\\). If the states are too small, the model might struggle to store all the information it will later require.\nWith this in mind, when we look at the GPT-2-style linear transformers we evaluated in our previous article, we find a clue about a potential cause of their poor performance: the states are many orders of magnitude smaller than the weights.\nFortunately, since the architecture is a linear transformer, this imbalance has a straightforward remedy. Since the state size includes a multiplicative factor of key_size, we can increase the state size by simply embedding the keys and queries in a higher-dimensional space. Previous work [2]–[5] has already observed that this improves the performance of linear transformers, but the resulting architectures are still not competitive with standard transformers.\nOf crucial importance is the choice of embedding function. In this article we focus on embeddings based on the tensor product and also the closely related symmetric power. In each case, we have a hyper-parameter \\(p\\) that controls the embedding dimension without affecting the weights of the model, allowing us to select the optimal state-weight ratio.\nOur central takeaway is that one family of architectures, symmetric power transformers, is a promising candidate for long-context modeling. For \\(p=4\\) and above, it outperforms the transformer baseline, and at \\(p=4\\) and below, it has a state size small enough to fit on a modern GPU.\nNote that in this article we run all the experiments with the quadratic-cost attention formulation, and so do not benefit from any speedup compared to the transformer baseline. This is because the goal of the research presented in this article is only to identify a promising candidate architecture. Implementing an efficient, linear-cost algorithm requires significant engineering effort, and so it is important to first be confident that the architecture will learn well. In a subsequent article, we will show how to combine the symmetric power embedding function with the efficient chunked algorithm."
  },
  {
    "objectID": "articles/symmetric-power-transformers/index.html#linear-transformers-with-embeddings",
    "href": "articles/symmetric-power-transformers/index.html#linear-transformers-with-embeddings",
    "title": "Symmetric Power Transformers",
    "section": "1. Linear Transformers with Embeddings",
    "text": "1. Linear Transformers with Embeddings\nWe begin with a review of linear transformers. The inputs to a transformer layer are sequences of \\(Q_i, K_i, V_i \\in \\R^d\\) of queries, keys, and values, where \\(i\\) ranges from \\(1\\) to the sequence length \\(t\\). The outputs are a sequence \\(Y_i\\in \\R^d\\). The formula for the output vectors is:\n\\[\nY_i = \\sum_{j=1}^i A_{ij} V_j \\qquad A_{ij}  = \\frac{ \\phi(Q_i)^T \\phi(K_j)}{\\sum_{k=1}^i \\phi(Q_i)^T \\phi(K_k) }\n\\]\n\n\nThis linear transformer is the same architecture as in our previous article, but here we present the complete formula in its full generality, including both the normalizing term and embedding function (previously suppressed for clarity).\nwhere \\(\\phi : \\R^d \\to \\R^D\\) is an embedding function that maps keys or queries into vectors of dimension \\(D\\). This formulation is what we call the attention formulation of a linear transformer, because it involves computing the the inner product between embedded keys and queries, akin to the self-attention operation in a standard transformer.\nThe exact same outputs can be computed via a recurrent formulation (see Appendix A for derivation):\n\\[\nY_{i} = \\frac{S_i \\phi(Q_i)}{Z_i \\phi(Q_i)} \\qquad Z_i = Z_{i-1} + \\phi(K_i)^T \\qquad S_i = S_{i-1} + V_i \\phi(K_i)^T\n\\]\nwhere \\(S_0 = Z_0 = \\mathcal{0}\\). Since \\(S_i \\in \\R^{d \\times D}\\) and \\(Z_i \\in \\R^{D}\\), the size of the state is \\(D(d+1)\\).\nHow should we choose \\(\\phi\\)? Here are some attributes that we want:\n\nAdjustable dimensionality. To balance the size of the state with the size of the weights, there should be some hyperparameter controlling the dimension \\(D\\).\nEfficient dot product. In the attention algorithm, \\(\\phi(Q_i)\\) and \\(\\phi(K_j)\\) appear only as intermediate steps in the computation of \\(\\phi(Q_i)^T\\phi(K_j)\\). For some choices of \\(\\phi\\), there is a more efficient formula for \\(\\phi(Q_i)^T\\phi(K_j)\\) that does not require computing these intermediate objects.1\nPositive dot product. We want \\(\\phi(Q_i)^T \\phi(K_j)\\) to always be positive. This ensures that each normalized output \\(Y_i\\) is a convex combination of all preceding values \\(V_1, \\cdots, V_i\\). We found this to be essential for stable and performant learning.\n\nWe are searching for the embedding function with the best empirical performance at each state size, subject to these three constraints. Since our ultimate goal is to replace transformers (trained with attention) with linear transformers (trained with the chunked algorithm), the bar for success is simple: a \\(\\phi\\) whose performance matches that of a transformer baseline at a state size small enough to be tractable. Concretely, let’s use 80 GB, which is the memory capacity of A100 and H100 GPUs.2\nMany possible embedding functions have already been investigated in the literature [2]–[5]. In this article, we will explore embeddings that use the tensor product and symmetric product to turn small vectors into large ones."
  },
  {
    "objectID": "articles/symmetric-power-transformers/index.html#tensor-product-transformers",
    "href": "articles/symmetric-power-transformers/index.html#tensor-product-transformers",
    "title": "Symmetric Power Transformers",
    "section": "2. Tensor Product Transformers",
    "text": "2. Tensor Product Transformers\nIn this section, we explore architectures based on the tensor product, a deep and ubiquitous mathematical idea. We begin with a brief introduction to the core mathematics, and then describe two ways to use the tensor products to embed vectors. (A third approach is discussed in Appendix B.)\n\n2.1. Mathematical Background\nThe tensor product of vectors generalizes the outer product and formalizes the concepts of multi-dimensional arrays. Given two vectors \\(v\\in \\R^{d_1}\\) and \\(w\\in \\R^{d_2}\\) one can think of their tensor product \\(v\\otimes w\\) as the matrix \\(v w^T \\in \\R^{d_1\\times d_2}\\), \\[\nv w^T = \\left[\n\\begin{array}{cccc}\nv_1w_1 & v_1w_2 & \\cdots & v_1w_m \\\\\nv_2w_1 & v_2w_2 & \\cdots & v_2w_m \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nv_nw_1 & v_nw_2 & \\cdots & v_nw_m \\\\\n\\end{array}\n\\right]\n\\] Similarly, \\(v \\otimes w \\otimes u\\) would be a 3-dimensional table containing all possible entries of the sort \\(v_i\nw_j u_k\\). But let’s make the intuition of multi-dimensional tables more rigorous.\nMulti-indices. A multi-index \\(\\alpha\\) specifies a location in a \\(p\\) dimensional table with dimension sizes \\(d_1,\n\\cdots, d_p \\in \\N\\). Let \\(\\N_d\\) denote the set \\(\\{1,2,\\cdots,d\\}\\), then we can say that \\(\\alpha = [\\alpha_1, \\cdots, \\alpha_p] \\in \\N_{d_1} \\times \\cdots \\times \\N_{d_p}\\).\nTensors. A tensor \\(T\\in \\R^{d_1 \\times \\cdots d_p}\\) corresponds to a high dimensional table where every location has a numerical value assigned to it. In other words, it is a map from multi-indices to the reals. \\[\nT: \\N_{d_1} \\times \\cdots \\times \\N_{d_p} \\to \\R\n\\]\nBy convention, we use subscript notation \\(T_\\alpha\\) instead of functional notation \\(T(\\alpha)\\) to index tensors.\nTensor product of vectors. Given a list of \\(p\\) vectors \\(v_i \\in \\R^{n_i}\\), we denote by \\(v_1 \\otimes \\cdots\n\\otimes v_p\\) (or alternatively \\(\\bigotimes_{i=1}^p v_i\\)) as the tensor in \\(\\R^{n_1 \\times \\cdots \\times n_p}\\) with entries given by the following formula: \\[\n\\left[\\bigotimes_{i=1}^p v_i\\right]_\\alpha = \\prod_{i=1}^p v_{i, \\alpha_i}\n\\]\nFlattening. We are going to use the tensor product to embed vectors into \\(\\R^{n_1\\times \\cdots n_p}\\) arrays. But once we’ve done that, we will no longer care about the array structure, and we will prefer to think of them as vectors in \\(\\R^D\\), where \\(D=\\prod_{i=1}^p n_i\\). The map \\(\\text{flat}: \\R^{n_1\\times \\cdots n_p} \\to \\R^D\\) implements this transformation by writing every entry of the array into a flat vector.\n\n\nExpand for a precise definition of the flattening of tensors.\n\nGiven a vector space \\(V\\) with an ordered basis \\(b_1 \\cdots b_n \\in V\\), we define \\(\\text{flat}: V \\to \\R^n\\) to map a generic vector \\(a = \\sum_i a_i b_i\\) to a column vector containing its coordinates. In other words, \\[\n\\flat{\\sum_i a_i b_i} =\n\\begin{bmatrix}\na_1 \\\\\na_2 \\\\\n\\vdots \\\\\na_n\n\\end{bmatrix}\n\\] A natural choice for the basis for \\(\\R^{n_1\\times \\cdots n_p}\\) are the 1-hot arrays. Given a multi-index \\(\\alpha=(\\alpha_1,\\cdots, \\alpha_p)\\), where \\(\\alpha_i \\in [1, n_i] \\cap \\mathbb{Z}\\), the 1-hot array is \\[E_\\alpha = \\bigotimes_{i=1}^p e^{n_i}_{\\alpha_i}\\]\nwhere \\(e^{n}_1, \\cdots, e^{n}_{n} \\in \\R^{n}\\) denote the 1-hot vectors. We also need to pick an ordering. It is natural to start with \\(\\alpha = [1, \\cdots, 1]\\) and increase the rightmost index carrying over the one next to it when the maximum value is reached. This is the same pattern used in row major ordering. For example, the indices of \\(\\R^{2\\times 2\\times 2}\\) would be \\([1,1,1], [1,1,2], [1,2,1], [1,2,2], [2,1,1], \\cdots\\)\n\nThe dot product of flattened tensors satisfies an easy-to-check property: \\[\n\\flat{\\bigotimes_{i=1}^p v_i}^T \\flat{\\bigotimes_{i=1}^p w_i} = \\prod_{i=1}^p v_i^T w_i \\qquad \\text{(Eq 1)}\n\\]\n\n\n2.2. Transformer Architectures\nArmed with the tensor product, we are ready to define an embedding, and in doing so define an architecture. Thanks to Equation 1, it is easy to ensure that embeddings based on flattened tensor products will have efficient dot products. Taking the two other criteria – adjustable state size and positive dot products – into account, there are a few natural choices of embedding to consider.\n\n\nIn this section, we focus on the effect of \\(\\phi\\) on keys \\(k\\), but wlog all discussion applies equally to queries.\n\n2.2.1. Softplus repeated tensor product\nThe softplus-repeat embedding uses an initial softplus to ensure positivity, and then repeatedly takes the tensor product of a \\(d\\)-sized key or query with itself \\(p\\) times, yielding a tensor of size \\(\\mathbb{R}^\n{d^p}\\). Formally, we define:\n\\[\n\\phi^p_{\\text{softplus-repeat}}(k) =\n\\text{flat}\\left(\\bigotimes_{i=1}^p \\text{softplus}(k)\\right)\n\\]\nA JAX implementation of softplus-repeat attention is simple. Note that in line 4, we are invoking Equation 1 to turn the dot product of flattened \\(p\\)-dimensional tensors into a product of \\(p\\) vector dot products.\ndef softplus_repeat_tensor_product_attn(K, Q, V, p):\n    Q, K = softplus(Q), softplus(K)\n    C = Q @ K.T\n    B = C**p\n    B = where(mask, B, 0)\n    A = B / B.sum(axis=1, keepdims=True)\n    Y = A @ V\n    return Y\nThis implementation is more pedagogical than practical, as it is not numerically stable. It is common for numerical issues to emerge when training at scale, and especially when training in mixed precision. In Appendix C, we provide an improved implementation that addresses these issues.\nRunning experiments, we see:\n\n\n\n\nPerformance improves meaningfully with increased \\(p\\), starting to approach the baseline. This supports our hypothesis that state size is a major cause of the gap between the linear transformer and the softmax transformer. However, we have still not succeeded at our goal of closing the gap completely, and have already far surpassed our memory quota, as the below table shows.\n\n\n\n\n\n\n\n\n\n\n\np\nState Size\nMemory ≤ 80 GB?\nRelative Loss at 100K Steps\nPerforms ≥ baseline?\n\n\n\n\n1\n589 KB\n✔\n1.23x\n✘\n\n\n2\n18 MB\n✔\n1.16x\n✘\n\n\n4\n309 GB\n✘\n1.11x\n✘\n\n\n6\n1.2 TB\n✘\n1.05x\n✘\n\n\n8\n5.2 EB\n✘\n-\n-\n\n\n\n\n\n\n2.2. Repeated tensor product\nThe repeat embedding is identical to the softplus-repeat embedding, but with no softplus. Since the softplus was used to guarantee positivity, its removal would seem to pose a problem; but one can easily check that, if \\(p\\) is even, positivity of the inner products is guaranteed even without softmax. As a result, this embedding can be used only with even \\(p\\).\n\\[\n\\phi^p_{\\text{repeat}}(k) =\n\\text{flat}\\left(\\bigotimes_{i=1}^p k\\right) \\in \\mathbb{R}^{d^p}\n\\]\nThe implementation is familiar, as it’s the same as the implementation in Section 2.1 but with the softplus removed:\ndef repeat_tensor_product_attn(K, Q, V, p):\n    C = Q @ K.T\n    B = C**p\n    B = where(mask, B, 0)\n    A = B / B.sum(axis=1, keepdims=True)\n    Y = A @ V\n    return Y\nSee Appendix C for a numerically-stable implementation. Running experiments, we see an exciting result:\n\n\n\n\nThis embedding gives performance which matches or even surpasses that of the baseline.\nContrasting this result with the curves in Section 3.1 yields an important insight: softplus massively degrades performance. Most existing literature on linear transformers has used softplus (or similar transformations) to uphold the positivity constraints, see for example [1]. One possible explanation for this degradation is that projecting all keys and queries to the positive quadrant (an exponentially small corner of the overall space) decreases the expressivity of the model. Whatever the reason, a side-by-side comparison of \\(\\phi^p_{\\text{repeat}}\\) to \\(\\phi^p_{\\text{softplus-repeat}}\\) makes it clear that this effect is large across all \\(p\\).\n\n\n\n\nReturning now to our main objective: have we found an embedding function whose performance is competitive with that of a strong transformer baseline, while, at the same time, having a state size small enough to fit on a GPU?\nUnfortunately, not yet. The table below shows the size of a single state, as measured in bytes (assuming fp16/bf16 precision), for a 124M-parameter GPT-2 repeated-tensor-product transformer at various \\(p\\).\n\n\nThe formula for the state size of a linear transformer with the repeated product embedding is layer_n * head_count *  key_size**p * value_size.\n\n\n\n\n\n\n\n\n\n\n\np\nState Size\nMemory ≤ 80 GB?\nRelative Loss at 100K Steps\nPerforms ≥ baseline?\n\n\n\n\n1\n589 KB\n✔\n1.23x\n✘\n\n\n2\n18 MB\n✔\n1.03x\n✘\n\n\n4\n309 GB\n✘\n.98x\n✔\n\n\n6\n1.2 TB\n✘\n.97x\n✔\n\n\n8\n5.2 EB\n✘\n-\n-\n\n\n\n\nThe settings of \\(p\\) that give sufficient performance have states that are far too large, so we still have not found an embedding with the properties that we desire. But we are close."
  },
  {
    "objectID": "articles/symmetric-power-transformers/index.html#symmetric-power-transformers",
    "href": "articles/symmetric-power-transformers/index.html#symmetric-power-transformers",
    "title": "Symmetric Power Transformers",
    "section": "3. Symmetric Power Transformers",
    "text": "3. Symmetric Power Transformers\nIn the next section, we explore a final embedding function, one with equivalent performance but much smaller state\nsizes. Once again, we will begin with an introduction to the relevant mathematical ideas, and then move on to describe a concrete architecture.\n\n3.1. Mathematical Background\nObserve that the embedding \\(\\phi^2_{\\text{repeated}}(v)= \\flat {v v^T}\\) is somewhat wasteful. The matrix \\(v v^T\\) is symmetric, and so it contains duplicated entries.\n\\[\nv v^T = \\left[\n\\begin{array}{cccc}\nv_1v_1 & v_1v_2 & \\cdots & v_1v_m \\\\\nv_2v_1 & v_2v_2 & \\cdots & v_2v_m \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nv_nv_1 & v_nv_2 & \\cdots & v_nv_m \\\\\n\\end{array}\n\\right]\n\\]\nEntries at indices \\((i,i)\\) appear a single time, and entries at indices \\((i,j)\\) where \\(i\\neq j\\) appear twice. This is because \\(v_i v_j = v_j v_i\\) due to the commutativity of scalar multiplication. Noticing this symmetry allows us to create an alternative embedding, \\(\\phi^2_\\text{sym}: \\R^d \\to \\R^{\\frac{d^2 +d} 2}\\), which can be implemented as:\ndef sym_emb_2(v):\n  x, d = [], v.size\n  for i in range(d):\n    for j in range(i, d):\n      count = 1 if i==j else 2\n      x.append(sqrt(count) * v[i] * v[j])\n  return x\nThis construction \\(\\phi^2_\\text{sym}\\) guarantees that \\(\\phi^2_\\text{sym}(v)^T \\phi^2_\\text{sym}(w) = \\phi^2_\\text\n{repeat} (v)^T \\phi^2_\\text{repeat}(w)\\). If you recall that, in the attention formulation of the linear transformer the embedding \\(\\phi\\) only influences the outputs via the attention scores, defined as: \\[\nA_{ij}  = \\frac{ \\phi(Q_i)^T \\phi(K_j)}{\\sum_{k=1}^i \\phi(Q_i)^T \\phi(K_k) }\n\\] that two linear transformers with embeddings \\(\\phi^2_\\text{repeat}(v)\\) and \\(\\phi^2_\\text{sym}(v)\\) will have exactly the same outputs, since they have the same \\(\\phi(Q_i)^T \\phi(K_j)\\). We’ve been able to exploit the symmetry of \\(v v^T\\) to construct an equivalent embedding function with approximately half the dimensionality!\nNext, we will generalize this idea to higher powers.\nSymmetric Tensors. The first thing we need to do is generalize the concept of symmetric matrices to symmetric tensors. For that, we need the concept of the permutation group of \\(p\\) elements. Let \\(\\rho: \\N_p \\to \\N_p\\) be an invertible function, then it is called a permutation. We write \\(G_p\\) for the collection of all such permutations of \\(p\\) elements.\nAllow us to overload the notation slightly and, for a multi-index \\(\\alpha = (\\alpha_1, \\cdots, \\alpha_p)\\) define the permutation of the multi-index as \\(\\rho(\\alpha) = (\\alpha_{\\rho(1)}, \\cdots, \\alpha_{\\rho(p)})\\). Then this allows us to define a symmetric tensor.\nDefinition: A tensor \\(T \\in \\R^{\\underbrace{n\\times \\cdots \\times n}_p}\\) is symmetric if, \\[\nT_\\alpha = T_{\\rho(\\alpha)}\n\\] for all multi-indices \\(\\alpha \\in \\N_d^{\\times p}\\) and permutations \\(\\rho \\in G_p\\). \\(\\blacksquare\\)\nAs we saw in the example of \\(\\phi^2_\\text{sym}(v)\\), when we tensor a vector \\(v\\in \\R^d\\) with itself \\(p\\) times, that produces tensors \\(T = \\bigotimes^p v\\) that are symmetric. Again, this is due to the commutativity of multiplication. For example, for any tensor constructed in this way, the multi-indices \\([1, 2, 3]\\) and \\([3, 2, 1]\\) will return the same entry: \\(T_{[1, 2, 3]} = v_1 v_2 v_3 = v_3 v_2 v_1 = T_{[3, 2, 1]}\\).\nThe general proof is simple: \\[\n\\left[\\bigotimes_{i=1}^p v \\right ]_{\\rho(\\alpha)} = \\prod_{i=1}^p v_{\\alpha_{\\rho(i)}} = \\prod_{i=1}^p v_{\\alpha_i} = \\left[\\bigotimes_{i=1}^p v \\right ]_{\\alpha}\n\\]\nWhat will an embedding \\(\\phi^p_\\text{sym}\\) of a vector of length \\(d\\) look like? As in the example above, the first thing to do is enumerate the unique elements. Since indices are permutation-invariant, we can do this by establishing any canonical permutation. The non-decreasing ordering is a natural choice. Thus, instead of storing the elements at all \\(d^p\\) possible multi-indices, we need only store the \\(\\binom{d+p-1}{p}\\) non-decreasing sequences (i.e \\(\\alpha\\) where \\(\\alpha_i \\le \\alpha_{i+1}\\)).\nNext, in order to make the inner product come out unchanged, we need to upweight frequently-seen terms by the square root of their count, just as we did in the example of \\(\\phi^2_\\text{sym}\\). To know how many times a permutation of an index \\(\\alpha\\) is repeated, define the index-count vector \\(c\\in \\N^d\\), where \\(c_i = \\sum_{j=1}^p \\delta(\\alpha_j, i)\\) counts how many times each \\(i\\in \\{1, \\cdots, d\\}\\) occurs in \\(\\alpha\\). (For example, if \\(\\alpha = (0,1,1,3)\\), we have \\(c_0 = 1\\), \\(c_1 = 2\\), \\(c_2 = 0\\), \\(c_3 = 1\\).) Then, there must be \\(\\frac{d!}{c_1 ! \\; \\cdots \\; c_p!}\\) elements of the repeated-product embedding that have that value. These coefficients are known as the multinomials.\nPutting these two ideas together, \\(\\phi^p_\\text{sym}\\) will give a list of \\(\\binom{d+p-1}{p}\\) numbers, each corresponding to a particular unique element of the repeated tensor product multiplied by a multinomial coefficient that counts its appearances. The inner product of two vectors embedded in this way is identical to the repeated tensor product embedding, \\(\\phi^2_\\text{sym}(v)^T \\phi^2_\\text{sym}(w) = \\phi^2_\\text {repeat}(v)^T \\phi^2_\\text {repeat}(w)\\). In fact, it can be show that this is the shortest list that can has this property; we have fully exploited the symmetry of \\(\\phi^p_\\text {repeat}\\).\n\n\nExpand to see a complete derivation of these properties.\n\nDuplicate counts By definition, the only constraint a symmetric tensor has, is that all the entries \\(T_{\\rho(\\alpha)}\\) must be the same for all permutations \\(\\rho \\in G_p\\). Now we want to understand the amount of duplication that that any specific \\(\\alpha\\) has. Since the number of permutations of the multi-indices is \\(|G_p| = p!\\), a naive estimate would be that every entrie \\(T_\\alpha\\) appears \\(p!\\) times in the tensor. And indeed, that is the case for some multi-indices. For example, every permutation \\(\\rho \\in G_3\\) sends the multi-index \\([1,4,6]\\) to a different multi-index, so there are \\(4!\\) entries with the same value. But. on the other hand, for the multi-index \\([1,1,1]\\) it doesn’t matter what permutation \\(\\rho\\) we apply, we always have that \\(\\rho \\alpha = \\alpha\\). So the entrie \\([1,1,1]\\) of symmetric tensor \\(T\\) could only appear a single time.\nTo answer this question of a generic \\(\\alpha\\) we need the orbit stabilizer theorem. It tells us that the number of different multi-indices that can be produced by applying permutations to \\(\\alpha\\) (a.k.a the size of the orbit) equals the total number of permutations (which is \\(p!\\)) divided by the number of permutations fixing \\(\\alpha\\) (the size of the stabilizer). The size of the stabilizer is easy to work out. The only permutations that leave multi-indices the same are the ones that interchange indices with the same value. For example, \\(\\alpha = [\\alpha_1, \\alpha_2, \\alpha_3] = [1,1,2]\\), we could apply a permutation that interchanges the first and second indices. For the generic formula, we need to know how many times each index \\(i\\in \\{ 1, \\cdots, d\\}\\) appears in the multi-index \\(\\alpha\\). Define \\[\n\\text{count}(\\alpha, i) = \\sum_j \\delta(\\alpha_j - i)\n\\] Where \\(\\delta\\) is the delta function sending \\(0\\) to \\(1\\) and every other number to \\(0\\). Clearly, the number of permutations fixing \\(\\alpha\\) is \\(\\prod_{i=1}^d \\text{count}(\\alpha, i)!\\). So applying the orbit stabilizer theorem we get that a symmetric tensor \\(T\\) at multi-index \\(\\alpha\\) has \\[\n\\frac {p!} {\\prod_{i=1}^d \\text{count}(\\alpha, i)!} = \\frac {p!}{\\stab(\\alpha)}\n\\] duplicated entries. This is also known as the formula for the multinomial theorem.\nBasis of symmetric tensors We know that a lot of multi-indices of a symmetric tensor are redundant. To understand the true structure (like the dimensionality) of symmetric tensors we need to find a way to select an instance of each, non redundant, multi-indices. One way to do that is to restrict oursleves to non decreasing multi-indices. Denote them by \\(P = \\{\\alpha \\in \\N_d^{\\times p} \\; | \\; \\alpha_i \\le \\alpha_{i+1} \\}\\). Then we can construct a basis for the space of symmetric tensors out of \\(\\alpha \\in P\\) like \\[\nS^\\alpha = \\frac 1 {\\stab(\\alpha)} \\sum_{\\rho \\in G_p} E^{\\rho(\\alpha)}\n\\] where you’ll recall that \\(E^\\alpha = \\bigotimes^p_{i=1} e_{\\alpha_i}\\) is the natural basis for the (non-symmetric) tensors. To convince ourselves that \\(\\{S^\\alpha | \\alpha \\in P \\}\\) forms a basis of the symmetric tensors we need to check that the set is linearly independent and that it spans all symmetirc tensors. Let’s check linear independence first. Assume that we have some coefficients \\(x_\\alpha \\in \\R\\) s.t. \\[\n\\sum_{\\alpha \\in P}  x_\\alpha S^\\alpha = 0\n\\] Then, for any \\(\\beta \\in P\\) \\[\\begin{align}\n0 &= \\left[\\sum_{\\alpha \\in P}  x_\\alpha S^\\alpha\\right]_\\beta\n= \\sum_{\\alpha \\in P}  x_\\alpha S^\\alpha_\\beta\n= \\sum_{\\alpha \\in P} \\frac {x_\\alpha} {\\stab(\\alpha)} \\sum_{\\rho \\in G_p} E^{\\rho(\\alpha)}_\\beta \\\\\n&= \\sum_{\\alpha \\in P} \\frac {x_\\alpha} {\\stab(\\alpha)} \\sum_{\\rho \\in G_p} 1(\\rho(\\alpha) = \\beta) \\\\\n&= \\frac {x_\\beta} {\\stab(\\beta)} \\sum_{\\rho \\in G_p} 1(\\rho(\\beta) = \\beta) \\quad \\text{Since } \\alpha, \\beta \\in P, \\text{the only way that } \\rho(\\alpha)=\\beta \\text{ is if } \\alpha = \\beta \\\\\n&= x_\\beta \\\\\n\\end{align}\\]\nThus, all \\(x_\\alpha = 0\\) so the set is linearly independent. To show \\(S^\\alpha\\) span all symmetric tensors it we can just show that, for any symmetric tensor \\(T\\), if we define \\[\nQ = \\sum_{\\alpha \\in P} T_\\alpha S^\\alpha\n\\] Then \\(T = Q\\). That can be easily seen by noticing that \\(Q\\) is a symmetric tensor and that, evaluating \\(Q\\) at \\(\\beta \\in P\\) \\[\\begin{align}\nQ_\\beta &= \\left[\\sum_{\\alpha \\in P} T_\\alpha S^\\alpha \\right]_\\beta\n= \\sum_{\\alpha \\in P} \\frac {T_\\alpha} {\\stab(\\alpha)} \\sum_{\\rho \\in G_p} E^{\\rho(\\alpha)}_\\beta \\\\\n&= \\sum_{\\alpha \\in P} \\frac {T_\\alpha} {\\stab(\\alpha)} \\sum_{\\rho \\in G_p} 1(\\rho(\\alpha) = \\beta) \\\\\n&= \\frac {T_\\beta} {\\stab(\\beta)} \\sum_{\\rho \\in G_p} 1(\\rho(\\beta) = \\beta) \\\\\n&= T_\\beta\n\\end{align}\\]\nConcluding the proof. CARLES NOTE: consider flipping the order and showing first that they span all symmetric tensors.\nDimension of symmetric tensors Since we’ve created a basis for the space of symmetric tensors out of non-decreasing sequences we can establish the dimension of the space by counting all such sequences. This is a standard combinatorial problem solved via the method of bars and stars. Which tells us that the dimension is \\[\\binom{d+p-1}{p}\\]\nThe only thing we must note to apply the standard combinatorial results is that there is a 1-1 correspondance between non-decreasing sequences and multisets of \\(d\\) elements of cardinality \\(p\\). This is made clear by looking at any example. The multiset \\(\\{ 1,1, 2, 4 \\}\\) has a unique way in which it can be ordered if the sequence must be non-decreasing. CARLES NOTE: I don’t know how clear this all is. I kind of want to give a proof from scratch presenting the method of bars and stars\nSymmetric powers of vectors span the symmetric tensors Showing that all tensors of the form \\(\\bigotimes^p v\\) are symmetric was tivial, but there is a harder question we might ask ourselves. Are all the \\(\\binom {d + p - 1} p\\) dimensions of the symmetric tensors actually necessary? Just like the naive way to store \\(\\bigotimes^p v\\) contins \\(d^p\\) most of which are duplicates of one another, perhaps there is still redundancy when we just store the entries at non-decreasing multi-indices.\nThe way to formalize this question is to ask whether the space of all symmetric tensors is spanned by tensors of the sort \\(\\bigotimes^p v\\). The answer is yes, and is proven by showing the following result known as the polarization identity:\nResult Let \\(T: \\N_d^{\\times p} \\to \\R\\) be a symmetric tensor and define \\(\\bar T: \\R^d \\to \\R\\) as \\[\n\\bar T(v) = T(v, \\cdots, v)\n\\] then the following identity holds \\[\nT = \\frac 1 {p!}  \\sum_{b_1, \\cdots, b_d = 0}^1 \\bigotimes_{i=1}^p \\left ( b_1 e_1 + \\cdots + b_d e_d \\right)\n\\]\n\n\n\n3.2. Algorithm\nImplementation. The following code exploits this structure to implement an embedding with the exact same inner products as \\(\\phi_\\text{repeat}\\), but a much smaller dimensionality:\n# helper function to iterate over non-decreasing multiindices\ndef index_iterator(a, d, p):\n  assert len(a) == p\n  a = a.copy()\n  for i in reversed(range(p)):\n    assert a[i]&gt;=0 and a[i]&lt;d\n    a[i] += 1\n    if a[i] &lt; d:\n      # no need to carry. Just set all indices to the right = to a[i]\n      for j in range(i, p): a[j] = a[i] \n      break\n  return a\n\n# helper function to compute number of times an element appears\ndef count(a, d, p):\n  c = []\n  for i in range(d):\n    ci = 0\n    for j in range(p):\n      if a[j] == i:\n        ci += 1\n    c.append(ci)\n  return c\n\ndef symmetric_emb(v, d, p):\n  a = [0] * p\n  D = binomial(d + p -1, p)\n  x = []\n  for i in range(D):\n    a = index_iterator(a, d, p)\n    c = count(a, d, p)\n    xi = sqrt(multinomail(d, c))\n    for j in range(p):\n      xi *= v[a[j]]\n    x.append(xi)\nUsing this embedding produces a massive dimensionality reduction compared to the dimensionality of \\(\\phi_\\text{repeat} ^p\\). The table below compares the size of the state between repeated tensor products and symmetric powers, as measured in bytes (assuming half-precision), for a 124M-parameter GPT-2 repeated-tensor-product transformer at various \\(p\\).\n\n\n\n\np\nRepeated Tensor Product\nSymmetric Power\nSavings\n\n\n\n\n2\n75 MB\n38 MB\n49%\n\n\n4\n309 GB\n14 GB\n95%\n\n\n6\n1.2 PB\n2.2 TB\n99.8%\n\n\n8\n5.2 EB\n196 TB\n99.996%\n\n\n\n\nSince this embedding produces identical outputs to the repeated tensor product, the learning curves are the same as those we saw in Section 3.2. We can evaluate each symmetric power architecture against our two metrics, state size (under 80 GB) and performance (loss below baseline).\n\n\n\n\n\n\n\n\n\n\n\np\nState Size\nMemory ≤ 80 GB?\nRelative Loss at 100K Steps\nPerforms ≥ baseline?\n\n\n\n\n2\n38 MB\n✔\n1.03x\n✘\n\n\n4\n14 GB\n✔\n0.98x\n✔\n\n\n6\n2.2 TB\n✘\n0.97x\n✔\n\n\n8\n196 TB\n✘\n-\n-\n\n\n\n\nSymmetric power with \\(p=4\\) passes our bar. This architecture is looking promising!"
  },
  {
    "objectID": "articles/symmetric-power-transformers/index.html#conclusion-upcoming-work",
    "href": "articles/symmetric-power-transformers/index.html#conclusion-upcoming-work",
    "title": "Symmetric Power Transformers",
    "section": "Conclusion & Upcoming Work",
    "text": "Conclusion & Upcoming Work\nIn this article, we have shown how to design a linear transformer which closes the performance gap to classic softmax transformers, using a tractably-small state: increase the state size with the tensor product, remove the softplus, and exploit symmetry. We expect this approach will provide transformer-level performance at greatly reduced training costs when combined with the chunked algorithm, as well as the constant-time inference common to all RNNs. In an upcoming article, we plan to release an open-source model that uses a symmetric power transformer at its core, together with an efficient CUDA kernel implementation. Stay tuned!\nAnother application of the symmetric power will be seen in a second upcoming article, where we explore Taylor Transformers: a family of embedding functions that end up perfectly approximating the classical transformer by replacing the exponential inside the softmax with its Taylor approximation. The symmetric power turns out to be the core computational tool needed.\nIn a third upcoming article studying state scaling, we will compare symmetric transformers more directly to other modern RNN approaches, by comparing the rate at which performance improves as the size of the state is increased.\nThanks for reading! Join our discord for discussion and early previews of releases.\n\n  \n  Subscribe to be notified of new posts:"
  },
  {
    "objectID": "articles/symmetric-power-transformers/index.html#a.-recurrence-formulation-derivation",
    "href": "articles/symmetric-power-transformers/index.html#a.-recurrence-formulation-derivation",
    "title": "Symmetric Power Transformers",
    "section": "A. Recurrence Formulation Derivation",
    "text": "A. Recurrence Formulation Derivation\n\nHere we derive the recurrent formulation of embedded linear transformer from its attention formulation:\n\\[\n\\begin{aligned}\nY_i &= \\sum_{j=1}^i A_{ij} V_j \\qquad A_{ij}  = \\frac{ \\phi(Q_i)^T \\phi(K_j)}{\\sum_{k=1}^i \\phi(Q_i)^T \\phi(K_k) }\\\\\nY_i &= \\sum_{j=1}^i \\frac{ \\phi(Q_i)^T \\phi(K_j)}{\\sum_{k=1}^i \\phi(Q_i)^T \\phi(K_k) } V_j \\\\\n    &= \\sum_{j=1}^i V_j \\frac{ \\phi(Q_i)^T \\phi(K_j)}{\\sum_{k=1}^i \\phi(Q_i)^T \\phi(K_k) } \\\\\n    &= \\sum_{j=1}^i V_j \\frac{ \\phi(K_j)^T \\phi(Q_i)}{\\sum_{k=1}^i \\phi(K_k)^T \\phi(Q_i) } \\\\\n    &= (\\sum_{j=1}^i \\frac{V_j \\phi(K_j)^T}{Z_i\\phi(Q_i)}) \\phi(Q_i) \\\\\n    &= (\\frac{\\sum_{j=1}^i V_j \\phi(K_j)^T}{Z_i\\phi(Q_i)}) \\phi(Q_i) \\\\\n    &= (\\frac{S_i}{Z_i\\phi(Q_i)})\\phi(Q_i) \\\\\n    &= \\frac{S_i\\phi(Q_i)}{Z_i\\phi(Q_i)} \\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "articles/symmetric-power-transformers/index.html#b.-softplus-split-tensor-product",
    "href": "articles/symmetric-power-transformers/index.html#b.-softplus-split-tensor-product",
    "title": "Symmetric Power Transformers",
    "section": "B. Softplus split tensor product",
    "text": "B. Softplus split tensor product\n\nThe softplus-split embedding uses an initial softplus to ensure positivity, then splits the key of length \\(d\\) into \\(p\\) evenly-sized pieces of length \\(\\frac{d}{p}\\), which are then combined using the tensor product. The resulting tensor is in \\(\\R^{\\left(\\frac{d}{p}\\right)^p}\\). Concretely, the equation for the softplus-split embedding is\n\\[\n\\phi^p_{\\text{softplus-split}}(k) = \\flat{\\bigotimes_{i=1}^p \\text{softplus}(k'_i)}\n\\]\nwhere each \\(k'_i\\) is the vector containing elements \\(\\big[k_{i\\frac{d}{p}},...,k_{(i+1)\\frac{d}{p} -1} \\big]\\).\nAttention for a linear transformer with this embedding function has the following JAX implementation (lines 5 and 6 correspond to the application of Eq 1):\ndef softplus_split_attn(K, Q, V, p):\n    t, d = K.shape\n    Q, K = softplus(Q), softplus(K)\n    Q_split = Q.reshape([t, p, d//p]).transpose(1,0,2)\n    KT_split = K.reshape([t, p, d//p]).transpose(1,2,0)\n    C_split = jax.vmap(jnp.matmul)(Q_split, KT_split)\n    B = jnp.prod(C_split, axis=0)\n    B = where(mask, B, 0)\n    A = B / B.sum(axis=1, keepdims=True)\n    Y = A @ V\n    return Y\nSee Appendix C for a numerically-stable implementation.\nLet’s put this into action:\n\n\n\n\nWe see that increasing \\(p\\) causes performance to improve. However, all values of \\(p\\) tested still fall short of the baseline."
  },
  {
    "objectID": "articles/symmetric-power-transformers/index.html#c.-numerical-stability",
    "href": "articles/symmetric-power-transformers/index.html#c.-numerical-stability",
    "title": "Symmetric Power Transformers",
    "section": "C. Numerical stability",
    "text": "C. Numerical stability\n\nNumerical instabilities come from numbers underflowing (too small) or overflowing (too large). Solutions typically fall into a few main categories:\n\nMake sure a number is not too small or too large, e.g. turn log(x) into log(x + ε). This prevents overflow.\nAccumulate in fp32. When accumulating a long list of small values in half-precision, it is sometimes the case that each addition will underflow and no accumulation will occur at all.\nManipulate an equation to pull out some common factors and cancel them out algebraically, rather than letting them cancel out computationally. For example, to calculate \\(\\frac{x}{y}\\) where \\(x = Am\\) and \\(y = An\\) for some large \\(A\\), compute m / n instead of x / y.\nSeparate magnitude and sign, and work with magnitude in log-space, i.e. manipulate sign(x) and log(abs(x)) instead of working with x directly. Convert back to linear-space with sign(x) * exp(f(log(abs(x)))), where f has already completed the relevant cancellations, so x is small enough too avoid overflow.\n\nWith these techniques in mind, we can implement a numerically-stable version of the softplus-split approach. The main change is that we sum in log-space instead of multiplying, and preemptively cancel any large terms in the numerators with their corresponding denominators by subtracting the log-space row-max.\n\n\nExpand for numerically-stable implementation of softplus-split.\n\ndef softplus_split_tensor_product_attn(Q, K, V, p, ε):\n    t, d = Q.shape # temporal & head-width dimensions\n    assert d % p == 0\n\n    # softplus to make simplex norm behave nicely\n    Q, K = softplus(Q), softplus(K)\n    # split via a reshape and transpose\n    Q_split = Q.reshape([t, p, d//p]).transpose(1,0,2)\n    KT_split = K.reshape([t, p, d//p]).transpose(1,2,0)\n\n    # compute each dot product\n    split_D = vmap(jnp.matmul)(Q_split, KT_split)\n    # combine splits in log space for numerical stability\n    log_C = sum(log(split_D + ε), axis=0, dtype=float32).astype(Q.dtype)\n    # apply the causal mask\n    log_C = where(tril(ones(log_C.shape)), log_C, -inf)\n    # subtract rowmax for numerical stability\n    log_C -= log_C.max(axis=-1, keepdims=True)\n\n    # return to linear space\n    B = exp(log_C)\n    # compute the normalizing term, accumulating in float32 for numerical stability\n    denom = B.sum(-1, keepdims=True, dtype=float32).astype(B.dtype)\n\n    # project to simplex, adding ε for numerical stability\n    A = B / (denom + ε)\n    # compute output\n    Y = A @ V\n    return Y\n\n\n\nExpand for a numerically-stable implementation of softplus-repeat.\n\ndef softplus_repeat_tensor_product_attn(Q, K, V, p, ε):\n    # use softplus to project to the positive quadrant\n    Q, K = softplus(Q), softplus(K)\n\n    # compute inner products\n    D = Q @ K.T\n\n    # raise to power, in log space for numerical stability\n    log_C = p * log(D + ε)\n    # apply causal mask\n    log_C = where(tril(ones(log_C.shape)), log_C, -inf)\n    # subtract rowmax for numerical stability\n    log_C -= log_C.max(axis=-1, keepdims=True)\n    \n    # Return to linear space\n    B = exp(log_C)\n    # Compute the normalizing term, accumulating in float32 for numerical stability\n    denom = B.sum(-1, keepdims=True, dtype=float32).astype(B.dtype)\n    \n    # project to simplex, adding ε for numerical stability\n    A = B / (denom + ε)\n    # compute output\n    Y = A @ V\n    return Y\n\n\n\nExpand for a numerically-stable implementation of repeat.\n\ndef repeat_tensor_product_attn(Q, K, V, p, ε):\n     # even only\n     assert p % 2 == 0\n\n    # compute inner products\n    D = Q @ K.T\n\n    # raise to power, in log space for numerical stability\n    log_C = p * log(D + ε)\n    # apply causal mask\n    log_C = where(tril(ones(log_C.shape)), log_C, -inf)\n    # subtract rowmax for numerical stability\n    log_C -= log_C.max(axis=-1, keepdims=True)\n    \n    # Return to linear space\n    B = exp(log_C)\n    # Compute the normalizing term, accumulating in float32 for numerical stability\n    denom = B.sum(-1, keepdims=True, dtype=float32).astype(B.dtype)\n    \n    # project to simplex, adding ε for numerical stability\n    A = B / (denom + ε)\n    # compute output\n    Y = A @ V\n    return Y"
  },
  {
    "objectID": "articles/symmetric-power-transformers/index.html#footnotes",
    "href": "articles/symmetric-power-transformers/index.html#footnotes",
    "title": "Symmetric Power Transformers",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is also known as the kernel method. It is essential when one works with infinite-dimensional embeddings, and it’s also useful in this case to avoid materializing large-but-finite embeddings.↩︎\nThis state-size threshold is admittedly somewhat arbitrary. In principle, larger states are possible with clever sharding; but for excessively large states, which must be sharded across a huge number of GPUs, the hardware cost of sharding becomes completely prohibitive. In this article, we are training models whose parameters fit on a single GPU, so it seems reasonable to use the memory of a single GPU as the threshold for tractability.↩︎"
  },
  {
    "objectID": "articles/gradient-flows/index.html",
    "href": "articles/gradient-flows/index.html",
    "title": "Why Gradient Descent Minimizes the Loss",
    "section": "",
    "text": "\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\der}{\\partial}\n\\newcommand{\\dldt}{\\frac{\\der l}{\\der t}}\n\\newcommand{\\len}{\\text{length}}\n\\newcommand{\\en}{\\text{energy}}\n\\newcommand{\\dim}{\\text{dim}}\n\\newcommand{\\tr}{\\text{trace}}\n\\newcommand{\\lin}{\\text{Lin}}\n\\newcommand{\\rnk}{\\text{rank}}\n\\newcommand{\\ht}{\\widehat}\n\\newcommand{\\dwdt}{\\frac{\\der w}{\\der t}}\n\\newcommand{\\l}{\\mathscr{l}}\n\\newcommand{\\E}{\\mathbb E}\n\\]\nIt’s commonly stated that neural networks are difficult to study because they are not convex, making it hard to say much about the behavior of gradient descent. But for the past few months I’ve been looking at ways to prove convergence guarantees to low loss without the assumption of convexity. Surprisingly, just with elementary concepts of calculus, it is possible to study simple neural networks like a two-layer MLP and show that learning will minimize the training loss.\nThis article follos a very geometrical perspective. Here we think of the parameters of the neural network as a vector in a vector space, and the process of learning as a continuous curve through that space. This geometric perspective emerges out of taking the limit where the learning rate goes to \\(0\\). When the parameterers follow a differentiable path through time, we can invoke concepts like the energy of and length the path, which turn out to have profound interpretations within the context of a learning problem.\nNaturally, to show that the training loss is minimized, we need a “repalcement for convexity”. An assumption that provides similar guarantees, but which neural networks actually satisfy. What this alternative should be is perhaps the most important idea in the article. It can be summarized as: “the gradient must be large when the loss is large”. With some caveats, this property seems to hold for the neural networks that are used in practice and thus, we can guarantee that learning will minimize the loss.\nThis article is stil work in progress. Apologies for any writing or math errors. The proofs are semi-rigorous at best, and a lot important questions remain unnanswered. I plan to keep workig and updating the article to generalize the results to architectures other than 2 layer MLPs. Also, I’m uncertain about the novelty of these ideas. Please let me know if you are aware of related work."
  },
  {
    "objectID": "articles/gradient-flows/index.html#notation-and-concepts",
    "href": "articles/gradient-flows/index.html#notation-and-concepts",
    "title": "Why Gradient Descent Minimizes the Loss",
    "section": "0) Notation and Concepts",
    "text": "0) Notation and Concepts\nMuch of the theory on this article is thinks of the space of parameters as a vector space equipped with an inner product. The parameters of a neural network tend to either be elements of \\(\\R^{n}\\) or matrices in \\(\\R^{n\\times m}\\). So we need to define suitable inner products for these two types of vectors.\nDefinition 0.1: Inner Product For two vectors \\(x,y \\in \\R^n\\) we take the inner product to be \\(&lt;x, y&gt; = x^T y\\). For two matrices \\(M,N\\in \\R^{n\\times m}\\) we use \\(&lt;M, N&gt; = \\tr(M^T N)\\). Remember that any inner product induces a norm \\(|\\cdot|\\) via \\(|v| = \\sqrt{&lt;v,v&gt;}\\). And you can verify that, in the \\(\\R^n\\) and \\(M,N\\in \\R^{n\\times m}\\) cases, this induced norm takes the following forms: \\[\\begin{align}\n|x| = \\sqrt{\\sum_i x_i^2} \\quad\\quad |M| = \\sqrt{\\sum_{ij} M_{ij}^2}\n\\end{align}\\]\nWhich are also known as the L2 norms of the respective vector spaces. \\(\\blacksquare\\)\nDefinition 0.2: Derivative Let \\(f: V \\to W\\) be a function between two vector spaces. The derivative at a point \\(v\\in V\\) is a linear function \\(\\der f(v): V\\to W\\). It tells us how changing the input \\(v \\to v + u\\) will affect the output (if the change \\(u\\in V\\) is small). Concretely, \\[\n\\der f(v)(u) \\simeq f(v + u) - f(v)\n\\]\nSome people would call \\(\\der f(v)(u)\\) the directional derivative of \\(f\\) along \\(u\\) at a point \\(v\\). \\(\\blacksquare\\)\nDefinition 0.3: Path Derivative A specially important case is when we are taking the derivative of a funciton \\(h:\\R \\to V\\) (a path through \\(V\\)). Here, using the notation \\(\\der h(x): \\R \\to V\\) is a little bit heavy handed. Any linear map \\(M: \\R \\to V\\) can instead be represented by the vector \\(v = M(1)\\) together with scalar vector multiplication. Just see that \\(M(r) = r \\; v\\) for all \\(r\\in \\R\\). Effectively, Newton’s notation \\(h'\\) applies this idea to path derivatives by defining \\(h'(t) = \\der h(t)(1)\\). \\(\\blacksquare\\)\nOne important use of the derivative of a path is to define the length and energy of the path.\nDefinition 0.4: length and energy of a path Given a normed vector space \\(V\\) and a differentiable path \\(h: [0, t] \\to V\\), the length and energy of the path are defined as \\[\\begin{align}\n\\len(h) &= \\int_0^t |h'(s)| ds \\\\\n\\en(h) &= \\int_0^t |h'(s)|^2 ds\n\\end{align}\\]\n\\(\\blacksquare\\)\nThe length and energy are related by the following inequality, which will be key in proving the central results of the article.\nResult 0.5: If \\(h: [0, t] \\to W\\) is a differentiable path, then \\[\n\\len(h)^2 \\le t\\;  \\en(h)\n\\]\nProof Just note that \\[\\begin{align}\n\\len(h) &=  \\int_0^t |h'(s)| ds  \\\\\n&\\le  \\sqrt{\\int_0^t  1^2 ds } \\sqrt{\\int_0^t | h'(s)|^2 ds } \\;\\;\\;\\;\\;\\; \\text{(by Cauchy Schwarz)} \\\\\n&\\le\\sqrt{ t \\; \\en(h)} \\\\\n\\end{align}\\]\nAnd, since \\(\\len(h)\\) is positive and squaring positive numbers is a monotone function, the result follows. \\(\\blacksquare\\)\nDefinition 0.6: Gradient The gradient of a differentiable function \\(f: V \\to \\R\\), denoted \\(\\nabla f\\), is a map \\(\\nabla f: V\\to V\\) defined so that \\(\\nabla f(v)\\in V\\) is the unique vector satisfying \\[\n\\der f(v)(u) = &lt;\\nabla f(v), u&gt;\n\\]\nfor all \\(u\\in V\\). \\(\\blacksquare\\)\nNote: You might be used to a different definition of the gradient. The one you know turns out to be equivalent to the one I’m using here. If you find this confusing you should take a look at this great article.\nSometimes it is completely clearn from the context which funciton \\(f\\) we are taking the gradient of. In those cases I sometimes use the shorthand \\(\\hat v = \\nabla f(v)\\)."
  },
  {
    "objectID": "articles/gradient-flows/index.html#learning-with-gradient-flows",
    "href": "articles/gradient-flows/index.html#learning-with-gradient-flows",
    "title": "Why Gradient Descent Minimizes the Loss",
    "section": "1) Learning with Gradient Flows",
    "text": "1) Learning with Gradient Flows\nDefinition 1.1: Learning Problem: The learning problem setup consists of a tuple \\((W, w_0, f)\\) where:\n\n\\(W\\) is a vector spcae equiped with an inner product \\(&lt;\\cdot, \\cdot&gt;\\). You can think of \\(W\\) as the parameter space which controls the behavior of the model.\n\\(w_0\\in W\\) is the initial point from which the learning will proceed.\n\\(f: W \\to \\R^+\\) is the loss function that tells us how good the parameters are (presumably at fitting some dataset). A loss function must be lower bounded so it’s nice to assume wlog that \\(\\inf_{w} f(w) = 0\\).\n\nOut of \\(f, W\\) and \\(w_0\\), the following objects emerge:\n\nA path \\(\\gamma: [0, \\infty) \\to W\\) given by the learning algorithm. In this document, we take \\(\\gamma\\) to be a gradient flow (defined below) of the loss funciton \\(f\\).\nA loss curve \\(l:\\R \\to \\R\\) defined as \\(l(t) = f\\circ \\gamma(t)\\).\n\\(L = l(0) = f(w_0)\\). Since \\(\\inf_{w} f(w) = 0\\) we can think of \\(L\\) as the amount of “work” that the learning algorithm will need to do to solve the problem.\n\n\\(\\blacksquare\\)\nThe most fundamtenal learning algorithm is gradinet descent. It starts starts with the initial parameters \\(w_0\\in W\\) and slowly moves them in a direction that minimizes the loss by applying the update rule \\(w_{t+1} = w_t - \\delta \\nabla f(w_t)\\) for some learning rate hyperparameter \\(\\delta \\in \\R^+\\). We would like to understand why this algorithm works so well when training neural networks, but the discreteness of the steps complicates the analysis. It will be very useful to study the learning behaviour in the limit \\(\\delta \\to 0\\). In this case the weights follow a continuous curve \\(\\gamma:\\R \\to W\\) which is called the gradient flow of \\(\\nabla f\\).\nDefinition 1.2: Gradient Flow: Let \\(W\\) be a vector space with an inner product. Given a differentiable, lower bounded function \\(f: W\\to \\R\\). The gradient flow starting at \\(w_0\\in W\\) is defined as the path \\(\\gamma: [0, \\infty) \\to V\\) satisfying \\[\n\\gamma(0) = w_0  \\quad\\text{and}\\quad \\gamma'(t) = - \\nabla f\\circ \\gamma(t)\n\\]\nThe existance of such a path follows from the existance and uniqueness of PDEs. TODO: Explain why the flow \\(\\gamma\\) doesn’t “diverge to infinity”, that the flow isn’t just a curve \\(\\gamma:[0, \\epsilon) \\to W\\), but a map with domain \\([0, \\infty)\\). This uses the assumption that \\(f\\) is lower bouded. \\(\\blacksquare\\)\nIt is evident that gradient descent and gradient flows are deeply connected, but the distinction between them deserves careful consideration. For any concrete learning problem \\((W, w_0, f)\\), if we succeed in showing that the gradient flow will minimize the loss, then we will also know that gradient descent with a small enough learning rate is bound to also minimize the loss. But understnading how small the learning rate needs to be is of profound importance. It will determine the computational cost of solving the problem.\nAnother very important thing that this document currently omits are the ideas of stochastic gradient descnet. Modern deep learning is always based on computing gradients on a small subset of the dataset. Using these gradients results in slighly worse updates, but much cheaper ones. A tradeoff worth making because, for the same amount of compute, it allows us to make many more updates, resulting in more learning overall.\nUltimately, the questions that really matter are all centered around the computational cost of solving a learning problem. And studying the behavior of gradient flows will always fall short of that obvjective. Yet, I believe that studying gradient flows will prove to be a useful intermediate step. The rest of this section goes over some of the most important mathematical properties of gradient flows that make them so well suited to study learning.\nResult 1.3 The derivative of the loss curve is the magnitude of the gradient: \\[l'(t) = - | \\nabla f\\circ \\gamma(t)| ^2\\]\nProof \\[\\begin{align}\nl'(t) &= \\der f(w)( \\small\\dwdt)\n\\quad &\\text{(chain rule)} \\\\\n&= &lt; \\nabla f(w), \\small\\dwdt&gt;\n\\quad &\\text{(gradient def.)} \\\\\n&= -&lt; \\nabla f(w), \\nabla f(w)&gt;\n\\quad &\\text{(grad flow def.)} \\\\\n&= - | \\nabla f\\circ \\gamma(t)| ^2\n\\end{align}\\]\n\\(\\blacksquare\\)\nSo the magnitude of the gradient is the rate of change of the loss. Large gradient means fast learning. A consequence of this result is that \\(\\en(\\gamma)\\) measures how much the path \\(\\gamma\\) has managed to reduced the loss.\nResult 1.4: If \\(\\gamma:[0, t] \\to W\\) is a gradient flow of \\(f:W \\to \\R\\), then:\n\\[\n\\en(\\gamma) = L - l(t)\n\\]\nAlso, the energy is bounded by the initial loss: \\(\\en(\\gamma) \\le L\\).\nProof \\[\\begin{align}\n\\en(\\gamma) &= \\int_0^t |\\gamma'(s)|^2 ds \\\\\n&= \\int_0^t |\\nabla f \\circ \\gamma(s)|^2 ds\n= \\int_0^t  l'(s) ds \\\\\n&= l(0) - l(t)\n\\end{align}\\]\nThe last step is to notice that \\(l(t)\\ge 0\\) and \\(l(0)=L\\). \\(\\blacksquare\\)\nThis is a very interesting shift in perspective. We can reformulate questions about the loss curve \\(l: \\R\\to\\R^+\\) into questions about \\(\\en(\\gamma)\\). But it also has another very important application. To prove our results about neural networks, we will need to ensure that they satisfy that nice condition of having large gradient on points with high loss. We will see that, at initializatio \\(w_0\\), the networks do indeed satisfy this property. But that is not the case for all \\(w\\in W\\), only for \\(w_0\\) and points nearby. So we’ll need a way to guarantee that \\(\\gamma\\) won’t move the parameters too far away from \\(w_0\\), at least not too quickly. The following result does just that.\nResult 1.5 If \\(f\\) is a loss funciton, \\(w_0\\) the initial parameters, \\(L=f(w_0)\\) and \\(\\gamma\\) a gradient flow of \\(f\\) starting at \\(w_0\\), then \\[\n|\\gamma(t) - w_0| \\le \\sqrt{t\\;L}\n\\]\nProof \\[\\begin{align}\n|\\gamma(t) - \\gamma(0)|\n&= \\left |\\int_0^t  \\gamma'(s) ds \\right| \\\\\n&\\le \\int_0^t  \\left | \\gamma'(s) \\right| ds \\quad &\\text{(result A.2)} \\\\\n&= \\len(\\gamma) \\\\\n&\\le \\sqrt{t \\; \\en(\\gamma)} \\quad &\\text{(result 0.5)}\n\\end{align}\\]\nThe last step is using result 1.4, which tells us that \\(\\en(\\gamma) \\le L\\). \\(\\blacksquare\\)"
  },
  {
    "objectID": "articles/gradient-flows/index.html#a-replacement-for-convexity",
    "href": "articles/gradient-flows/index.html#a-replacement-for-convexity",
    "title": "Why Gradient Descent Minimizes the Loss",
    "section": "2) A Replacement for Convexity",
    "text": "2) A Replacement for Convexity\nWe are trying to show that neural networks trained via gradient descent will (approximately) reach 0 loss. One way to get such type of guarantees is to assume the loss funciton \\(f\\) is convex. But that is simply not the case for NNs. We need to find a replacement for convexity that is applicable to NNs. I believe this property is that the gradient \\(\\nabla f(w)\\) must be large at all points \\(w\\) where the loss \\(f(w)\\) is large. Concretely, for some \\(\\alpha \\in \\R^+\\) we need that\n\\[\n\\frac{|\\nabla f(w)|^2}{f(w)}  \\ge \\alpha\n\\]\nThis is another way to guarantee no local minima exist. Think about it. The definition of a local minima is a point \\(w\\) with no gradient but high loss, the existance of which is ruled out by the condition.\nBut the following result tells us something much stronger. If the property holds, the loss is guaranteed to decay to \\(0\\) exponentially fast with time.\nResult 2.1 If \\(f: W\\to \\R^+\\) satisfyies \\(\\frac{|\\nabla f(w)|^2}{f(w)}  \\ge \\alpha\\) for some \\(\\alpha \\in \\R^+\\), then \\[\nl(t) \\le L \\; e^{-\\alpha t}\n\\]\nProof \\[\\begin{align}\n\\frac{\\der \\ln l(t)}{\\der t} &= \\frac{l'(t)}{l(t)}\n= -\\frac{|\\nabla f \\circ \\gamma(t)|^2}{f\\circ \\gamma(t)}\n\\le  -\\alpha \\\\\n\\end{align}\\]\nso \\[\n\\ln l(t) - \\ln l(0) = \\int_0^t \\frac{\\der \\ln l(s)}{\\der s}  ds \\le - \\int_0^t \\alpha ds = - \\alpha t\n\\] And then \\(\\ln l(t)  \\le  \\ln L - \\alpha t\\). To conclude just use the fact that exponential is a monotone function and \\[\nl(t) = e^{\\ln l(t)} \\le  e^{\\ln L - \\alpha t} = L e^{ - \\alpha t}\n\\]\nas desired. \\(\\blacksquare\\)\nThe next question we should ask is obvious: Do Neural Networks Satisfy property 3.1? The short answer is No! To see that, just consider an MLP with “degenerate” parameters where all the weight matrices are \\(0\\). The MLP will have 0 gradient even when it has high loss.\nBut the long answer is more interesting. Turns out that the standard initialization techniques of NNs guarantee that, at initialization, the property hods for some large \\(\\alpha\\). And even better, the property remains true for all points near \\(w_0\\). We will look into the particular guarantees for a 2 layer MLP in the next section, but for now just assume that we are looking at a learning problem satisfying\n\\[\n\\frac{|\\nabla f(w)|^2}{f(w)}  \\ge \\alpha \\quad\\forall w\\in W \\text{ s.t. } |w-w_0| \\le \\epsilon\n\\]\nThis is where the work from the last section is going to pay off. We learned that \\(|\\gamma(t) - w_0| \\le \\sqrt{t\\;L}\\) so the parameters cannot move very far in a short period of time. Then, as long as \\(\\sqrt{t L} \\le \\epsilon\\) we can be sure that the assumptions of result 3.1 hold. And so the bound\n\\[\nl(t) = L e^{ - \\alpha t}\n\\]\nWill be true for all \\(t \\le \\frac {\\epsilon^2} L\\). Thus, \\[\n\\l_\\infty \\le l(\\frac {\\epsilon^2} L) \\le L \\exp({-\\frac {\\alpha \\epsilon^2} L })\n\\]\nThis is the simplest way we can put together all the important ideas, but not the most elegant one. Instead of assuming that the gradient \\(\\frac{|\\nabla f(w)|^2}{f(w)}  \\ge \\alpha\\) for all \\(w\\) within a ball of \\(w_0\\), we will be able to prove for a 2 layer MLP that \\(\\frac{|\\nabla f(w)|^2}{f(w)}  \\ge \\alpha - \\beta \\; |w-w_0|\\) for all \\(w\\in W\\). This results in a more elegant bound.\nResult 2.2 If \\(f\\) and \\(w_0\\) satisfy \\(\\frac{|\\nabla f(w)|^2}{f(w)}  \\ge \\alpha - \\beta \\; |w-w_0|\\) for all \\(w\\in W\\), then \\[\nl(\\infty) \\le L \\; \\exp({-\\frac {\\alpha^3}{3 \\beta^2 L} })\n\\]\nProof See appendinx \\(\\blacksquare\\)\nNow it’s time to look at a simple neural network and show that the properties of result 3.2 hold."
  },
  {
    "objectID": "articles/gradient-flows/index.html#the-simplest-neural-network",
    "href": "articles/gradient-flows/index.html#the-simplest-neural-network",
    "title": "Why Gradient Descent Minimizes the Loss",
    "section": "3) The Simplest Neural Network",
    "text": "3) The Simplest Neural Network\nThe objective of this section is very simple. Take the absolute simplest neural network, and show that the conditions of result 2.2 are satisfied. We’ll look at a 2 layer MLP with ReLU nonlinearity. It has two parameter matrices \\(M\\in \\R^{k\\times m}\\) and \\(N\\in \\R^{n\\times k}\\). The ReLU is denoted by \\(\\sigma\\). Given an input \\(x \\in \\R^m\\) the MLP produces the output \\[\ny = N \\sigma(M x)\n\\]\nIt’s convinient to break down the computation into steps to define intermediate variables. First we apply a matrix to the input \\(a = Mx\\) then the nonlineary to get \\(b=\\sigma(a)\\) and finally we produce the output \\(y=Nb\\). The variables/activations \\(a,b,y\\) are dependant on the parameters \\(M,N\\) so, when we consider changing the parameters with time, these activations will change too.\nParameter space The weight vectors are pairs of matrices \\(w = (N, M)\\) and so the parameter space is \\[W= \\R^{n\\times k} \\oplus \\R^{k\\times m}\\]\nThe loss function Keeping in with the philosophy of studying the simplest example, we’ll take the loss function to be the L2 error on a single input-target pair \\((x, q)\\in \\R^n \\oplus \\R^m\\). \\[\nf(w) = \\frac 1 2 |y - q|^2\n\\]\nWe will need \\(x\\) to be normalized, so we just assume that \\(|x|^2 = m\\). It’s also useful to define the loss variable \\(\\l = f(w)\\).\nInitialization The standard way to initialize neural networks is to sample independently all the entries in the weight matrices form some distribution. In our case we use \\[\nM_{ij} \\sim \\mathcal N (0, {\\small \\frac 2 m} ) \\quad\\quad N_{ij} \\sim \\mathcal N (0, {\\small \\frac 2 k})\n\\]\nThis is the commonly used He init, which works well for networks with ReLU nonlinearities.\nLearning Guarantees Appendix B is dediacted to showing that this choice of architecture, loss function and initialization satisfies (with high probability):\n\\[\n\\frac{|\\nabla f(w)|^2}{f(w)} \\ge 2k - 4 \\sqrt {kn} |w_0 - w|\\\\\n\\]\nSo by setting \\(\\alpha = 2k\\) and \\(\\beta = 4 \\sqrt {kn}\\), the application of result 2.2 gives \\[\nl(\\infty) \\le L \\; \\text{exp}({-\\frac {k^2}{6 n L} })\n\\]\nFrom looking at the above equation, it is apparent that the scale of the MLP helps it learn. By growing \\(k\\) we can very quickly lower the loss to any acceptable value."
  },
  {
    "objectID": "articles/gradient-flows/index.html#appendix-a",
    "href": "articles/gradient-flows/index.html#appendix-a",
    "title": "Why Gradient Descent Minimizes the Loss",
    "section": "Appendix A:",
    "text": "Appendix A:\nDefinition A.1: Operator Norm If \\(M: V \\to W\\) is a linear map between two vector spaces equipped with inner products, the the operator norm \\(|| \\cdot ||\\) is defined as \\[\n||M|| = \\max_{v\\in V} \\frac{|Mv|}{|v|}\n\\]\nWhere the numerator uses the norm of \\(W\\) and the denominator the one of \\(V\\). When the two norms on \\(V\\) and \\(W\\) arise from inner products, then \\(||M||\\) is the largest singular value of \\(M\\). \\(\\blacksquare\\)\nResult A.2: triangle inequality for integrals If \\(V\\) is a normed vector space, \\(a,b\\in \\R\\) and \\(h: [a, b] \\to V\\) is a differentiable path, then the following inequality holds: \\[\n\\left|\\int_a^b h(s) ds \\right| \\le \\int_a^b |h(s)| \\: ds\n\\]\nProof The proof is a simple application of the definition of the integral (as the limit of a sum) combined with the triangle inequality of the inner product. \\(\\blacksquare\\)\nResult A.3 Let \\(A\\in \\R^{n\\times m}\\) and recall that \\(|\\cdot|\\) and \\(||\\cdot||\\) denote the Frobenious and Operator norms respectively. Then \\[||  A || \\le { \\sqrt {\\text{rank}(A) } } \\; |A|\\]\nProof TODO \\(\\blacksquare\\)\nResult A.4 If \\(\\gamma: \\R \\to \\R^{n\\times m}\\) is a differentiable path and the derivative satisfies \\(\\text{rank}( \\gamma'(t) ) \\le r\\) for all \\(t\\), then \\[\n||M - M_0||  \\le  \\sqrt r \\; \\len(\\gamma)\n\\]\nProof First we use the triangle inequality of the operator norm \\[||M - M_0|| = || \\int_0^t \\gamma'(s) ds || \\le \\int_0^t || \\gamma'(s) || ds\n\\]\nThen, result A.3 tells us that \\(||  A || \\le \\sqrt{\\rnk(A)} \\; |A|\\) and so \\[\n||M - M_0||\n\\le \\int_0^t \\sqrt r \\;| \\gamma'(s) | ds\n= \\sqrt r \\;\\len(\\gamma)\n\\]\n\\(\\blacksquare\\)\nResult 2.2 If \\(f\\) and \\(w_0\\) satisfy \\[\n\\frac{|\\nabla f(w)|^2}{f(w)}  \\ge \\alpha - \\beta \\; |w-w_0|\n\\] for all \\(w\\in W\\), then \\[\nl(\\infty) \\le L \\; \\exp({-\\frac {\\alpha^3}{3 \\beta^2 L} })\n\\]\nProof The proof follows a very similar argument to 3.1,\n\\[\\begin{align}\n\\frac{\\der \\ln l(t)}{\\der t} &= -\\frac{|\\nabla f(w)|^2}{f(w)} \\quad &\\text{(result 1.3)} \\\\\n&\\le - \\alpha + \\beta \\;  |w - w_0| \\\\\n&\\le - \\alpha + \\beta \\sqrt {L t} \\quad &\\text{(result 1.5)} \\\\\n\\end{align}\\]\nIntegrating both sides from \\(0\\) to \\(t\\) we get \\[\n\\ln l(t) - \\ln L \\le - \\alpha t + \\frac{2}{3}  \\beta \\sqrt L \\; t^{3/2}\n\\]\nwhich implies \\[\nl(t) \\le L \\exp(- \\alpha t + \\frac{2}{3}  \\beta \\sqrt L \\; t^{3/2})\n\\]\nNow solve for the value of \\(t\\) that minimizes the term in the expoential. By setting the derivative to \\(0\\) you can easily verity that it is \\(t^* = \\frac {\\alpha^2} {\\beta^2 L}\\). At this particular time, the bound is minimized. It tells us that: \\[\nl(t^*) \\le L \\exp({-\\frac{\\alpha^3}{3\\beta^2 L}})\n\\]\nSo \\(l(\\infty) \\le l(t^*) \\le L\\exp({-\\frac{\\alpha^3}{3\\beta^2 L}})\\) proving the result. \\(\\blacksquare\\)"
  },
  {
    "objectID": "articles/gradient-flows/index.html#appendix-b",
    "href": "articles/gradient-flows/index.html#appendix-b",
    "title": "Why Gradient Descent Minimizes the Loss",
    "section": "Appendix B:",
    "text": "Appendix B:\nAn analysis of the gradients, activations at initializations, and bounds on how much the activations and gradients can move for the 2 layer MLP descrived in section 3.\n\nGradients\nThe gradients of the loss \\(\\l\\) wrt all the intermediate variables and weights are: \\[\n\\ht y = y -q, \\quad\n\\ht b = N^T \\ht y, \\quad\n\\ht a =\\der \\sigma(a)^T \\ht b, \\quad\n\\ht M = \\ht a x^T, \\quad\n\\ht N = \\ht y b^T, \\quad\n\\]\nNote that \\(|\\hat y |^2 = 2 \\l\\) so, when the loss is large, the gradient of \\(\\l\\) wrt \\(y\\) is large too. Ultimately are trying to show something similar, but about gradients of \\(\\l\\) wrt \\(M\\) to then apply result 2.2.\nBelow I’ve writen a short derivation of all these gradient formulas, but it uses some unconventional notation and techniques. If you find them confusing, just work out the gradients in your own way and confirm you get the same answers.\nStarting with \\(\\ht y\\), the gradient of \\(\\l\\) wrt \\(y\\). Let \\(\\dot y \\in \\R^n\\) denote an arbitrary change to \\(y\\). Then \\[\\begin{align}\n&lt;\\ht y, \\dot y&gt; =\\frac {\\der \\l} {\\der y} (\\dot y)\n&\\simeq \\frac 1 2 |y +\\dot y - q|^2 - \\frac 1 2 |y - q|^2 \\\\\n&= \\frac 1 2 ( &lt;\\dot y, y-q&gt; + &lt;\\dot y, \\dot y&gt; + &lt;y-q, \\dot y&gt; ) \\\\\n&\\simeq &lt;y-q, \\dot y&gt; \\quad \\text{(dropping the lower order term)} \\\\\n\\end{align}\\]\nNow let’s see how a change in \\(b\\) affects \\(y\\). Like before, let \\(\\dot b\\) denote a change to \\(b\\). The derivative \\(\\frac {\\der y}{\\der b}(\\dot b) \\simeq N(b+ \\dot b) - M b = \\dot M b\\). So, the gradient of \\(\\l\\) wrt \\(b\\) satisfies \\[\n&lt;\\ht b, \\dot b&gt;\n= {\\frac {\\der \\l}{\\der b}(\\dot b)}\n= {\\frac {\\der \\l}{\\der y}} \\left(  {\\frac {\\der y}{\\der b}}(\\dot b) \\right)\n=  &lt;\\ht y,  {\\frac {\\der y}{\\der b}} (\\dot b)&gt;\n= &lt;\\hat y, M \\dot b&gt; = &lt;M^T \\ht y, \\dot b&gt;\n\\]\nNow let’s look at \\(N\\). The derivative \\(\\frac {\\der y}{\\der N}(\\dot N) \\simeq (N+\\dot N) b - N b = \\dot N b\\). And the gradient \\[\n&lt;\\ht N, \\dot N&gt;  = &lt;\\ht y, \\dot N b&gt; = &lt;\\ht y b^T, \\dot N&gt;\n\\]\nRecall that, since we are taking the inner product of two matrices, we are using the trace under the hood. To see why the last step is true just use the cyclic property of the trace. Finally, gradients of \\(a\\) and \\(M\\). \\[\\begin{gather}\n&lt;\\hat a, \\dot a&gt;\n= &lt;\\hat b, \\der \\sigma(a)(\\dot a)&gt;\n= &lt;\\der \\sigma(a)^T \\ht b, \\dot a&gt;  \\\\\n&lt;\\ht M, \\dot M&gt;\n= &lt;\\ht a, \\dot M x&gt; = &lt;\\ht a x^T, \\dot M&gt;\n\\end{gather}\\]\n\n\nActivations at Initialization\nRemember that we are sampling the initial weight matrices via \\[\nM_{ij} \\sim \\mathcal N (0, {\\small \\frac 2 m} ) \\quad\\quad N_{ij} \\sim \\mathcal N (0, {\\small \\frac 2 k})\n\\]\nSince \\(M,N\\) are random variables, so are \\(a\\) and \\(b\\). This initialization used carefuly chosen variances to ensure that the entries of the initial activations \\(a = M x\\) and \\(b = \\sigma(a)\\) are within some reasonable range. In particular, we want the entries of \\(b_i\\) to have \\(0\\) mean and variance \\(1\\). And since all the entries are independent, that will mean that \\(|b|^2 \\simeq k\\) and so \\(b\\) will be close to the sphere of radius \\(\\sqrt k\\) with high probability. But this is only the case if the input vector is also normalized. That is why we needed the assumption that \\(|x|^2 = m\\). This section is dedicated to proving these statements.\nFirst we want to understand \\(\\E[a_i^2]\\) \\[\\begin{align}\n\\E[a_i^2]\n&= \\E[ ({\\small \\sum_j M_{ij} x_j})^2] \\\\\n&= \\E[ {\\small \\sum_j M_{ij}^2 x_j^2 + \\sum_{k\\neq j} M_{ij} x_j M_{ik} x_k } ] \\\\\n&= \\sum_j \\E[M_{ij}^2 x_j^2] \\\\\n&= \\frac 2 m |x|^2= 2\n\\end{align}\\]\nwhere we used the independence of different entries of \\(M\\) and the fact that \\(\\E[M_{ij}]=0\\). Then \\[\n\\E[|a|^2] = \\sum_i \\E[a_i^2] = 2k\n\\]\nLet \\(p_M\\) denote the pdf’s of the entries of \\(M_{ij}\\) (all entries have the same pdf because they are iid). The pdf of \\(a_i\\) will be the nested integral of \\(\\delta(a_i - M_i^T x)\\) as \\(M_{i1},\\cdots,M_{im}\\) range from \\(-\\infty \\to \\infty\\), where \\(\\delta\\) denote the Dirac delta function. \\[\np_{a}(z) = \\int_{-\\infty}^\\infty \\cdots \\int_{-\\infty}^\\infty p_M(M_{i1}) \\cdots p_M(M_{im}) \\; \\delta (z - M_i^T x)\\; d M_{i1} \\cdots d M_{im}\n\\]\nRecall that a distribution is symmetric if \\(p(z) = p(-z)\\). Since \\(p_M\\) is a gaussian it is symmetric. The next thing we need to prove is that \\(p_a\\) is symmetric too.\n\\[\\begin{align}\np_{a}(z) &= \\int_{-\\infty}^\\infty \\cdots \\int_{-\\infty}^\\infty p_M(M_{i1}) \\cdots p_M(M_{im}) \\; \\delta (z - M_i^T x)\\; d M_{i1} \\cdots d M_{im} \\\\\n&=  \\int_{-\\infty}^\\infty \\cdots \\int_{-\\infty}^\\infty p_M(-M_{i1}) \\cdots p_M(-M_{im}) \\; \\delta (z + M_i^T x)\\; d M_{i1} \\cdots d M_{im} \\\\\n&=  \\int_{-\\infty}^\\infty \\cdots \\int_{-\\infty}^\\infty p_M(M_{i1}) \\cdots p_M(M_{im}) \\; \\delta (-z - M_i^T x)\\; d M_{i1} \\cdots d M_{im} \\\\\n&= p_a(-z)\n\\end{align}\\]\nThe first step uses the change of variable \\(M_{ij}\\to -M_{ij}\\) and the second one exploits the symmetry of \\(p_M\\) and \\(\\delta\\). Finally, this allows us to compute the term we care about \\[\\begin{align}\n\\E[b_i^2] &= \\int_{-\\infty}^\\infty p_a(a_i) \\sigma(a_i)^2 da_i \\\\\n&= \\int_{-\\infty}^0 p_a(a_i) \\:0\\: da_i + \\int_0^\\infty p_a(a_i) a_i^2 da_i \\\\\n&= \\frac 1 2 \\int_{-\\infty}^\\infty p_a(a_i) a_i^2 da_i \\quad \\text{(using symmetry)}\\\\\n&= \\frac 1 2 \\E[a_i^2] \\\\\n&= k\n\\end{align}\\]\nWe have only been working out \\(\\E[|b|^2] = k\\) but we wanted to make claims about the particular samples themselves being approximately \\(|b|^2\\simeq k\\). Of course, the rigorous way to go about it is to prove statements like \\(\\Pr( k - \\epsilon \\le |b|^2 \\le k+\\epsilon) \\le \\alpha\\). But this type of argument is very tedious and adds very little insight about the ideas this document is exploring. So to conclude the proof in an elegant way, I’ll just assume that \\(|b|^2 = k\\). When the dimension \\(k\\) is large, this approximation will be very accurate.\n\n\nActivation and Gradient Bounds\nDenote by \\(a_0=M_0 x, b_0=\\sigma(a_0)\\) and \\(y_0=N_0 b_0\\) the activaions at initialization \\(w_0 = (M_0, N_0)\\). In the last section we were showing that these activations started in some reasonable range, but that isn’t enough. We need to ensure they remain stable during learning. Concretely, we want to derive upper bounds on \\(|a - a_0|\\) and \\(|y - y_0|\\) based on on \\(|w-w_0|\\). First \\[\\begin{align*}\n|a_0 - a| &\\le |M_0 x - Mx| \\\\\n          & = ||M_0 - M|| \\; |x|\\quad \\text{(operator norm def.)} \\\\\n          & = \\sqrt m |M_0 - M| \\quad \\text{(using result A.3)} \\\\\n          & \\le \\sqrt m \\; |w-w_0|\n\\end{align*}\\]\nTo bound \\(|b_0 - b |\\) we need to use the fact that the ReLU \\(\\sigma\\) is 1-Lipschitz. So \\[\n|b_0 - b| = |\\sigma(a) - \\sigma(a_0) | \\le |a_0 - a | \\le \\sqrt n \\; |w-w_0|\n\\]\nTo apply result 2.2. we need to find \\(\\alpha, \\beta \\in \\R^+\\) such that \\(|\\nabla f(w)|^2 \\ge f(w)( \\alpha - \\beta \\; |w-w_0|)\\). The first step is to lower bound the gradient magnitude of \\(N\\). \\[\\begin{align}\n| \\ht{N} |^2 &= |\\ht y b^T|^2 = \\text{trace}(b y^T y b^T)\n              = |\\ht{y}|^2 |b|^2\n              = 2 \\l |b|^2 \\\\\n              &\\ge \\l (|b_0|  - |b_0 - b|)^2\n              = 2 \\l ( \\sqrt k - |b_0 - b| )^2 \\\\\n              &= 2 \\l ( k - 2 \\sqrt k |b_0 - b| +  |b_0 - b| ^2 ) \\\\\n              &\\ge 2 \\l ( k - 2 \\sqrt k |b_0 - b| ) \\\\\n              &\\ge 2 \\l ( k - 2 \\sqrt {kn} |w_0 - w| ) \\\\\n\\end{align}\\]\nWe could also attempt to derive a lower bound for \\(|\\hat M|^2\\) but it’s not really necessary. We already have enough to apply 2.2. \\[\\begin{align}\n\\frac{|\\nabla f(w)|^2}{f(w)} &= \\frac{|\\hat M|^2 + |\\hat N|^2}{f(w)} \\\\\n&\\ge \\frac{|\\hat N|^2}{f(w)} \\\\\n&\\ge \\frac{2 \\l ( k - 2 \\sqrt {kn}|w_0 - w|)}{f(w)} \\\\\n&\\ge 2k - 4 \\sqrt {kn} |w_0 - w|\\\\\n\\end{align}\\]"
  },
  {
    "objectID": "drafts/research/index.html",
    "href": "drafts/research/index.html",
    "title": "An Architecture For Neural Language Modeling",
    "section": "",
    "text": "\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\Z}{\\mathbb{Z}}\n\\newcommand{\\N}{\\mathbb{N}}\n\\newcommand{\\sft}{\\text{softmax}}\n\\newcommand{\\List}{\\text{List}}\n\\newcommand{\\Seq}{\\text{Seq}}\n\\newcommand{\\SeqT}{\\text{SeqT}}\n\\newcommand{\\CSeqT}{\\text{CSeqT}}\n\\newcommand{\\Dist}{\\text{Dist}}\n\\newcommand{\\SM}{\\text{SM}}\n\\newcommand{\\id}{\\text{id}}\n\\newcommand{\\Fn}{\\text{Fn}}\n\\newcommand{\\Tok}{\\text{Tok}}\n\\newcommand{\\Aij}{ A_{[i,j]}}\n\\]\nA crucial decision in neural language modeling is the choice of architecture, which governs the compuatational and statistical properties of the model. A well-selected architecture can improve performance-per-dollar by orders of magnitude.\nThe objective of this work is to find an architecture with the following four properties:\nAt the time of writing, the most popular architecture for neural language modeling, the Transformer, has only three of these properties (it lacks efficient sampling). Many other architectures have been been proposed, but none that meets all four criteria.\nWe believe that such an architecture exists. This document is an expository writeup of progress we have made towards the goal of discovering it. This document contains our evolving understanding of the key concepts and results needed to understand neural language modeling, including mathematical, computational, and experimental results. Unlike a traditional research paper, this is living document that will continually be updated as we make progress.\nMuch of the content of this document will already be familiar to anyone who has thought deeply about language modeling. There is nothing particularly groundbreaking here. But we found a lot of value in explicitly formalizing the concepts, e.g. how think of Transformers and RNNs as members the same family; as well as grounding all of these ideas in rigorous mathematics. Also, in order to properly understand our specific setting, we often find that it is helpful to describe more general objects of which neural language modeling is a specific case. Where applicable and insightful, we provide such results in their full generality; and as such, this document contains many ideas that may be of independent interest to researchers in related areas.\nIt’s not about minimizing floating point operations. It’s about optimizing tokens per second on our hardware. GPUs perform well with tasks that hare highly parallelizable. RNNs, the more “clasical” architectures for sequences, are not very well suited for these types of tasks.\nimportant feature of the transformer architecture is that it can compute We find that it’s important to think about sequences, and maps between sequences as the core objects."
  },
  {
    "objectID": "drafts/research/index.html#sequence-transformations",
    "href": "drafts/research/index.html#sequence-transformations",
    "title": "An Architecture For Neural Language Modeling",
    "section": "2.1) Sequence Transformations",
    "text": "2.1) Sequence Transformations\nDefinition We say \\(h: \\Seq X \\to \\Seq Y\\) is a sequence transformation if \\(h\\) preserves the length of the sequence. Meaning that if the input sequence has lenght \\(t\\), so does the output sequence.\nWe refer to \\(\\SeqT(X, Y)\\subset \\Fn(\\Seq X, \\Seq Y)\\) as the subset of all sequence transformations.\nIn deep learning we care about composing multiple layers into more complex architectures. Sequence transformations are very well behaved in this way, as the composition of two sequence transforms is a sequence transform itself.1 This is a completely obvious fact, but since it is quite an important one it’s worth stateting it as a result.\nResult: If \\(h\\in \\SeqT(X, Y)\\) and \\(f\\in \\SeqT(Y, Z)\\) then \\(f\\circ h \\in \\SeqT(X,Z)\\).\nIn the next section we’ll see how RNNs and Transformers are examples of sequence transformations. So are elementwise functions on the sequence elements.\nJust from a quick glance at the list above, it’s quite obvious that many lerning problems naturally involve sequence transformations. For tasks like protein folding and image coloring, each entrie of the output sequence is trying to predict missing information of the same entrie on the input sequence. Thus, for this type of tasks we need the output sequence to have the same length as the input one, and so we our model architectures must be a sequence transformation. But of course that isn’t always the case. In particular our central objective of autoregressive language modeling requires architectures with a different signature \\(\\Seq X \\to Y\\).\nWhat is surprizing is that even for tasks like these, sequence transformations turn out to be an extremely useful building block that is used under the hood. To see why, take a moment to think about how you might construct an architecture with the desired input and output spaces. In one side you have the space \\(\\Seq X\\), where a point might contain an unboudedly large amount of informaiton. On the other, you have a \\(Y\\), which often is a finite dimensional vector space. \\(Y\\) is a much “bounded” space than \\(\\Seq X\\). When designing your architecture you are forced to decide where to place the projection from the variable sized space to the fixed sized one. Given that neural network architectures are allways constructed as the composition of many individual layers, there are two very natural choices of where to place the projection: in the beginning, or at the end. We would call the choice early drop and late drop respectively. Early drop architectures would look something like \\[\n\\Seq X \\to \\R^d \\to \\cdots \\to \\R^d \\to Y\n\\]\n\n\n\\(d\\) represents the width of each layer of the neural network. The arrows represent the individual layers of the neural network.\nand the late drop type would be constructed out of many sequence transforms layers except for the last one \\[\n\\Seq X \\to \\Seq \\R^d \\to \\cdots \\to \\Seq \\R^d \\to Y\n\\]\nOne can definetely construct neural network architectures in the two ways, and we are about to see some examples of both. But it does seem to be the case that all the architectures we actually used in practice (e.g. transformers and RNNs) are of the drop last type. We could speculate about why:\n\nusing sequences interally is a good inductive bias for tasks that are naturally expressed as sequences.\nadaptive computation. More complex tasks have more internal state abailable to solve them\n\nBut honestly, that just leads to a lot of talk with little substance. What is certainly true is that the second type is the one with widespread use, it seems to be the deep learning way of doing things. Moreover, in section 3 we will see how thinking about architectures based on sequence transforms enable one massive optimization to train autoregressive language models.\nTODO: Even when your architecture can be thought of in a different way, sequence transform perspective is what allows you to implement them efficiently on hardware. Unleash all your cores to compute different parts of the output all at the same time. Unconstrain yourself"
  },
  {
    "objectID": "drafts/research/index.html#example-architectures",
    "href": "drafts/research/index.html#example-architectures",
    "title": "An Architecture For Neural Language Modeling",
    "section": "2.3) Example Architectures",
    "text": "2.3) Example Architectures\nTODO: make the point that sequence transforms give us a unified perspective to think about RNNs and Transformers\n\n2.3.1) Recurrent Neural Networks\nAn RNN has the signature \\[\nr(s_{t-1}, x_t) = (s_t, y_t)\n\\]\nFor example, a very basic RNN layer might be something like \\[\ns_{i+1} = \\sigma(W s_i) + x_i  \\;\\;\\;\\;\\;\\;\\;\\;\\;\\; y_i = U s_i\n\\]\nwhere \\(x_i, s_i, y_i \\in \\R^d\\) are the inputs, states and outputs respectively and \\(W, U \\in \\R^{d\\times d}\\) are weight matrices and \\(\\sigma\\) is a nonlinearlity like a sigmoid. Of course, there is an infinitude of variations. The specific step equations matter much less than the general pattern of a layer with the signature of \\(r\\).\nAs written, it’s not clear that we have a sequence transformation in our hads. But once we pick an initial state \\(s_{-1}\\in \\R^d\\), any function with such a recurrent formulation can be Given the input sequence \\(x_1 \\cdots x_t\\) we can unrolled the recurrent step and compute the sequence \\(y_1 \\cdots y_t\\). Thus we have a sequence transform in our hands.\nIn deep learning we will want to stack many such layers \\(r\\). For example let’s consider stacking \\(n\\) layers \\(r\\) to construct a deep RNN. The stack of layers will have the same signature of our basic RNN, the only difference is that the state will consist of \\(n\\) vectors in \\(\\R^d\\). The following diagram represents the computation of a deep RNN with 2 layers and \\(t=3\\). The blue boxes represent each step of unrolling the RNN through time.\n But people don’t tend to implement deep RNNs this way! Anyone who has seen an implementation of an RNN will recognize that the actual computation we implement is instead:\n\n\n\nFile (12)\n\n\nIt’s like we try to avoid the recurrent formulation of a deep RNN and instead prefer to see it as a stack of sequence transformations. But why? Ofc they are mathematicaully equivalent, but there is a difference in practical performance in hardware: cache hits EXPAND FURTHER. We start to see how thinking about sequence transforms seems to have some computational advantages.\nTo create the function we need for our autoregressive LM we could take a stack of RNN layers and on the last layer throw out all the outputs except for \\(y_t\\). That gives us a function with signature \\(\\Seq X\\to Y\\).\n\n\n2.3.2) Transformers\nA transformer layer takes in a sequence of inputs \\(X_1 \\cdots X_t \\in \\R^d\\) and uses weight matrices \\(W_Q, W_K, W_v \\in \\R^{d\\times d}\\) to compute \\(Q_i = W_Q X_i\\) etc.. The outputs of the layer are \\[\nY_i = \\sum_{j=0}^t e^{Q_i K_j^T} V_j\n\\]\nAn the causal transformer layer is \\[\nY_i = \\sum_{j=0}^t e^{Q_i K_j^T} V_j\n\\]\nThey are both clearly sequence transformations since we get \\(t\\) outputs \\(Y_i\\).\nOne massive advantage of transformers over alternative architectures like the RNNs we’ve just seen is that they are highly parallelizable. Our GPUs with many cores can split the work of computing all the \\(Y_i\\) in parallel. You don’t need to finish computing \\(Y_1\\) before you can move on to \\(Y_2\\).\nThe potential for this parallelism is something quite natural when one thinks in terms of sequence transformations \\(h: \\Seq X \\to \\Seq Y\\). There is nothing inherent that says that one must compute the output sequence one step at a time.\nTo create a practical architecture for our autoregressive language modeling we would stack a sereies of transformer layers, normally with some MLPs in between. Just like for RNNs, we would throw out all the outputs of the last layer except for \\(Y_t\\). That will gives us a function with the desired signature \\(\\Seq X\\to Y\\).\nNote how it really doesn’t matter is we use causal or non causal transformers for this task. We can get an autoregressive LM out of each. Yes, there is a very good reason to use causal transformers, but to see why we will need to think carefuly about training costs. Something that we will do in section 3.\n\n\n2.3.3) k-gram MLP\nWe’ve seen a couple of common examples of “drop last” architectures. Let’s give 1 of the “drop first” type. As stated, this is not a common thing to do, and the experienced deep learning practitioner will recognize it’s kind of funky. Let’s do it non the less for the sake of completeness\nInspired by the classic \\(k\\)-gram models. If the input sequence is \\(\\Seq \\R^d\\), we could “stack” the \\(k\\) last inputs and apply an MLP to the vector in \\(\\R^{nd}\\). The last layer of the \\(MLP\\) would map into \\(Y\\).\nIndependent of how well this arch learns. It turns out that it is very poorly behaved computationally compared to RNNs and causal transformers. In section 3 we will see how this architecture is poorly suited for the porpuses of autoregressive language modeling in the same way than non causal transformers are."
  },
  {
    "objectID": "drafts/research/index.html#cost-of-training",
    "href": "drafts/research/index.html#cost-of-training",
    "title": "An Architecture For Neural Language Modeling",
    "section": "3.1) Cost of Training",
    "text": "3.1) Cost of Training\nNow, let’s return to considering autoregressive language modeling, \\(f : \\Seq X \\to \\Dist X\\). How many times do we have to call the underlying function \\(f\\) to evaluate the loss \\(l\\)?\nWhen we evaluate the loss we have to compute \\[ l = \\sum_{x\\in \\Seq_t X} \\sum_{i=1}^t -\\ln[f(x_1 \\cdots x_{i-1})(x_i)]\\]\nFor example, suppose our dataset consists of a single sentence: \\(\\text{\"I like penguins\"}\\). To evaluate the loss we need to evaluate our model \\(f\\) on the empty sequence \\(\\text{\"\"}\\) and the first character \\(\\text{\"I\"}\\) and \\(\\text{\"I \"}\\) and \\(\\text{\"I l\"}\\), \\(\\text{\"I lik\"}\\) etc..\n\\[\nf(\\text{\"\"}) \\\\\nf(\\text{\"I\"}) \\\\\nf(\\text{\"I \"}) \\\\\nf(\\text{\"I l\"}) \\\\\nf(\\text{\"I li\"}) \\\\\n\\vdots\n\\]\n\n\n\n385505895_353915030504501_2594593902089499332_n.jpeg\n\n\nSo, for every sequence \\(x_1 \\cdots x_t \\in D\\), to compute the loss we need to call our model on \\(t\\) different inputs.\nThe motivatiion to find a speed up We have been considering the possibility of using an architecture of the type \\[\nf:\\Seq X \\to \\Seq \\R^d \\to \\cdots \\to \\Seq \\R^d \\to Y\n\\]\nbut it might be dumb that all the steps of the computation \\(f(x\\cdots x_t)\\) involve sequences \\(t\\) and at the end we project down and output a vector.\nPerhaps, we could use a different architecture that was a sequence transformation\n\\[\ng: \\Seq X \\to \\Seq \\R^d \\to \\cdots \\to \\Seq \\R^d \\to \\Seq Y\n\\]\nThe dream would be that \\(g(\\text{\"I like penguins\"}) = [f(\\text{\"I\"}), f(\\text{\"I \"}), f(\\text{\"I l\"}), f(\\text{\"I li\"}), \\cdots]\\), so the output sequence contains all the values we need in a single call to our architecture."
  },
  {
    "objectID": "drafts/research/index.html#causal-sequence-transformations",
    "href": "drafts/research/index.html#causal-sequence-transformations",
    "title": "An Architecture For Neural Language Modeling",
    "section": "3.2) Causal Sequence Transformations",
    "text": "3.2) Causal Sequence Transformations\nThe fundamental principle of causality is that the past does not depend on the future. The way to formalize this intuition is to say that evaluating a causal sequence transformation on a subsequence produces an output sequence that is also a subsequence.\nDefinition We say \\(h\\in \\SeqT(X,Y)\\) is a causal sequence transformation if, given a sequence \\(x_1 \\cdots x_t \\in \\Seq_t X\\) with output sequence \\(y_1 \\cdots y_t = h(x_1 \\cdots x_t)\\), for any \\(i\\le t\\), evaluating \\(h\\) on the subsequence \\(x_1 \\cdots x_i \\in \\Seq_i X\\), the outputs \\(y'_1 \\cdots y'_i = h(x_1 \\cdots x_i)\\) have the property that \\(y_j = y'_j\\) for all \\(j\\le i\\).\nWe refer to \\(\\CSeqT(X, Y) \\subset \\SeqT(X, Y)\\) as the subset of all causal sequence transformations.\nThe reader is encouraged to check that all functions below are causal sequence transformations * RNNs as defined in EQX * Transformers as defined in EQY * Elementwise functions \\(h(x_1 \\cdots x_t) = \\sigma(x_1) \\cdots \\sigma(x_t)\\)\nJust like sequence transformations, causal sequence transformations form a category, meaning that they are closed under composition. This means we can use layers that we know are causal sequence transforms and combine as building blocks into more complex architectures.\nResult: If \\(h\\in \\CSeqT(X, Y)\\) and \\(f\\in \\CSeqT(Y, Z)\\) then \\(f\\circ h \\in \\CSeqT(X,Z)\\). Proof Bruh, why you expanding this proof? Don’t be lazy and do it yourself. It’s real easy.\nFor example, a full transformer architecture usually would be constructed by composing many transformer blocks. Where each block consists of a transformer layer as in EQY followed by an MLP applied elementwise to all outputs of the transformer layer. Thanks to the previous result we know that the full transformer architecture will be causal just by confirming that elementwise functions and EQY are causal sequence transformations.\nAlright, now that we understand causal sequence transformations and we have a vast space of such architectures at our dispoal, let’s see why they are so useful for the problem we were trying to solve.\nDefinition: The “take last” function \\(L: \\CSeqT(X, Y) \\to \\Fn(\\Seq X, Y)\\) is defined the following way: given an \\(h\\in \\CSeqT(X,Y)\\), if \\(x_1 \\cdots x_t \\in \\Seq_t X\\) then \\(L(h)(x_1 \\cdots x_t) = [h(x_1 \\cdots x_t)]_t\\). In other words, if \\(y_1 \\cdots y_t = h(x_1 \\cdots x_t)\\) then \\(L(h)(x_1 \\cdots x_t) = y_t\\).\nResult \\(L\\) is a 1-1 mapping where the inverse “concatifies” a function \\(f \\in \\Fn(\\Seq X, Y)\\) by evaluating it on \\(x_1\\) and \\(x_1, x_2\\) etc.. Concretely: \\[\nL^{-1}(f)(x_1 \\cdots x_t) = \\left(f(x_1), f(x_1, x_2), \\cdots, f(x_1 \\cdots x_t) \\right)\n\\]\nProof First note that, given an \\(f\\in \\Fn(\\Seq X, Y)\\), if \\(h\\) is the concatification of \\(f\\), meaning that \\(h(x_1 \\cdots x_t) = \\left(f(x_1), f(x_1, x_2), \\cdots, f(x_1 \\cdots x_t) \\right)\\) then clearly, \\(L(h)(x_1 \\cdots x_t) = f(x_1 \\cdots x_t)\\).\nWe also need to check the other direction. Given an \\(h\\in \\CSeqT(X, Y)\\): \\[\\begin{align}\nh(x_1 \\cdots x_t) &= \\left([h(x_1 \\cdots x_t)]_1, [h(x_1 \\cdots x_t)]_2, \\cdots, [h(x_1 \\cdots x_t)]_t \\right)  \\\\\n&= \\left([h(x_1)]_1, [h(x_1, x_2)]_2, \\cdots, [h(x_1 \\cdots x_t)]_t \\right)   &  \\text{using the fact that $h$ is causal}  \\\\\n&=  \\left(L(h)(x_1), L(h)(x_1, x_2), \\cdots, L(h)(x_1 \\cdots x_t) \\right) &  \\text{using the definition of $L$}\n\\end{align}\\]\nso if we concatify \\(L(h)\\) we get \\(h\\) back. \\(\\blacksquare\\)\nIn summary, \nWe’ve established a 1-1 mapping between the two function spaces. But computationally there is a very big difference between applying \\(L\\) and applying \\(L^{-1}\\). One is expensive the other is cheap. The color of the arrows represent that.\nWe get another interesting diagram when we combine this result with the previous one we get the following picture (where horizontal lines indicate the function is invertible and vertical ones indicate the function is surjective)\n\n\n\nFile (6)\n\n\nWhen we are trying to construct our distribution over length \\(t\\) sequences we loose no expresivety if we start with an \\(h\\in \\CSeqT(X, \\Dist X)\\) and we apply \\(A_t\\circ L: \\CSeqT(X, \\Dist X) \\to \\Dist \\Seq_t X\\). For any \\(f\\in \\Dist \\Seq_t X\\) there will be an \\(h\\in \\CSeqT(X, \\Dist X)\\) s.t. \\(f = A_t \\circ L (h)\\). So mathematically it really doesn’t matter if our base architecture is \\(g\\in \\Fn(\\Seq X, \\Dist X)\\) or \\(h\\in \\CSeqT(X, \\Dist X)\\), but computationally it makes a massive differene!\nSuppose \\(g = L (h)\\). Then, to evaluate the loss on a single sequence \\(\\text{\"hat\"})\\) we required 3 invokations of the function \\(g\\) \\[\ng(\\text{\"h\"}), \\; g(\\text{\"ha\"}), \\; g(\\text{\"hat\"})\n\\]\nBut when we evaluate \\(h(\\text{\"hat\"})\\), since \\(h = L^{-1} \\circ L (h) = L^{-1} \\circ g\\) then \\[\nh(\\text{\"hat\"}) = L^{-1}(g)(\\text{\"hat\"})  = \\left (g(\\text{\"h\"}), \\; g(\\text{\"ha\"}), \\; g(\\text{\"hat\"})  \\right)\n\\]\nFor a sequence of length \\(t\\), just by evaluating \\(h\\) a single time we get the \\(t\\) evaluations of \\(f\\) that we need for the loss. Using a causal sequence transformation is an incredible bargain. You compute 1 and get \\(t-1\\) for free!"
  },
  {
    "objectID": "drafts/research/index.html#efficient-sampling",
    "href": "drafts/research/index.html#efficient-sampling",
    "title": "An Architecture For Neural Language Modeling",
    "section": "3.3) Efficient Sampling",
    "text": "3.3) Efficient Sampling\nCan we do even better? Let’s consider what would it would look like to follow the autoregressive sampling procedure for the RNN of EQX. First we call \\(g(x_1)\\) which requires \\[\ns_1, y_1 = r(s_0, x_1) \\;\\;\\;\\;\\; z_1 \\sim y_1 \\in \\Dist Z\n\\]\nThen we want to call \\(g(x_1, x_2)\\) which requires \\[\ns_1, y_1 = r(s_0, x_1) \\;\\;\\;\\;\\; s_2, y_2 = r(s_1, x_2) \\;\\;\\;\\;\\; z_2 \\sim y_2 \\in \\Dist Z\n\\]\nAnd we can already see the problem. The second invocation of \\(g\\) internally recomputes the term \\(s_1\\) which was already computed in the first call. This is a consequence that the way we are calling \\(g\\) is very structured. And often there is potential for cacheing values and reusing computation between the individual calls.\nIn the case of an RNN, the way you actually want to sample is to first compute \\(s_1, y_1 = r(s_0, x_1)\\) and sample \\(z_1 \\sim y_1\\). Then reuse \\(s_1\\) and compute \\(s_2, y_2 = r(s_1, x_2)\\) and sample \\(z_2 \\sim y_2\\) and so on. This is a big optimization because we are cacheing computation into the state."
  },
  {
    "objectID": "drafts/research/index.html#state-machines",
    "href": "drafts/research/index.html#state-machines",
    "title": "An Architecture For Neural Language Modeling",
    "section": "3.4) State Machines",
    "text": "3.4) State Machines\nTo optimize training we exchanged \\(t\\) calls to an expensive function \\(g\\) into \\(1\\) call to an equally expensive function \\(h\\). To optimize sampling we will echange \\(t\\) calls to an expensive function \\(g\\) into \\(t\\) calls to a function \\(r\\) which is cheaper because it reuses cached computation from the previous calls.\nDefinition: A state machine A state machine in \\(\\SM(X,Y)\\) is a tuple \\((S, s_0, r)\\) where \\(S\\) is a set called the state space, \\(s_0 \\in S\\) is a point we call the origin and \\(u: S, X \\to S, Y\\) is a step/tick function \\[\nr(s_{t-1}, x_t) = (s_t, y_t)\n\\]\nThere is no difference between these equations and the basic form of an RNN, but there are a few reasons we are choosing to call the object state machine instead of RNN. * We will apply this type of equation to things that in no way can be considered neural networks. A good example is a tape, which we will discuss shortly. Using the term NN (i.e. neural network) to refer to such objects feels silly. * Another reason is that for RNN it is usually assumed that \\(s_t \\in \\R^d\\) for some \\(d\\in \\N\\). In other words, the state has a “fixed size”. As we will now see, to be able to unify transformers and RNNs, it’s very important to think in terms of state machines with states of “growing size”.\nIt’s kind of obvious that, if the whole idea of state machines is to allow for the cacheing of information during the repreated invokations of \\(g\\), there will always be a “worst case state machine” for any \\(g\\): the state machine that caches no computations. We call this the tape state machine because the only thing it stores is the inputs.\nDefinition: The tape state machine \\(P(g)\\in \\SM(X,Y)\\) of a function \\(g\\in \\Fn(\\Seq X, Y)\\) is \\[\nP(g)=( \\underbrace{\\Seq X}_S, \\underbrace{[\\;]}_{s_0}, r_g )\n\\]\nwhere the tick funciton \\(r_g\\) is defined as \\[\nr_g \\bigg( \\underbrace{[x_1 \\cdots x_{t-1}]}_{s_{t-1}}, \\; x_t \\bigg)\n= \\bigg( \\underbrace{[x_1 \\cdots x_t]}_{s_t}, \\; \\underbrace{g(s_t)}_{y_t} \\bigg)\n\\]\nThe tape state machine might seem like a bit of a silly object,\n\nIt helps us see that all \\(g\\) can be thought of as state machiens. Once we have this unified framework to compare sampling costs of different models. We just compare the size of the sates and the cost of every tick. We are going to do that next.\nEven though you never want to run the tape state machine, it’s a useful starting point to start the analysys of imporovements. We can sample from autoregressive MLPs. It’s also useful to construct baselines.\nAnd is useful as a software engineering idea. You can see if the samples from a model are good and only then take the effort to implement a highly optimized state machine. Another aplication is testing. When you are implementing an optimized SM, it’s useful to have a (slow) reference that has the exact same interface and outputs as the funciton you are trying to implement.\n\nResult: The unrolling funciton \\(U: \\SM(X,Y) \\to \\CSeqT(X,Y)\\) is surjective. Given \\(m = (S, s_0, p) \\in \\SM(X,Y)\\) \\[\nU(m)(x_1 \\cdots x_t) = [y_1 \\cdots y_t] \\;\\;\\;\\; \\text{where} \\;\\;\\;\\;  (s_t, y_t) = p(s_{t-1}, x_t)\n\\]\nProof: Use the tape state machine… \\(\\blacksquare\\)\nThe following diagram summarizes the result\n\n\n\nFile (10)\n\n\nNote how \\(P\\circ L\\) behaves similarly to an inverse of \\(U\\) because \\(U \\circ P \\circ L = \\text{id} : \\Fn(X,Y) \\to \\Fn(X,Y)\\). But the other direction is not necessarily true. For example if we start with a classic RNN \\((\\R^d, 0, r)\\) and we \\(P \\circ L \\circ U\\) we end up with tape state machine that is different from the RNN.\nWe can also combine it with the autoregressive correspondance result to get the diagram: \n\nRNN State Machines\n\nTape SM: \\(O(t)\\) memory and \\(O(td^2)\\) compute\n\n\nEfficient SM: \\(O(d)\\) memory and \\(O(d^2)\\) compute\n\n\n\nSelf Attention State Machines\n\nTape SM: \\(O(t)\\) memory and \\(O(t^2d)\\) compute\n\n\nKV cache SM: \\(O(td)\\) memory and \\(O(td)\\) compute\nFunnily enough, this imporvement #### Efficient SM: \\(O(d^2)\\) memory and \\(O(d^2)\\) compute Does it exist for the transformer? Yes under the assumption that…"
  },
  {
    "objectID": "drafts/research/index.html#summary",
    "href": "drafts/research/index.html#summary",
    "title": "An Architecture For Neural Language Modeling",
    "section": "3.5) Summary",
    "text": "3.5) Summary\nOk, we’ve seen a lot of changes of perspectives. The following diagram sumamrizes what is the relationship between the objects we’ve been using, and what each perspective is useful for.\n\n\n\nFile (1)\n\n\nOur objective now is to construct good architectures with both, an efficient causal sequence transformation and state machine implementations. We will see how causal transformers can be seen as examples of causal sequence transformations, and then we will present our star architecture a small modification that we call linear self attention that retains all the advantages of the transformers but gives a big improvement on the causal sequence implementation\nBut it’s going to be easier to introduce the key ideas that allow for these optimizations in the simpler setting of sequence transformations.\nTransformers are amazing for two general reasons * generalize very well (better than RNNs?) * run very fast on our GPUs (way better than RNNs)\nbut suffer from some disadvantages: * Cost \\(O(t^2)\\) on train on datasets of length \\(t\\) as opposed to \\(O(t)\\) for RNNs * Sampling a sequence of length \\(t\\) has cost \\(O(t^2)\\) vs \\(O(t)\\) for RNNs\nOur empirical results will be focused on an architecture we implemented that we call Self Attention RNN (SARNN) / Self Attention State Machine / We think its a very succseful architecture at combining the best of both worlds because it has the properties: * generalize very well * run very fast on our GPUs * Cost \\(O(t)\\) to train on datasets of length \\(t\\) * Sampling a sequence of length \\(t\\) has cost \\(O(t)\\)"
  },
  {
    "objectID": "drafts/research/index.html#footnotes",
    "href": "drafts/research/index.html#footnotes",
    "title": "An Architecture For Neural Language Modeling",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSequence transformations form a category.↩︎"
  },
  {
    "objectID": "blogposts/faster-after-all/index.html",
    "href": "blogposts/faster-after-all/index.html",
    "title": "Linear Transformers Are Faster After All",
    "section": "",
    "text": "\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\Z}{\\mathbb{Z}}\n\\newcommand{\\N}{\\mathbb{N}}\n\\newcommand{\\sft}{\\text{softmax}}\n\\newcommand{\\List}{\\text{List}}\n\\newcommand{\\Seq}{\\text{Seq}}\n\\newcommand{\\SeqT}{\\text{SeqT}}\n\\newcommand{\\CSeqT}{\\text{CSeqT}}\n\\newcommand{\\Dist}{\\text{Dist}}\n\\newcommand{\\SM}{\\text{SM}}\n\\newcommand{\\Fn}{\\text{Fn}}\n\\newcommand{\\Tok}{\\text{Tok}}\n\\newcommand{\\Aij}{ A_{[i,j]}}\n\\]\nIt is well-known that removing the exponential from the attention layer of a transformer allows for a recurrent reformulation, with computational cost that is linear instead of quadratic on context length [1]. One would expect that such an architecture would be far faster to train, especially when the context size is large. However, when training deep neural networks on hardware accelerators (e.g. GPUs), reductions in FLOPs do not always straightforwardly translate into practical speedups. Initial empirical experiments showed that large language models based on linear transformers train more slowly than classic transformers, and led many to dismiss the approach as nice-in-theory but unhelpful-in-practice [2].\nAt the moment, the conventional wisdom in the field is that a quadratic-cost algorithm with highly optimized hardware utilization (e.g. FlashAttention) gives the best training throughput [3]. But this is mistaken. In this post, we explain several different ways of implementing linear transformers and we show that, in fact, they can produce massive speed-ups.\nThe experiment below showcases the central takeaway. We compare the speed of an optimized transformer, a straightforward recurrent implementation of the linear transformer (as described in [1]), and an optimized linear transformer (described in Section 3). We see that, despite exhibiting better growth with respect to context size, the linear transformer trains much more slowly than the transformer in wall-clock time. In contrast, our algorithm is strictly faster than even the highly-optimized FlashAttention baseline, at all context and model sizes. At moderately large context sizes, this speedup has an larger impact than FlashAttention itself.\nBut speed is only one part of the picture. We also need to know whether linear transformers actually minimize the loss as well as standard transformers do. Unfortunately, as the next plot shows, directly converting GPT2 to use linear transformer layers hurts learning significantly.\nThese results are discussed in detail in Section 5, but in short: linear transformers seem to become increasingly unstable as the sequence length grows, negatively affecting learning. This means that our linear transformers are somewhat useless,1 as the positive impact from the speedup seen in long contexts is undermined by the negative impact of degraded learning.\nOther variants of linear transformers have been proposed that claim resolve these learning issues [5]–[11], but we do not survey them today. Since our main motivation in this post is to share our insights on how to implement efficient linear transformers we stick with the most natural variant, which is most closely analogous to standard transformers. In a future post, we will explain how to improve the learning of linear transformers, allowing us to efficiently train unprecedentedly-long-context language models with super-fast sampling."
  },
  {
    "objectID": "blogposts/faster-after-all/index.html#linear-transformers",
    "href": "blogposts/faster-after-all/index.html#linear-transformers",
    "title": "Linear Transformers Are Faster After All",
    "section": "1. Linear Transformers",
    "text": "1. Linear Transformers\nThe inputs to a transformer layer are sequences of \\(Q_i, K_i, V_i \\in \\R^d\\) of query, key and values, where \\(i\\) ranges from \\(1\\) to the sequence length \\(t\\). The outputs are a sequence \\(Y_i\\in \\R^d\\). The well-known formula for the transformer layer, first popularized by Vaswani et al [12], is: \\[\nY_i^\\text{Transformer} = \\sum_{j=1}^i e^{Q^T_i K_j} V_j\n\\] Here we are excluding the denominator of the softmax for simplicity of notation. Although the normalization provided by the denominator is important for good learning performance, it doesn’t have a major impact on the computational cost or implementation speed, which are our main focus here.\n\n\nEven though we omit the normalization for the mathematical exposition, it is included in our implementation for all experiments.\nThe formula for the linear transformer (LT) layer is quite similar: just change the term \\(e^{Q^T_i K_j} \\to  Q^T_i K_j\\) yielding \\[\nY_i^\\text{LinearTransformer} = \\sum_{j=1}^i Q^T_i K_j V_j\n\\]\n\n\nAll our experiments with linear transformers also include a normalization factor, which we empirically found to be important for learning performance. Drawing on [1], we divide each \\(Y_i\\) by \\(\\sum_{j=1}^i Q^T_i K_j\\) after eunsuring the sum is positive by making keys and queries live in the positive quadrant using softplus.\nThis layer is “linear” in that the outputs \\(Y\\) are linearly related to all of \\(Q\\), \\(K\\), and \\(V\\).2 From now on, we will omit the superscript of \\(Y_i^\\text{LinearTransformer}\\) and just write \\(Y_i\\). To begin our exploration of the computational cost of linear transformers, consider the following implementation.\ndef LT_attention(Q, K, V):\n    \"\"\"\n    Shapes of inputs are\n     Q: [t, d]  K: [t, d]  V: [t, d]\n    Shapes of outputs are\n     Y: [t, d]\n    \"\"\"\n    t, d = Q.shape\n    Y_list = []\n    for i in range(t):           # loop cost: O(t^2 d)\n        Y_i = zeros(d)\n        Q_i = Q[i]\n        for j in range(i):       # loop cost: O(id)\n            A_ij = inner(K[j], Q_i)  # cost: O(d)\n            Y_i += A_ij * V[j]   # cost: O(d)\n        Y_list.append(Y_i)\n    return stack(Y_list)\nAnyone who has worked with self-attention will recognize that this is a terrible implementation, since nested for-loops are not GPU-friendly. Don’t worry: in the next section, we will discuss implementations that parallelize computation, and thus run much faster on modern hardware. For now, the main point of this pseudocode is to highlight that first mode of computation, which we call attention formulation, has a FLOP cost of \\(O(t^2 d)\\).\nThe key to the massive speedups we saw in the introduction comes from leveraging linearity to restructure the computation. Consider the following factorization: \\[\nY_i = \\sum_{j=1}^i Q^T_i K_j V_j = \\underbrace{ \\left (  \\sum_{j=1}^i V_j  K_j^T\\right )}_{S_i} \\; \\; Q_i\n\\] Written in this form, we notice that the term labeled \\(S_i \\in \\R^{d\\times d}\\) can be thought of as a state summarizing all the relevant information up to time \\(i\\). It’s easy to rewrite into the following recurrent equations \\[\nY_{i} = S_i Q_i\n\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\nS_i = S_{i-1} + V_i K_i^T\n\\] where we assume \\(S_{0} = 0\\in \\R^{d\\times d}\\). Written in this form, we realize that a linear transformer is an RNN. We can also write pseudocode for this approach, which we call the state formulation, and analyze the cost:\ndef LT_state(Q, K, V):\n    \"\"\"\n    Shapes of inputs are\n     Q: [t, d]  K: [t, d]  V: [t, d]\n    Shapes of outputs are\n     Y: [t, d]\n    \"\"\"\n    t, d = Q.shape\n    S_i = zeros(d, d) # shape [d,d]\n    Y_list = []\n    for i in range(t):        # loop cost: O(t d^2)\n        S_i += outer(K[i], V[i]) # cost: O(d^2)\n        Y_i = S_i @ Q[i]      # cost: O(d^2)\n        Y_list.append(Y_i)\n    return stack(Y_list)\nWe see that the cost here is \\(O(t d^2)\\).\nSo, while a standard transformer layer always has cost \\(O(t^2 d)\\), linear transformers have two formulations with different costs. By switching from the attention formulation to the state formulation, we can change the cost from \\(O(t^2 d)\\) to \\(O(t d^2)\\), trading a \\(t\\) term for a \\(d\\) term."
  },
  {
    "objectID": "blogposts/faster-after-all/index.html#parallel-implementations",
    "href": "blogposts/faster-after-all/index.html#parallel-implementations",
    "title": "Linear Transformers Are Faster After All",
    "section": "2. Parallel Implementations",
    "text": "2. Parallel Implementations\nIn general, for-loops are a terrible way to implement anything that will run on a GPU. When using high-level frameworks like PyTorch or JAX, the easiest way to get high GPU utilization is to only use primitives that are already highly optimized for GPU: matrix multiplication, elementwise operations, etc. We can rewrite our attention and state algorithms in this style to make them efficient.\nFirst, let’s do this for attention. Our main technique is to compute the attention matrix \\(A\\), which contains all the terms outer(Q[i], K[j]) that appeared inside the for-loops of LT_attention, using a single heavyweight matrix multiply.\ndef LT_attention_parallel_no_flash(Q, K, V):\n    \"\"\"\n    Shapes of inputs are\n     Q: [t, d]  K: [t, d]  V: [t, d]\n    Shapes of outputs are\n     Y: [t, d]\n    \"\"\"\n    t = Q.shape[0]\n    M = causal_mask(t)\n    A_raw = Q @ K.T  # cost O(t^2 d)\n    A = A_raw * M    # cost O(t^2)\n    Y = A @ V        # cost O(t^2 d)\n    return Y\nThis implementation lies at the core of nearly every transformer of the past five years, powering all of the AI models that have recently exploded in popularity. Since it is composed of such a small number of ultra-parallelizable ops, it obtains high GPU utilization. Recently, specialized flash attention kernels [3] have been used to get even further speedups by avoiding explicitly storing the attention matrix \\(A\\), and thereby saving both memory and time spent on memory-transfers. Algorithmically, though, flash attention is a variant of parallel attention, and has the same computational cost (as measured in FLOPs). We use LT_attention_parallel to refer to the flash attention implementation.\nNext, let’s parallelize the recurrent formulation. This optimization is less well-known. The key is to compute all the terms \\(V_i K^T_i\\) in parallel, and then use a cumulative-sum, which can be parallelized, to combine them.\ndef LT_state_parallel(Q, K, V):\n    \"\"\"\n    Shapes of inputs are\n     Q: [t, d]  K: [t, d]  V: [t, d]\n    Shapes of outputs are\n     Y: [t, d]\n    \"\"\"\n    P = V[:,:,None] @ K[:,None,:]  # cost: O(t d^2)\n    S = cumsum(P, axis=0)          # cost: O(log_2(t) t d^2)\n    Y = S @ Q[:,:,None]            # cost: O(t d^2)\n    return Y[:,:0]\nThe cost in FLOPs of this algorithm is \\(O(\\log_2(t) t d^2)\\).3\nNow that we have four mathematically-equivalent implementations of a linear transformer, let’s time the training step of our GPT2 models and see how big of a difference it makes. For our LT_attention_parallel implementation, we use a custom linear self-attention flash kernel we implemented in Triton [13] based on OpenAI’s FlashAttention2 implementation.\n\n\n\n\nHere are some takeaways:\n\nAs expected, the attention variants all have a quadratic asymptotic cost (slope of 2 on a log-log plot4). The state variants all have linear asymptotic cost (slope 1). 5\nLT_state_parallel is an order-of-magnitude faster than LT_state.\nLT_attention_parallel_no_flash is two orders-of-magnitude faster than LT_attention.\nLT_attention_parallel seems to asymptotically stabilize into being an order-of-magnitude faster than LT_attention_parallel_no_flash.\nFor the majority of settings, LT_attention_parallel is the fastest. (This is the linear version of the algorithm used by the standard transformer.)\nParallel attention is the fastest algorithm for small context sizes. However, LT_state_parallel overcomes LT_attention_parallel_no_flash at around 13k context size, and overcomes LT_attention_parallel at around 100k.\n\nOverall, these results paint a clear picture: use the attention algorithm for small contexts and the state algorithm for large contexts. But do we really face a binary choice? Can we combine the state and attention ideas and get the best of both words?"
  },
  {
    "objectID": "blogposts/faster-after-all/index.html#chunked-formulation",
    "href": "blogposts/faster-after-all/index.html#chunked-formulation",
    "title": "Linear Transformers Are Faster After All",
    "section": "3. Chunked Formulation",
    "text": "3. Chunked Formulation\nIt’s evident that, for small context sizes, computing the \\(t\\) by \\(t\\) attention matrix is much more efficient than computing many \\(d\\) by \\(d\\) state matrices. But as \\(t\\) grows, there is a point where the quadratic cost of the attention matrix ends up dominating. Noticing that attention is extremely efficient for small \\(t\\) and that states are necessary for large \\(t\\) motivates doing one last reworking of the LT equation.\nLet \\(c \\in \\N\\) be a positive integer that we’ll call the chunk size. For any \\(i\\in \\N\\) find the unique \\(n\\in \\Z\\) s.t. \\(cn &lt; i \\le c(n+1)\\). We can easily see that the following equations are equivalent to the previous ones. \\[\nY_{i} = S_{cn}Q_i + \\sum_{j=cn+1}^i Q_i^T K_j V_j\n\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\nS_{c(n+1)} = S_{cn} + \\sum_{j=cn+1}^{c(n+1)} V_j K_j^T\n\\] The key idea is that we are only going to compute a subset of all states: \\(S_0, S_c, S_{2c}, \\cdots\\). Then, to compute each output \\(Y_i\\), we need only to take into account the contribution via the most recent state \\(S_{cn}\\), as well as the contribution (computed via attention) of all moments in time \\(j\\) in the range \\(cn &lt; j \\le i\\).\nAs pseudocode, this looks like:\ndef LT_attention_with_initial_state(S, Q, K, V):\n    \"\"\"\n    Shapes of inputs are\n     S: [d, d]  Q: [c, d]  K: [c, d]  V: [c, d]\n    Shapes of outputs are\n     Y: [c, d]\n    \"\"\"\n    Y_state = Q @ S                               # cost O(c d^2)\n    Y_attention = LT_attention_parallel(Q, K, V)  # cost O(c^2 d)\n    Y = Y_state + Y_attention                     # cost O(cd)\n    return Y\n\ndef LT_chunked(Q, K, V, c):\n    \"\"\"\n    Shapes of inputs are\n     Q: [t, d]  K: [t, d]  V: [t, d], c: int\n    Shapes of outputs are\n     Y: [t, d]\n    \"\"\"\n    t, d = Q.shape\n    assert t % c == 0\n    Q_, K_, V_ = [arr.reshape(t//c, c, d)\n    `               for arr in [Q,K,V]]\n    P_ = K_.transpose([0,2,1]) @ V_  # cost O(t d^2)\n    S_ = cumsum(P_, axis=0) - P_     # cost O(log_2(t/c)(t/c)d^2)\n    Y_ = vmap(LT_attention_with_initial_state, axis=0)(\n                S_, Q_, K_, V_)      # cost O(td^2 + tcd)\n    return Y_.reshape(t, d)\nThe cost is \\(O\\left(td^2 + tcd + \\log_2(t/c)(t/c)d^2\\right)\\), once again avoiding a quadratic dependency on \\(t\\). Also, note that this algorithm makes an inner call to LT_attention_parallel, so we can use a flash-attention kernel to do that part of the computation.\nThis algorithm has a hyperparameter, the chunk size, which must be set correctly for optimal performance. Fortunately, it is inexpensive to identify the best chunk size empirically, since measuring just a few steps of training at each chunk size is sufficient to identify which is the fastest. In the plot below, we plot the speed of the optimally-chunked algorithm in each setting.\n\n\n\n\nWe see LT_chunked gives the desired best-of-both-worlds behavior, as it is equal-or-better than all other approaches in all settings. And we now see the massive (& rapidly growing) speedup relative to standard self-attention, finally unlocking the true power of linear transformers."
  },
  {
    "objectID": "blogposts/faster-after-all/index.html#sampling",
    "href": "blogposts/faster-after-all/index.html#sampling",
    "title": "Linear Transformers Are Faster After All",
    "section": "4. Sampling",
    "text": "4. Sampling\nWhen working with language models, efficient training is not the only performance that deserves consideration. Once the model is trained, how expensive is it to utilize? When we are sampling we have a sequence of tokens, \\(z_1 \\cdots z_t\\), and we want to sample the next token, \\(z_{t+1}\\).\nThe most efficient algorithm to sample from traditional transformers is called the KV-cache algorithm [14]. This algorithm assumes that when we generate token \\(z_{t+1}\\), we will have already computed and cached all the \\(K_i, V_i\\) for all \\(0 \\le i \\le t\\). In order to compute the output of the attention layer at time \\(t+1\\) given this cached information, we can use \\[\nY_{t+1}^\\text{Transformer} = \\sum_{j=1}^{t+1} e^{Q^T_i K_j} V_j\n\\] It is easy to see that this is an \\(O(td)\\) operation. In other words, as the sequence length grows, sampling each subsequent token becomes more computationally expensive.6 This is one of the major limitations of the classic transformer architecture.\nWith linear transformers, however, we have access to the recurrent formulation. A linear transformer is an RNN where the state has size \\(O(d^2)\\). \\[\nY_{i} = S_i Q_i\n\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\nS_i = S_{i-1} + V_i K_i^T\n\\] We can compare the time it takes to generate any particular token when sampling a sequence:\n\n\n\n\nAs expected, we see that the time to sample of the linear transformer is independent of context length, whereas that of the KV-cache transformer grows linearly. This leads to a large gap in inference speed at large contexts.7"
  },
  {
    "objectID": "blogposts/faster-after-all/index.html#learning-performance",
    "href": "blogposts/faster-after-all/index.html#learning-performance",
    "title": "Linear Transformers Are Faster After All",
    "section": "5. Learning Performance",
    "text": "5. Learning Performance\nUntil now, our focus has been on how to implement linear transformers efficiently. But an architecture that runs really fast is useless unless it also learns well. We will now compare the learning curves of the GPT2 baselines with their linear transformer counterparts, at various context sizes.\nIn order to control for the fact that longer contexts help learning by introducing more tokens per update, we hold the number of tokens-per-update constant across context sizes by decreasing batch size. At all context-lengths, each update sees \\(2^{19}\\) tokens.8 Importantly, for this set of experiments, we have used the dataset c4 [4], which does not have much long-term structure: it consists of a shuffled collection of short articles, of average length ~500 tokens.\nFirst, let’s look at the parameter scaling law of GPT2 and our modified Linear-GPT2. The following plot shows the final performance after 24h of training on our 8xH100 server for each scale and each context size. Click the second tab if you want to see the full learning curves.\n\n\n\n\nBoth architectures scale similarly with respect to model size, but there is a gap in performance. This gap seems to grow as context size increases, together with more loss spikes and general instability. It’s possible that the gap can be explained by the linear transformer not effectively utilizing its context. To explore whether or not this is the case, we can use these same experiments, but visualize all context-lengths together.\n\n\n\n\nWe see a dramatically different trend between the two architectures. For GPT2, each increase in context length slows down the initial pace of learning, but ultimately all context-lengths (beyond at least 1024) converge to a similar final loss. Whereas for Linear-GPT2, not only does increasing the context slow down learning in the beginning, but it also causes convergence to a worse final performance and nasty-looking loss spikes.\nThe results for GPT2 are what we would expect from a healthy model, given the setting. Note that since our dataset consists of short articles, of average length ~500 tokens, we can assume that even a context window of 1024 would include nearly everything relevant. Thus, we wouldn’t expect that increasing the context length beyond this point would decrease the loss the model ultimately converges to. Longer-context models do however need to learn to ignore many more irrelevant tokens, explaining the slowed initial learning.9\nIn contrast, the results for Linear-GPT2 seem to indicate some underlying instability of the linear transformer architecture. It doesn’t even seem capable of ignoring irrelevant information, let alone exploiting useful long-term structure.\nRemedying these learning issues will be the main focus of our future work in linear transformers. Our current architecture is essentially a one-to-one clone of GPT2, sticking as architecturally close to the original as possible; aspects such as initializations, normalizations and hyperparameters were directly copied. It is well-known that these decisions can have a large impact on scaling and stability and often need to be tuned in an architecture-specific way. In the literature, various other linear variants of self-attention have been proposed, that incorporate techniques such as gating mechanisms in order to improve stability and learning [5]–[11]. A future post will include a thorough study of the impact of all of these choices.\nUltimately, it may be impossible for any linear transformer to perfectly match the context scaling laws of the classic transformer baseline. Although removing the exponential seems like a minor change, it represents a meaningful decrease in expressivity.10 But as we’ve shown, linear transformers can be trained orders-of-magnitude more efficiently. On equivalent compute, linear transformers can be trained for many more steps; or, keeping steps constant as well, we can train a much larger model. If this additional scaling is sufficient to compensate for the reduced expressivity, linear transformers will be the more efficient architecture overall.\n\n  \n  Subscribe to be notified of new posts:"
  },
  {
    "objectID": "blogposts/faster-after-all/index.html#footnotes",
    "href": "blogposts/faster-after-all/index.html#footnotes",
    "title": "Linear Transformers Are Faster After All",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs discussed in Section 4, a second benefit of linear transformers is that the cost to sample a token does not grow with context size. Perhaps one could argue that this improvement in sampling speed could, on its own, justify using linear transformers for applications where the inference costs vastly exceed training costs. But it is evident to us that, for linear transformers to become actually useful, we need to address these instability issues.↩︎\nIt is not named for the fact that the computational cost is linear with respect to \\(t\\)! That is just a coincidence. (And is not even always true, as we will see.)↩︎\nInterestingly, LT_state_parallel is actually more expensive than LT_state. (This is in contrast with the attention formulation, where LT_attention and LT_attention_parallel share the same \\(O(t^2 d)\\) cost.) As we will see in a moment, this extra \\(\\log_2(t)\\) factor is well worth the parallelization benefits.↩︎\nIf \\(y=x^2\\), a log-log plot where \\(y'=\\log_a(y)\\) and \\(x'=\\log_a(x)\\) for any base \\(a\\), then \\(y'=\\log_a(y) = \\log_a(x^2) = 2 \\log_a(x) = 2 x'\\). So the graph will be a line with slope 2.↩︎\nThe reason we see the expected slopes asymptotically is that we are timing a full GPT2 architecture which has many other components besideds the attention layer. If we were only timing the attention layer, the plots would all be straight lines.↩︎\nAn interesting connection is that the KV-cache can be understood as the state of an RNN with non-constant state size; namely, one whose state-size is \\(O(td)\\).↩︎\nThis comparison may not be completely fair. In these experiments, our implementation of neither sampling algorithm makes use of specialized kernels. A lot of the ideas of flash attention can be used to write a much faster KV cache sampling algorithm; on the other hand, it’s unclear if much improvement is possible on the recurrent sampling. Thus, it’s possible that with engineering effort the gap between the two algorithms could become smaller. However, the overall pattern will certainly remain the same.↩︎\ne.g. runs with context-size 1024 would have batch-size of \\(2^{19} / 2^{10} = 2^{9} = 512\\).↩︎\nPut another way: doubling the size of the input vastly increases the size of the function space over which gradient descent must search, and it’s intuitive that in a larger space it takes somewhat longer to find a good solution.↩︎\nWe plan to elaborate on this topic in a future blog post.↩︎"
  },
  {
    "objectID": "coming-soon.html",
    "href": "coming-soon.html",
    "title": "Manifest AI",
    "section": "",
    "text": "Coming Soon\nThe page you requested is an upcoming release and not yet available.\nIf you believe this to be in error please reach out to us at\ncontact at manifest ai dot com"
  },
  {
    "objectID": "blogposts/mission/index.html",
    "href": "blogposts/mission/index.html",
    "title": "Our Mission",
    "section": "",
    "text": "A poem is a shadow of the act of writing poetry.\nHumanity casts many shadows. All literature, letters, recipes, and tweets. Mathematical proofs and git repositories. Laws, treaties, and declarations of war. Financial statements, employee performance reports, and bankruptcy filings. Podcasts, operas, accidental voicemails, YouTube videos and Hollywood blockbusters. Restaurant reviews and love letters and times tables. Individually, each of these pieces of information is nothing but the faintest shadow of the process that produced it. But collectively, these shadows tell a rich story about the world.\nSince the dawn of humanity, the brain alone could reconstruct the world from these shadows. But the last decade of deep learning has convinced us that this won’t be the case for much longer. Though significant challenges remain, we stand poised to solve them. If successful, it will become possible to synthesize every documentable aspect of humanity into the weights of a neural network.\nOur mission is to train a neural network to model all human output.\nThere are two primary challenges in our pursuit of this goal.\n\nCurrently, it’s not technically feasible to train a model that can ingest all the data that we can collect. Limitations around context length, modality, and throughput force us to use only a small subset of the data available to us.\nMuch of the data we will need has never been collected, curated, and organized into datasets that we can use for training.\n\nWe are building a world-class team of engineers and researchers to tackle these challenges, united around shared principles and a specific research agenda. We value clear thinking, sharing knowledge, and an extreme commitment to scientific honesty. Our research is guided by mathematical beauty and grounded in rigorous empiricism. We are committed to letting the quality of our work speak for itself. No hype, no fluff. All meat.\nIf this vision resonates, please reach out:          777b7a607577605479757a7d72716760757d3a777b79\n\n\n  \n  Or, subscribe to be notified of new posts:"
  },
  {
    "objectID": "drafts/research.html",
    "href": "drafts/research.html",
    "title": "Research",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\n\n\n\n\nDec 27, 2023\n\n\nAn Architecture For Neural Language Modeling\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "articles/longcrawl64/index.html",
    "href": "articles/longcrawl64/index.html",
    "title": "LongCrawl64: A Long-Context Natural-Language Dataset",
    "section": "",
    "text": "As part of our broader mission of training language models with ultra-long context, we are releasing a dataset for use in research on architectures and algorithms for long-context modeling. This dataset, which we call LongCrawl64, is available for download. It consists of 6,661,465 pre-tokenized documents, each of which is 65,536 tokens long, for a total token count of 435 billion."
  },
  {
    "objectID": "articles/longcrawl64/index.html#footnotes",
    "href": "articles/longcrawl64/index.html#footnotes",
    "title": "LongCrawl64: A Long-Context Natural-Language Dataset",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe use the same criteria as [1].↩︎\nThe SI prefix “kibi” means 1024, so 64 KiT = 65536 tokens.↩︎\nBy “roll”, we mean in the numpy.roll sense. For example, rolling [12, 5, 7, 4, 21] by 3 would yield [7, 4, 21, 12, 5]. This preprocessing step causes us to sometimes be predicting tokens from the start of a document, conditional on tokens from its end. This is atypical, but completely legitimate; we invite any skeptics to watch the Star Wars movies in release order, beginning with Episode IV.↩︎\nWe split this into a train set of 6,609,334 documents and a heldout set of 52,131 documents.↩︎"
  },
  {
    "objectID": "articles/mission/index.html",
    "href": "articles/mission/index.html",
    "title": "Our Mission",
    "section": "",
    "text": "A poem is a shadow of the act of writing poetry.\nHumanity casts many shadows. All literature, letters, recipes, and tweets. Mathematical proofs and git repositories. Laws, treaties, and declarations of war. Financial statements, employee performance reports, and bankruptcy filings. Podcasts, operas, accidental voicemails, YouTube videos and Hollywood blockbusters. Restaurant reviews and love letters and times tables. Individually, each of these pieces of information is nothing but the faintest shadow of the process that produced it. But collectively, these shadows tell a rich story about the world.\nSince the dawn of humanity, the brain alone could reconstruct the world from these shadows. But the last decade of deep learning has convinced us that this won’t be the case for much longer. Though significant challenges remain, we stand poised to solve them. If successful, it will become possible to synthesize every documentable aspect of humanity into the weights of a neural network.\nOur mission is to train a neural network to model all human output.\nThere are two primary challenges in our pursuit of this goal.\n\nCurrently, it’s not technically feasible to train a model that can ingest all the data that we can collect. Limitations around context length, modality, and throughput force us to use only a small subset of the data available to us.\nMuch of the data we will need has never been collected, curated, and organized into datasets that we can use for training.\n\nWe are building a world-class team of engineers and researchers to tackle these challenges, united around shared principles and a specific research agenda. We value clear thinking, sharing knowledge, and an extreme commitment to scientific honesty. Our research is guided by mathematical beauty and grounded in rigorous empiricism. We are committed to letting the quality of our work speak for itself. No hype, no fluff. All meat.\nIf this vision resonates, please reach out:          777b7a607577605479757a7d72716760757d3a777b79\n\n\n  \n  Or, subscribe to be notified of new posts:"
  },
  {
    "objectID": "articles/compute-optimal-context-size/index.html",
    "href": "articles/compute-optimal-context-size/index.html",
    "title": "Compute-Optimal Context Size",
    "section": "",
    "text": "The objective of language modeling is to predict each token in a sequence. Each prediction is conditional on a subset of previous tokens, which are called the context for the prediction. Intuitively, expanding the context should make prediction task strictly easier. If an extra token provides relevant information, the model can learn to use it; otherwise, the model can simply ignore it. Therefore, given any well-trained language model, we expect the average loss to be lower on longer-context predictions. To verify this, we trained a 772-million-parameter transformer1 with context size 32 kibitokens (KiT).\nWe refer to this as a contextwise loss curve, and the general phenomenon of the loss improving on longer contexts as inference-time context scaling. This trend is not specific to our training setup, and has been observed elsewhere in the literature. For example, below is a plot from Gemini [1], illustrating the same effect.\nInference-time context scaling provides a quantitative justification for increasing the training context. Intuitively, training on longer contexts increases the extent to which we can leverage inference-time context scaling, ultimately decreasing loss. This motivates an approach to selecting the size of the training context: choose the context size that optimizes loss given training budget.2 This is a natural complement to existing research on scaling laws. For example, Kaplan et al [2] and Hoffmann et al [3] investigated the optimal way to scale the model size & amount of tokens seen, but both works held context length fixed. To complete this analysis, one must optimize over context length as well.\nIn Section 3, we will do exactly this, using GPT-2-style transformers at scales ranging from 124 million to 1.6 billion parameters. The results show that the optimal training-context length increases with larger training budgets. But devising a proper experimental setting to compare between train-context lengths is surprisingly tricky. It turns out that popular datasets (such as openwebtext or C4) and standard metrics (average train loss) are inappropriate. We begin by discussing these two subtle but important details: in Section 1, we address the choice of dataset, and in Section 2, we address the choice of evaluation metric.\nWe conclude with Section 4, a discussion of some applications that are unlocked by models with ultra-long contexts, from kilotokens up to petatokens. But the vast potential of models with ultra-long contexts cannot be realized if they are trained in a setting that is far from compute-optimal. And so, we need research focused on increasing the optimal training-context size. We believe that careful evaluation of context scaling will be an essential ingredient in progress, and hope that the dataset, ideas, and evaluations presented in this article will prove useful towards that objective."
  },
  {
    "objectID": "articles/compute-optimal-context-size/index.html#data-with-long-term-structure",
    "href": "articles/compute-optimal-context-size/index.html#data-with-long-term-structure",
    "title": "Compute-Optimal Context Size",
    "section": "1. Data with Long-term Structure",
    "text": "1. Data with Long-term Structure\nBelow is a contextwise loss curve similar to the ones in the introduction. It shows the average loss at every context length for a 1.6-billion-parameter model trained using 8 KiT of context on openwebtext. In the first part of the curve, this plot shows contextwise scaling, with performance improving as more tokens are seen. But the trend of improvement tapers off. After around 2 KiT, additional tokens no longer improve the loss.\n\n\n\n\nTo understand the reason for this, one only need look at the document-length distribution of the openwebtext dataset.\n\n\n\n\nOver 90% of the documents are less than 2 KiT long. In order to train train 8-KiT-context models on this dataset, somehow longer documents must be constructed out of smaller ones (in our experiments, we simply concatenated multiple documents). But the resulting “long” documents do not truly contain any long-term structure, and so there is no benefit to seeing additional tokens at inference-time.\nThis problem is not restricted to openwebtext. Many other popular datasets, such as C4 and RedPajama, have similar document-length distributions. This is insufficient for our goals, because it does not allow one to thoroughly evaluate contextwise scaling properties.\nTo solve this issue, we created LongCrawl64, a large natural langauge dataset composed entirely of documents of length 64 KiT. This data is a subset of Common Crawl, tokenized using OpenAI’s TikToken and with short documents filtered out. The end result is a 6661465 x 65336 Zarr array of uint16s, representing 6,661,465 documents each of size 64 KiT. The total token count is 435 billion, two orders of magnitude larger than openwebtext (6 billion). Read our release for the details around the construction and usage of the dataset; for example, how to efficiently load documents when training at context lengths shorter than 64 KiT.\nArmed with this new dataset, we can repeat our experiment and again compute the contextwise loss curve of a 1.6-billion-parameter transformer with context size 8 KiT:\n\n\n\n\nOn LongCrawl64, we see consistent contextwise scaling throughout the train context. With this first issue resolved, let’s move on to the second."
  },
  {
    "objectID": "articles/compute-optimal-context-size/index.html#the-training-loss-is-misleading",
    "href": "articles/compute-optimal-context-size/index.html#the-training-loss-is-misleading",
    "title": "Compute-Optimal Context Size",
    "section": "2. The Training Loss is Misleading",
    "text": "2. The Training Loss is Misleading\nBelow, we show the contextwise loss curves for two trained transformers. The average training loss of each model is given by a dotted line. The details of training are not relevant for this section3, so we will simply call them Model A and Model B. But an important difference between the two is that Model A is trained with 4 KiT of context, and Model B with 16 KiT.\n\n\n\n\nModel B has better training loss (2.244) than Model A (2.253). But do we truly prefer Model B? Note that Model A makes better predictions than Model B at every context length where they can be compared. Furthermore, Model A with a 4 KiT context reaches a lower loss than the 16 KiT model ever does. This means that at inference time, if we had 16 KiT of context available, we would be better off throwing away the first 12 KiT of context and feeding the remainder to Model A, instead of feeding all 16 KiT to Model B. Doing so would result in better predictions. In fact, there is no situation where we prefer Model B.\nWhy does the training loss mislead us? The training loss can be computed as the average of the contextwise loss curve, where the x-axis ranges from 1 to the training-context size. For a 16 KiT model, a much larger component of the training loss comes from situations where the model has a large amount of information in its context. For example, if we look at the proportion of the training loss coming from predictions with at least 3 KiT of context, we see that for Model A this is only 25%, whereas for Model B it is over 80%.\nThe upshot is: when comparing models trained with different context sizes, the training loss inaccurately ranks their performance. In order to select the optimal training-context size, we must find a more reliable metric to optimize.\nIntuitively, we want our metric to reflect the model’s ability to predict the next token at inference time. If we make the assumption that the users of the model have access to arbitrarily many tokens to put in the context, then a natural metric would be the lowest loss that the model attains at any context size. We refer to this as the best-context loss. To measure the best-context loss, compute the contextwise loss curve, and take its minimum.\n\n\nConsider, for example, the common practice of “prompting” a chatbot: pre-placing tokens into the context ahead of the user’s query. Conventional wisdom holds that longer and more thorough prompts improve final performance. If a maximum-length prompt is always utilized, our assumption is fulfilled, and best-context loss drives performance.\nIn fact, since the transformer we’ve been working with uses rotary embeddings, we can evaluate it beyond its training context. And, with the LongCrawl64 dataset, we have data with long-term structure up to 64 KiT. Thus, we can extend the contextwise scaling plots up to 64 KiT:\n\n\n\n\nBeyond the context size used during training, there is a rapid deterioration of prediction ability. Clearly, this model does not generalize well to the beyond-train-context regime. We’ve observed this exact same phenomenon for transformers of all sizes trained on all sorts of context sizes.\n\n\nEven though there have many claims that language models can generalize beyond their training context [4]–[6], to the best of our knowledge, nobody has shown a model for which the loss on natural-language text monotonically decreases with the context size. We consider this to be the true criterion for “sequence length generalization”.\nThis empirical fact is unfortunate, but has a silver lining: it simplifies measurement of the best-context loss. For models that do not generalize beyond their training context, we can measure the best-context loss by simply reporting the loss at the largest context size seen during training.4 This is the approach that we take in this article. But note that it is merely a convenient heuristic, and is valid only when working with models that fail to generalize in this way."
  },
  {
    "objectID": "articles/compute-optimal-context-size/index.html#context-scaling-experiments",
    "href": "articles/compute-optimal-context-size/index.html#context-scaling-experiments",
    "title": "Compute-Optimal Context Size",
    "section": "3. Context Scaling Experiments",
    "text": "3. Context Scaling Experiments\nWith our experimental setting established, it is time to evaluate scaling trends for the train-context size of transformers. The basic experiment we conducted is: train GPT-2 + rotary embeddings + flash attention, for a variety of parameter counts (124 million, 354 million, 772 million, 1.6 billion) and a variety of train-context sizes (128 tokens, 256 tokens, 512 tokens, …, 64 KiT). Each training run used 8 H100 GPUs with data parallel, and ran for 160 GPU-hours. We kept the batch size (i.e. number of tokens per gradient step) constant, so that, as the context size ranged from 128 to 64KiT, the number of documents per update varied from 2048 to 8.\nThe results of this experiment are visualized in the plot below. The x-axis of is the context size used during training. The y-axis is the best-context loss. Every line corresponds to a different model size. The training resources (in terms of GPU hours) can be interactively controlled via the slider. The colored circles show the optimal train context at each model size, and the dashed line shows the overall optimum.\n\n\n\n\nYou can see that varying the context size tends draw a U-shaped curve at all resource levels. Picking too small or too large a context size results in severely degraded performance. By playing with the slider you can adjust amount of training resources and confirm that this trend holds generally.\nIt is clear from this data that for any model size we should grow the context size with the training resources. We can directly visualize this trend with a second plot. For each model size, we plot a line with the hours of training on the x-axis, and the optimal context size on the y-axis.5\n\n\n\n\nClearly, as more resources become available we should train our models using longer context sizes. Also, an interesting observation is that the optimal context size grows more slowly for larger models.\nSo far, we’ve just been looking at the optimal context size for a given model scale. What if we select for the optimal combination of model size and context size?\n\n\n\n\n\n\nIdeally, we would quantify these trends and propose scaling laws. This would merely require extending our methodology by a few additional orders of magnitude of model scale, and to sweep over a few other hyperparamters (e.g. learning rate). This is beyond our current capacity, and we cannot meaningfully extrapolate from existing experiments, so we leave quantitative context-size scaling laws to future work.\nAs we expected, we see that as resources grow one wants to increase both model size and train-context size. But, relative to the previous plot (where we held model size fixed), the growth of the optimal context size noticeably slows down. This seems to be a consequence of the fact that, with a larger GPU hour budget, we want to use larger model sizes, and the optimal context size for those larger models tends to grow slower."
  },
  {
    "objectID": "articles/compute-optimal-context-size/index.html#final-thoughts-on-context-scaling",
    "href": "articles/compute-optimal-context-size/index.html#final-thoughts-on-context-scaling",
    "title": "Compute-Optimal Context Size",
    "section": "4. Final Thoughts On Context Scaling",
    "text": "4. Final Thoughts On Context Scaling\nAt Manifest AI, we believe that context size is one of the most important bottlenecks the field of AI is facing. Many of the most important applications of the technology are just waiting to be unlocked once sufficiently-long-context models become available. Most likely, we will be surprised by which applications end up being most important, but here are some guesses as to what type of use cases will become possible at every context-size scale:\n\nkilotoken scale: Read & write emails. Hold a short chatbot-style conversation. Customize behavior with a prompt. Few-shot learning with a small number of examples.\nmegatoken scale: Write books. Review news articles. Read & edit code. Answer questions from a large scientific literature. Navigate web interfaces.\ngigatoken scale: Read everything tweeted in a day and summarize global opinion. Execute a full software engineering workflow. In-context learning of entire datasets (replacing fine-tuning). Solve complex mathematical problems by iteratively improving over many proof attempts.\nteratoken scale: Manipulate all the data created by a corporation (contracts, documents, emails, etc).\npetatoken scale: Coordinate the affairs of an entire society by integrating all information it produces.\n\nIn light of this astonishing potential, it is tempting to simply always train on the longest context that is computationally feasible. But, as our experiments indicate, naively increasing the train context merely leads to models which are massively under-performant – able to ingest long contexts but unable to use their contents to make good predictions. The goal is not merely to train on long contexts, but to efficiently train on long contexts, by finding a setting where long contexts are compute-optimal. This is what it will take to truly leverage vast context sizes.\nSuch a setting will likely require radical algorithmic and architectural changes. An example of research that has successfully pushed the context size frontier is flash attention [7]. The reason is that it can decrease the cost of training with long contexts. That is also why we are excited about linear transformers, which reduce the cost of training on a context of length \\(t\\) from \\(O(t^2)\\) to \\(O(t)\\). Another angle that seems important is to develop models that generalize beyond the training context, in the specific sense that the contextwise loss curve keeps improving beyond the context size used for training.\nWe hope that the mindset, methodology, and dataset introduced in this article will be helpful in progressing to the petatoken scale and beyond.\n\n  \n  Subscribe to be notified of new posts:"
  },
  {
    "objectID": "articles/compute-optimal-context-size/index.html#footnotes",
    "href": "articles/compute-optimal-context-size/index.html#footnotes",
    "title": "Compute-Optimal Context Size",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor full experimental details, see Section 3.↩︎\nOur approach can be contrasted with the common mindset of train models with the largest context that the training budget will permit.↩︎\nFor those who are curious: both models are 124 million parameters and were trained on LongCrawl64 for 50,000 steps.↩︎\nIn practice, we take the average loss for the final 10% of the training context, which is less noisy.↩︎\nThe optimal context size tends to jump around due to noise in the loss, so this plot is smoothed by taking the most common context size in any given window.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Manifest AI",
    "section": "",
    "text": "writing\n\n\n\nDate\nTitle\n\n\n\n\nJan 5, 2024\nOur Mission\n\n\nJan 5, 2024\nLinear Transformers Are Faster\n\n\nMay 16, 2024\nCompute-Optimal Context Size\n\n\nAug 13, 2024\nLongCrawl64: A Long-Context Dataset\n\n\n\n\n  \n  Subscribe to posts:\n  \n  \n  \n  \n\n\n\nabout us\nCarles Gelada and Jacob Buckman. We each have 8+ years of deep learning research at institutions like OpenAI, Google Brain, and Mila. Our research has been published at NeurIPS, ICLR, ICML, etc., and been cited 1000+ times. After five years of academic collaboration, we founded Manifest AI in March 2023, backed by Decibel.\n\n\njoin us\nWe are hiring core technical team members.\nThe role has elements of both software engineering and research. Responsibilities include implementing deep learning architectures, deriving algorithms, developing research infrastructure, running large-scale experiments, and interpreting and communicating results.\nWe will work well together if you are independent-minded, capable of self-teaching, and value thinking from first principles. Skills we are looking for include comfort with mathematics, strong communication, deep knowledge in areas like CUDA, XLA/MLIR, Jax, or distributed systems/HPC, and experience training large-scale deep learning models.\nWe do not care about formal credentials. If you share our vision and would like to get involved, please send an example of some technical work that you are proud of to 777b7a607577605479757a7d72716760757d3a777b79"
  }
]