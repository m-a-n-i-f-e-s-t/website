[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Manifest AI",
    "section": "",
    "text": "Subscribe for updates:\n  \n  \n  \n  \n\n\nmodels\nPowerCoder-3B\n\n\ncode\nPower Retention\nVidrial\n\n\nresearch\nScaling Context Requires Rethinking Attention\n\n\nwriting\n\n\n\nDate\nTitle\n\n\n\n\nJan 1, 2024\nOur Mission\n\n\nJan 5, 2024\nLinear Transformers Are Faster\n\n\nMay 16, 2024\nCompute-Optimal Context Size\n\n\nAug 14, 2024\nLongCrawl64: A Long-Context Dataset\n\n\nAug 15, 2024\nSymmetric Power Transformers\n\n\nSep 23, 2024\nWhy Gradient Descent Minimizes Training Loss\n\n\nSep 23, 2025\nRelease: Power Retention\n\n\nSep 23, 2025\nWhat is Power Retention?\n\n\nSep 23, 2025\nWhat is Vidrial?\n\n\n\n\n\nmedia\nThis Research Lab Is Giving AI A Better Memory (Forbes)\nRecurrence & Attention For Long-Context Transformers (TWIML)\nBeyond Transformers With Power Retention (Latent Space)\nMoving Beyond Transformers To Scale In-Context Learning (DeMI)\n\n\nabout us\nCarles Gelada, Jacob Buckman, Sean Zhang. We are 8+ year veteran deep learning researchers, formerly at labs like OpenAI, Google Brain, and Meta. Our research has been published at NeurIPS, ICLR, ICML, etc., and been cited 2000+ times. We are backed by a small group of highly technical long-term investors including Decibel and True.\n\n\njoin us\nWe are hiring core technical team members.\nThe role has elements of both software engineering and research. Responsibilities include implementing deep learning architectures, deriving algorithms, developing research infrastructure, running large-scale experiments, and interpreting and communicating results.\nWe will work well together if you are independent-minded, capable of self-teaching, and value thinking from first principles. Skills we are looking for include comfort with mathematics, strong communication, deep knowledge in areas like CUDA, XLA/MLIR, Jax, or distributed systems/HPC, and experience training large-scale deep learning models.\nWe do not care about formal credentials. If you share our vision and would like to get involved, please send an example of some technical work that you are proud of to 777b7a607577605479757a7d72716760757d3a777b79"
  },
  {
    "objectID": "blogposts/faster-after-all/index.html",
    "href": "blogposts/faster-after-all/index.html",
    "title": "Linear Transformers Are Faster After All",
    "section": "",
    "text": "\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\Z}{\\mathbb{Z}}\n\\newcommand{\\N}{\\mathbb{N}}\n\\newcommand{\\sft}{\\text{softmax}}\n\\newcommand{\\List}{\\text{List}}\n\\newcommand{\\Seq}{\\text{Seq}}\n\\newcommand{\\SeqT}{\\text{SeqT}}\n\\newcommand{\\CSeqT}{\\text{CSeqT}}\n\\newcommand{\\Dist}{\\text{Dist}}\n\\newcommand{\\SM}{\\text{SM}}\n\\newcommand{\\Fn}{\\text{Fn}}\n\\newcommand{\\Tok}{\\text{Tok}}\n\\newcommand{\\Aij}{ A_{[i,j]}}\n\\]\nIt is well-known that removing the exponential from the attention layer of a transformer allows for a recurrent reformulation, with computational cost that is linear instead of quadratic on context length [1]. One would expect that such an architecture would be far faster to train, especially when the context size is large. However, when training deep neural networks on hardware accelerators (e.g. GPUs), reductions in FLOPs do not always straightforwardly translate into practical speedups. Initial empirical experiments showed that large language models based on linear transformers train more slowly than classic transformers, and led many to dismiss the approach as nice-in-theory but unhelpful-in-practice [2].\nAt the moment, the conventional wisdom in the field is that a quadratic-cost algorithm with highly optimized hardware utilization (e.g. FlashAttention) gives the best training throughput [3]. But this is mistaken. In this post, we explain several different ways of implementing linear transformers and we show that, in fact, they can produce massive speed-ups.\nThe experiment below showcases the central takeaway. We compare the speed of an optimized transformer, a straightforward recurrent implementation of the linear transformer (as described in [1]), and an optimized linear transformer (described in Section 3). We see that, despite exhibiting better growth with respect to context size, the linear transformer trains much more slowly than the transformer in wall-clock time. In contrast, our algorithm is strictly faster than even the highly-optimized FlashAttention baseline, at all context and model sizes. At moderately large context sizes, this speedup has an larger impact than FlashAttention itself.\nBut speed is only one part of the picture. We also need to know whether linear transformers actually minimize the loss as well as standard transformers do. Unfortunately, as the next plot shows, directly converting GPT2 to use linear transformer layers hurts learning significantly.\nThese results are discussed in detail in Section 5, but in short: linear transformers seem to become increasingly unstable as the sequence length grows, negatively affecting learning. This means that our linear transformers are somewhat useless,1 as the positive impact from the speedup seen in long contexts is undermined by the negative impact of degraded learning.\nOther variants of linear transformers have been proposed that claim resolve these learning issues [5]–[11], but we do not survey them today. Since our main motivation in this post is to share our insights on how to implement efficient linear transformers we stick with the most natural variant, which is most closely analogous to standard transformers. In a future post, we will explain how to improve the learning of linear transformers, allowing us to efficiently train unprecedentedly-long-context language models with super-fast sampling."
  },
  {
    "objectID": "blogposts/faster-after-all/index.html#linear-transformers",
    "href": "blogposts/faster-after-all/index.html#linear-transformers",
    "title": "Linear Transformers Are Faster After All",
    "section": "1. Linear Transformers",
    "text": "1. Linear Transformers\nThe inputs to a transformer layer are sequences of \\(Q_i, K_i, V_i \\in \\R^d\\) of query, key and values, where \\(i\\) ranges from \\(1\\) to the sequence length \\(t\\). The outputs are a sequence \\(Y_i\\in \\R^d\\). The well-known formula for the transformer layer, first popularized by Vaswani et al [12], is: \\[\nY_i^\\text{Transformer} = \\sum_{j=1}^i e^{Q^T_i K_j} V_j\n\\] Here we are excluding the denominator of the softmax for simplicity of notation. Although the normalization provided by the denominator is important for good learning performance, it doesn’t have a major impact on the computational cost or implementation speed, which are our main focus here.\n\n\nEven though we omit the normalization for the mathematical exposition, it is included in our implementation for all experiments.\nThe formula for the linear transformer (LT) layer is quite similar: just change the term \\(e^{Q^T_i K_j} \\to  Q^T_i K_j\\) yielding \\[\nY_i^\\text{LinearTransformer} = \\sum_{j=1}^i Q^T_i K_j V_j\n\\]\n\n\nAll our experiments with linear transformers also include a normalization factor, which we empirically found to be important for learning performance. Drawing on [1], we divide each \\(Y_i\\) by \\(\\sum_{j=1}^i Q^T_i K_j\\) after eunsuring the sum is positive by making keys and queries live in the positive quadrant using softplus.\nThis layer is “linear” in that the outputs \\(Y\\) are linearly related to all of \\(Q\\), \\(K\\), and \\(V\\).2 From now on, we will omit the superscript of \\(Y_i^\\text{LinearTransformer}\\) and just write \\(Y_i\\). To begin our exploration of the computational cost of linear transformers, consider the following implementation.\ndef LT_attention(Q, K, V):\n    \"\"\"\n    Shapes of inputs are\n     Q: [t, d]  K: [t, d]  V: [t, d]\n    Shapes of outputs are\n     Y: [t, d]\n    \"\"\"\n    t, d = Q.shape\n    Y_list = []\n    for i in range(t):           # loop cost: O(t^2 d)\n        Y_i = zeros(d)\n        Q_i = Q[i]\n        for j in range(i+1):     # loop cost: O(id)\n            A_ij = inner(K[j], Q_i)  # cost: O(d)\n            Y_i += A_ij * V[j]   # cost: O(d)\n        Y_list.append(Y_i)\n    return stack(Y_list)\nAnyone who has worked with self-attention will recognize that this is a terrible implementation, since nested for-loops are not GPU-friendly. Don’t worry: in the next section, we will discuss implementations that parallelize computation, and thus run much faster on modern hardware. For now, the main point of this pseudocode is to highlight that first mode of computation, which we call attention formulation, has a FLOP cost of \\(O(t^2 d)\\).\nThe key to the massive speedups we saw in the introduction comes from leveraging linearity to restructure the computation. Consider the following factorization: \\[\nY_i = \\sum_{j=1}^i Q^T_i K_j V_j = \\underbrace{ \\left (  \\sum_{j=1}^i V_j  K_j^T\\right )}_{S_i} \\; \\; Q_i\n\\] Written in this form, we notice that the term labeled \\(S_i \\in \\R^{d\\times d}\\) can be thought of as a state summarizing all the relevant information up to time \\(i\\). It’s easy to rewrite into the following recurrent equations \\[\nY_{i} = S_i Q_i\n\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\nS_i = S_{i-1} + V_i K_i^T\n\\] where we assume \\(S_{0} = 0\\in \\R^{d\\times d}\\). Written in this form, we realize that a linear transformer is an RNN. We can also write pseudocode for this approach, which we call the state formulation, and analyze the cost:\ndef LT_state(Q, K, V):\n    \"\"\"\n    Shapes of inputs are\n     Q: [t, d]  K: [t, d]  V: [t, d]\n    Shapes of outputs are\n     Y: [t, d]\n    \"\"\"\n    t, d = Q.shape\n    S_i = zeros(d, d) # shape [d,d]\n    Y_list = []\n    for i in range(t):        # loop cost: O(t d^2)\n        S_i += outer(K[i], V[i]) # cost: O(d^2)\n        Y_i = S_i @ Q[i]      # cost: O(d^2)\n        Y_list.append(Y_i)\n    return stack(Y_list)\nWe see that the cost here is \\(O(t d^2)\\).\nSo, while a standard transformer layer always has cost \\(O(t^2 d)\\), linear transformers have two formulations with different costs. By switching from the attention formulation to the state formulation, we can change the cost from \\(O(t^2 d)\\) to \\(O(t d^2)\\), trading a \\(t\\) term for a \\(d\\) term."
  },
  {
    "objectID": "blogposts/faster-after-all/index.html#parallel-implementations",
    "href": "blogposts/faster-after-all/index.html#parallel-implementations",
    "title": "Linear Transformers Are Faster After All",
    "section": "2. Parallel Implementations",
    "text": "2. Parallel Implementations\nIn general, for-loops are a terrible way to implement anything that will run on a GPU. When using high-level frameworks like PyTorch or JAX, the easiest way to get high GPU utilization is to only use primitives that are already highly optimized for GPU: matrix multiplication, elementwise operations, etc. We can rewrite our attention and state algorithms in this style to make them efficient.\nFirst, let’s do this for attention. Our main technique is to compute the attention matrix \\(A\\), which contains all the terms outer(Q[i], K[j]) that appeared inside the for-loops of LT_attention, using a single heavyweight matrix multiply.\ndef LT_attention_parallel_no_flash(Q, K, V):\n    \"\"\"\n    Shapes of inputs are\n     Q: [t, d]  K: [t, d]  V: [t, d]\n    Shapes of outputs are\n     Y: [t, d]\n    \"\"\"\n    t = Q.shape[0]\n    M = causal_mask(t)\n    A_raw = Q @ K.T  # cost O(t^2 d)\n    A = A_raw * M    # cost O(t^2)\n    Y = A @ V        # cost O(t^2 d)\n    return Y\nThis implementation lies at the core of nearly every transformer of the past five years, powering all of the AI models that have recently exploded in popularity. Since it is composed of such a small number of ultra-parallelizable ops, it obtains high GPU utilization. Recently, specialized flash attention kernels [3] have been used to get even further speedups by avoiding explicitly storing the attention matrix \\(A\\), and thereby saving both memory and time spent on memory-transfers. Algorithmically, though, flash attention is a variant of parallel attention, and has the same computational cost (as measured in FLOPs). We use LT_attention_parallel to refer to the flash attention implementation.\nNext, let’s parallelize the recurrent formulation. This optimization is less well-known. The key is to compute all the terms \\(V_i K^T_i\\) in parallel, and then use a cumulative-sum, which can be parallelized, to combine them.\ndef LT_state_parallel(Q, K, V):\n    \"\"\"\n    Shapes of inputs are\n     Q: [t, d]  K: [t, d]  V: [t, d]\n    Shapes of outputs are\n     Y: [t, d]\n    \"\"\"\n    P = V[:,:,None] @ K[:,None,:]  # cost: O(t d^2)\n    S = cumsum(P, axis=0)          # cost: O(log_2(t) t d^2)\n    Y = S @ Q[:,:,None]            # cost: O(t d^2)\n    return Y[:,:0]\nThe cost in FLOPs of this algorithm is \\(O(\\log_2(t) t d^2)\\).3\nNow that we have four mathematically-equivalent implementations of a linear transformer, let’s time the training step of our GPT2 models and see how big of a difference it makes. For our LT_attention_parallel implementation, we use a custom linear self-attention flash kernel we implemented in Triton [13] based on OpenAI’s FlashAttention2 implementation.\n\n\n\n\nHere are some takeaways:\n\nAs expected, the attention variants all have a quadratic asymptotic cost (slope of 2 on a log-log plot4). The state variants all have linear asymptotic cost (slope 1). 5\nLT_state_parallel is an order-of-magnitude faster than LT_state.\nLT_attention_parallel_no_flash is two orders-of-magnitude faster than LT_attention.\nLT_attention_parallel seems to asymptotically stabilize into being an order-of-magnitude faster than LT_attention_parallel_no_flash.\nFor the majority of settings, LT_attention_parallel is the fastest. (This is the linear version of the algorithm used by the standard transformer.)\nParallel attention is the fastest algorithm for small context sizes. However, LT_state_parallel overcomes LT_attention_parallel_no_flash at around 13k context size, and overcomes LT_attention_parallel at around 100k.\n\nOverall, these results paint a clear picture: use the attention algorithm for small contexts and the state algorithm for large contexts. But do we really face a binary choice? Can we combine the state and attention ideas and get the best of both words?"
  },
  {
    "objectID": "blogposts/faster-after-all/index.html#chunked-formulation",
    "href": "blogposts/faster-after-all/index.html#chunked-formulation",
    "title": "Linear Transformers Are Faster After All",
    "section": "3. Chunked Formulation",
    "text": "3. Chunked Formulation\nIt’s evident that, for small context sizes, computing the \\(t\\) by \\(t\\) attention matrix is much more efficient than computing many \\(d\\) by \\(d\\) state matrices. But as \\(t\\) grows, there is a point where the quadratic cost of the attention matrix ends up dominating. Noticing that attention is extremely efficient for small \\(t\\) and that states are necessary for large \\(t\\) motivates doing one last reworking of the LT equation.\nLet \\(c \\in \\N\\) be a positive integer that we’ll call the chunk size. For any \\(i\\in \\N\\) find the unique \\(n\\in \\Z\\) s.t. \\(cn &lt; i \\le c(n+1)\\). We can easily see that the following equations are equivalent to the previous ones. \\[\nY_{i} = S_{cn}Q_i + \\sum_{j=cn+1}^i Q_i^T K_j V_j\n\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\nS_{c(n+1)} = S_{cn} + \\sum_{j=cn+1}^{c(n+1)} V_j K_j^T\n\\] The key idea is that we are only going to compute a subset of all states: \\(S_0, S_c, S_{2c}, \\cdots\\). Then, to compute each output \\(Y_i\\), we need only to take into account the contribution via the most recent state \\(S_{cn}\\), as well as the contribution (computed via attention) of all moments in time \\(j\\) in the range \\(cn &lt; j \\le i\\).\nAs pseudocode, this looks like:\ndef LT_attention_with_initial_state(S, Q, K, V):\n    \"\"\"\n    Shapes of inputs are\n     S: [d, d]  Q: [c, d]  K: [c, d]  V: [c, d]\n    Shapes of outputs are\n     Y: [c, d]\n    \"\"\"\n    Y_state = Q @ S                               # cost O(c d^2)\n    Y_attention = LT_attention_parallel(Q, K, V)  # cost O(c^2 d)\n    Y = Y_state + Y_attention                     # cost O(cd)\n    return Y\n\ndef LT_chunked(Q, K, V, c):\n    \"\"\"\n    Shapes of inputs are\n     Q: [t, d]  K: [t, d]  V: [t, d], c: int\n    Shapes of outputs are\n     Y: [t, d]\n    \"\"\"\n    t, d = Q.shape\n    assert t % c == 0\n    Q_, K_, V_ = [arr.reshape(t//c, c, d)\n    `               for arr in [Q,K,V]]\n    P_ = K_.transpose([0,2,1]) @ V_  # cost O(t d^2)\n    S_ = cumsum(P_, axis=0) - P_     # cost O(log_2(t/c)(t/c)d^2)\n    Y_ = vmap(LT_attention_with_initial_state, axis=0)(\n                S_, Q_, K_, V_)      # cost O(td^2 + tcd)\n    return Y_.reshape(t, d)\nThe cost is \\(O\\left(td^2 + tcd + \\log_2(t/c)(t/c)d^2\\right)\\), once again avoiding a quadratic dependency on \\(t\\). Also, note that this algorithm makes an inner call to LT_attention_parallel, so we can use a flash-attention kernel to do that part of the computation.\nThis algorithm has a hyperparameter, the chunk size, which must be set correctly for optimal performance. Fortunately, it is inexpensive to identify the best chunk size empirically, since measuring just a few steps of training at each chunk size is sufficient to identify which is the fastest. In the plot below, we plot the speed of the optimally-chunked algorithm in each setting.\n\n\n\n\nWe see LT_chunked gives the desired best-of-both-worlds behavior, as it is equal-or-better than all other approaches in all settings. And we now see the massive (& rapidly growing) speedup relative to standard self-attention, finally unlocking the true power of linear transformers."
  },
  {
    "objectID": "blogposts/faster-after-all/index.html#sampling",
    "href": "blogposts/faster-after-all/index.html#sampling",
    "title": "Linear Transformers Are Faster After All",
    "section": "4. Sampling",
    "text": "4. Sampling\nWhen working with language models, efficient training is not the only performance that deserves consideration. Once the model is trained, how expensive is it to utilize? When we are sampling we have a sequence of tokens, \\(z_1 \\cdots z_t\\), and we want to sample the next token, \\(z_{t+1}\\).\nThe most efficient algorithm to sample from traditional transformers is called the KV-cache algorithm [14]. This algorithm assumes that when we generate token \\(z_{t+1}\\), we will have already computed and cached all the \\(K_i, V_i\\) for all \\(0 \\le i \\le t\\). In order to compute the output of the attention layer at time \\(t+1\\) given this cached information, we can use \\[\nY_{t+1}^\\text{Transformer} = \\sum_{j=1}^{t+1} e^{Q^T_i K_j} V_j\n\\] It is easy to see that this is an \\(O(td)\\) operation. In other words, as the sequence length grows, sampling each subsequent token becomes more computationally expensive.6 This is one of the major limitations of the classic transformer architecture.\nWith linear transformers, however, we have access to the recurrent formulation. A linear transformer is an RNN where the state has size \\(O(d^2)\\). \\[\nY_{i} = S_i Q_i\n\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\nS_i = S_{i-1} + V_i K_i^T\n\\] We can compare the time it takes to generate any particular token when sampling a sequence:\n\n\n\n\nAs expected, we see that the time to sample of the linear transformer is independent of context length, whereas that of the KV-cache transformer grows linearly. This leads to a large gap in inference speed at large contexts.7"
  },
  {
    "objectID": "blogposts/faster-after-all/index.html#learning-performance",
    "href": "blogposts/faster-after-all/index.html#learning-performance",
    "title": "Linear Transformers Are Faster After All",
    "section": "5. Learning Performance",
    "text": "5. Learning Performance\nUntil now, our focus has been on how to implement linear transformers efficiently. But an architecture that runs really fast is useless unless it also learns well. We will now compare the learning curves of the GPT2 baselines with their linear transformer counterparts, at various context sizes.\nIn order to control for the fact that longer contexts help learning by introducing more tokens per update, we hold the number of tokens-per-update constant across context sizes by decreasing batch size. At all context-lengths, each update sees \\(2^{19}\\) tokens.8 Importantly, for this set of experiments, we have used the dataset c4 [4], which does not have much long-term structure: it consists of a shuffled collection of short articles, of average length ~500 tokens.\nFirst, let’s look at the parameter scaling law of GPT2 and our modified Linear-GPT2. The following plot shows the final performance after 24h of training on our 8xH100 server for each scale and each context size. Click the second tab if you want to see the full learning curves.\n\n\n\n\nBoth architectures scale similarly with respect to model size, but there is a gap in performance. This gap seems to grow as context size increases, together with more loss spikes and general instability. It’s possible that the gap can be explained by the linear transformer not effectively utilizing its context. To explore whether or not this is the case, we can use these same experiments, but visualize all context-lengths together.\n\n\n\n\nWe see a dramatically different trend between the two architectures. For GPT2, each increase in context length slows down the initial pace of learning, but ultimately all context-lengths (beyond at least 1024) converge to a similar final loss. Whereas for Linear-GPT2, not only does increasing the context slow down learning in the beginning, but it also causes convergence to a worse final performance and nasty-looking loss spikes.\nThe results for GPT2 are what we would expect from a healthy model, given the setting. Note that since our dataset consists of short articles, of average length ~500 tokens, we can assume that even a context window of 1024 would include nearly everything relevant. Thus, we wouldn’t expect that increasing the context length beyond this point would decrease the loss the model ultimately converges to. Longer-context models do however need to learn to ignore many more irrelevant tokens, explaining the slowed initial learning.9\nIn contrast, the results for Linear-GPT2 seem to indicate some underlying instability of the linear transformer architecture. It doesn’t even seem capable of ignoring irrelevant information, let alone exploiting useful long-term structure.\nRemedying these learning issues will be the main focus of our future work in linear transformers. Our current architecture is essentially a one-to-one clone of GPT2, sticking as architecturally close to the original as possible; aspects such as initializations, normalizations and hyperparameters were directly copied. It is well-known that these decisions can have a large impact on scaling and stability and often need to be tuned in an architecture-specific way. In the literature, various other linear variants of self-attention have been proposed, that incorporate techniques such as gating mechanisms in order to improve stability and learning [5]–[11]. A future post will include a thorough study of the impact of all of these choices.\nUltimately, it may be impossible for any linear transformer to perfectly match the context scaling laws of the classic transformer baseline. Although removing the exponential seems like a minor change, it represents a meaningful decrease in expressivity.10 But as we’ve shown, linear transformers can be trained orders-of-magnitude more efficiently. On equivalent compute, linear transformers can be trained for many more steps; or, keeping steps constant as well, we can train a much larger model. If this additional scaling is sufficient to compensate for the reduced expressivity, linear transformers will be the more efficient architecture overall.\n\n  \n  Subscribe to be notified of new posts:"
  },
  {
    "objectID": "blogposts/faster-after-all/index.html#footnotes",
    "href": "blogposts/faster-after-all/index.html#footnotes",
    "title": "Linear Transformers Are Faster After All",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs discussed in Section 4, a second benefit of linear transformers is that the cost to sample a token does not grow with context size. Perhaps one could argue that this improvement in sampling speed could, on its own, justify using linear transformers for applications where the inference costs vastly exceed training costs. But it is evident to us that, for linear transformers to become actually useful, we need to address these instability issues.↩︎\nIt is not named for the fact that the computational cost is linear with respect to \\(t\\)! That is just a coincidence. (And is not even always true, as we will see.)↩︎\nInterestingly, LT_state_parallel is actually more expensive than LT_state. (This is in contrast with the attention formulation, where LT_attention and LT_attention_parallel share the same \\(O(t^2 d)\\) cost.) As we will see in a moment, this extra \\(\\log_2(t)\\) factor is well worth the parallelization benefits.↩︎\nIf \\(y=x^2\\), a log-log plot where \\(y'=\\log_a(y)\\) and \\(x'=\\log_a(x)\\) for any base \\(a\\), then \\(y'=\\log_a(y) = \\log_a(x^2) = 2 \\log_a(x) = 2 x'\\). So the graph will be a line with slope 2.↩︎\nThe reason we see the expected slopes asymptotically is that we are timing a full GPT2 architecture which has many other components besideds the attention layer. If we were only timing the attention layer, the plots would all be straight lines.↩︎\nAn interesting connection is that the KV-cache can be understood as the state of an RNN with non-constant state size; namely, one whose state-size is \\(O(td)\\).↩︎\nThis comparison may not be completely fair. In these experiments, our implementation of neither sampling algorithm makes use of specialized kernels. A lot of the ideas of flash attention can be used to write a much faster KV cache sampling algorithm; on the other hand, it’s unclear if much improvement is possible on the recurrent sampling. Thus, it’s possible that with engineering effort the gap between the two algorithms could become smaller. However, the overall pattern will certainly remain the same.↩︎\ne.g. runs with context-size 1024 would have batch-size of \\(2^{19} / 2^{10} = 2^{9} = 512\\).↩︎\nPut another way: doubling the size of the input vastly increases the size of the function space over which gradient descent must search, and it’s intuitive that in a larger space it takes somewhat longer to find a good solution.↩︎\nWe plan to elaborate on this topic in a future blog post.↩︎"
  },
  {
    "objectID": "articles/what-is-power-retention/index.html",
    "href": "articles/what-is-power-retention/index.html",
    "title": "What Is Power Retention?",
    "section": "",
    "text": "Power retention is a new architecture developed by Manifest that enables AI to handle millions of tokens at a fraction of today’s cost. It replaces Transformers’ attention as the de facto approach for designing large language models, and unlocks long-context applications that were previously prohibitively expensive or impractically slow."
  },
  {
    "objectID": "articles/what-is-power-retention/index.html#footnotes",
    "href": "articles/what-is-power-retention/index.html#footnotes",
    "title": "What Is Power Retention?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe only reason why current Transformer-based chatbots don’t take thirty hours to respond is that they are very short-lived. Since they have fewer experiences to mentally replay, they can respond quickly enough to be tolerable, but the limited context hurts their usefulness.↩︎\nBefore Transformers, the best sequence architectures were “recurrent neural networks”.↩︎\nAlthough by other names, e.g. this work.↩︎\nBeyond the inherent benefits of retention over attention, our power retention kernels are implemented so efficiently that they achieve higher GPU utilization than Flash Attention.↩︎"
  },
  {
    "objectID": "articles/rnn-state-scaling/index.html",
    "href": "articles/rnn-state-scaling/index.html",
    "title": "RNN State Scaling",
    "section": "",
    "text": "TODO write intro\nIn this post we go over how to think about RNN state size. We find an architecture, symmetric power, that looks very promising."
  },
  {
    "objectID": "articles/rnn-state-scaling/index.html#background",
    "href": "articles/rnn-state-scaling/index.html#background",
    "title": "RNN State Scaling",
    "section": "1. Background",
    "text": "1. Background\nWe begin by introducing some conceptual scaffolding to connect important ideas around transformers and RNNs.\nAll auto-regressive sequence models can be thought of as state machines (SMs). A state machine is a generic abstract model for any system that takes in an input and a state, and gives an output and updated state. In the case of neural language modeling, the inputs are typically tokens, the outputs are typically next-token distribution predictions, and the states are typically arrays of floating-point numbers.\nLet \\(\\mathcal{X}, \\mathcal{Y}, \\mathcal{S}\\) denote the input space, output space, and state space, respectively. A state machine is a triplet \\((S_0, \\delta, \\rho)\\), where \\(S_0 \\in \\mathcal{S}\\) denotes the initial state, \\(\\delta \\in (\\mathcal{S} \\times \\mathcal{X}) \\to \\mathcal{S}\\) denotes the state transition function, \\(\\rho \\in (\\mathcal{S} \\times \\mathcal{X}) \\to \\mathcal{Y}\\) denotes the output function, and \\[\n\\begin{aligned}\nS_t &:= \\delta(S_{t-1}, X_t)\\\\\nY_t &:= \\rho(S_{t}, X_t)\\\\\n\\end{aligned}\n\\] where \\(X_t\\) and \\(Y_t\\) denote the input and output at time \\(t\\), respectively.1 This simple but powerful abstraction encapsulates every modern language model, including models based on the standard softmax-based transformer, RNNs (e.g. LSTMs, GRUs, linear transformers), and state-space models like Mamba. At first glance, it may be surprising that transformers are included in this list; transformer language models are not typically presented as state machines. See Appendix A.1. for a constructive proof.\nWe are further interested in a more specific concept: a finite state machine (FSM). We use \\(\\operatorname{size}\\) to denote the function which returns the dimensionality of an object. In the context of this article, we define a finite state machine as a state machine with the property that there exists \\(\\varsigma \\in \\mathbb{N}\\) such that \\(\\operatorname{size}(S) \\leq \\varsigma\\) for all \\(S \\in \\mathcal{S}\\). We call \\(\\varsigma\\) the state size.2\nIt is easy to see that the prototypical RNN is a finite state machine: each state \\(S \\in \\mathcal{S}\\) is a fixed-and-finite-dimensional array.3 Furthermore, it can be shown that transformers are not finite state machines (see Appendix A.2). Intuitively, this is because the state size of a transformer grows with the length of its input history, so no fixed \\(\\varsigma\\) is larger than every possible states.\nThis paints a clear picture of the relationship between transformers and RNNs. The two approaches can be unified under the umbrella of state machines, but the crucial distinction between them is that transformers do not have a finite-sized state. From this perspective, it becomes clear that increasing the state size of an RNN will make it more similar to a transformer (and in fact, in Sections 2.2 and 2.5 we give families of RNNs that converge exactly to the transformer in the limit of increasing state size). Noting the performance gap between transformers and RNNs motivates a research question: can the performance of an RNN be improved by increasing its state size?\nThe remaining sections of article will answer that question in the affirmative, showing that the performance of RNNs with large states matches, and sometimes surpasses, that of transformers."
  },
  {
    "objectID": "articles/rnn-state-scaling/index.html#state-scaling-of-rnns",
    "href": "articles/rnn-state-scaling/index.html#state-scaling-of-rnns",
    "title": "RNN State Scaling",
    "section": "2. State Scaling Of RNNs",
    "text": "2. State Scaling Of RNNs\nLet’s take a look at some sequence modeling architectures through the lens of state size scaling. The table below gives an overview of what we will discover.\n\n\n\n\n\n\n\n\n\n\nRNN Architecture\nAdjustable State-Weight Ratio\nUnlimited Effective Context Window\nConverges To Transformer\nState-Weight Ratio\n\n\n\n\nFeedforward RNN\n✗\n✓\n✗\n\\(\\frac{1}{2d}\\)\n\n\nWindowed Transformer\n✓\n✗\n✓\n\\(\\frac{w}{6d + 1}\\)\n\n\nLinear Transformer\n✗\n✓\n✗\n\\(\\frac{d/head}{12d + 2}\\)\n\n\nSplit-Product Transformer\n✓\n✓\n✗\n\\(\\frac{(d/(head\\cdot s))^s}{12d + 2}\\)\n\n\nTaylor Transformer\n✓\n✓\n✓\n\\(\\frac{\\sum_{i=0}^{D}\\binom{\\frac{d}{head} + i - 1}{i}}{12d + 2}\\)\n\n\nSymmetric Power Transformer\n✓\n✓\n✗\n\\(\\frac{\\binom{\\frac{d}{head} + D - 1}{D}}{12d + 2}\\)\n\n\n\nAll architectures we discuss in this article have the same high-level structure. Letting \\(d \\in \\mathbb{N}\\) denote the width of a network and \\(\\ell \\in \\mathbb{N}\\) to denote the depth, each architecture has an embedding layer mapping tokens to \\(\\mathbb{R}^d\\), followed by \\(\\ell\\) copies of an RNN with \\(\\mathcal{X} = \\mathcal{Y} = \\mathbb{R}^d\\), and finally an outbedding layer mapping \\(\\mathbb{R}^d\\) to next-token distributions. Since this structure is shared, the section studying each article focuses only on the inner RNN, where we use \\(s\\) to denote the number of splits in Split-Product Transformer, and use \\(D\\) to denote the degrees of power in Taylor Transformer and Symmetric Power Transformer.\n\n2.1. Feedforward RNN\nA natural place to begin is with the classic feedforward RNN layer, defined as:\n\\[\n\\begin{aligned}\nS_0 &:= \\textbf{0} \\in \\mathbb{R}^d\\\\\n\\delta(S_{t-1}, X_t) &:= \\sigma(W_{\\delta}(S_{t-1} + X_t))\\\\\n\\rho(S_{t}, X_{t}) &:= W_{\\rho}S_t\n\\end{aligned}\n\\]\nwhere weights \\(W_{\\delta}, W_{\\rho} \\in \\mathbb{R}^{d \\times d}\\) and \\(\\sigma\\) denotes a nonlinearity like ReLU.\nSince \\(S \\in \\mathbb{R}^d\\) for all \\(S \\in \\mathcal{S}\\), the state size of each layer of this network is \\(d\\). The overall state size is therefore \\(d\\ell\\). If we want to increase the state size, we need to increase the width \\(d\\) or the depth \\(\\ell\\). But notice that, since the parameter count is \\(2d^2\\ell\\), increasing the width or depth also increases the number of parameters. This is problematic for two reasons, one scientific and one practical.\nScientifially, we were hoping to answer the question of does increasing the state size improve performance? This motivates an experimental design where we increase the state size and measure the change in performance. But if, when we increase state size, we also increase parameter count, our experiment becomes unable to disentagle the impact of increasing the size of the state from the familiar performance boost that comes with increasing the model scale.\nOn the practical side, the concern is best illustrated by highlighting a quantity we call the state-weight ratio. For the classic RNN, this is \\(\\frac{d\\ell}{2d^2\\ell} = \\frac{1}{2d}\\). This ratio is small and we have no way of increasing it. As we increase the state size by increasing \\(d\\), this architecture will actually spend a smaller fraction of its memory on its state. In order to design an RNN to have a similar parameter scaling law to that of a transformer, we need it to have not only large states, but a large state-weight ratio.\n\n\n2.2. Windowed Transformer\nIn the this section, we study an RNN architecture that allows us to freely adjust the state-weight ratio. The formula for a transformer layer is:\n\\[\nY^\\text{Transformer}_t = \\sum_{i=1}^t e^{Q^T_t K_i} V_i\n\\]\nIn Appendix A.1, we construct an equivalent state machine with \\(S_t = (K_{i=1}^t, V_{i=1}^t)\\). This means that \\(\\operatorname{size}(S_t) = 2td\\ell\\), which, being dependent on \\(t\\), has no upper bound.\nHowever, simply truncating the state beyond a certain size straightforwardly leads to a transformer-like RNN, which we call a windowed transformer. The truncation is given by a window size hyperparameter, which we denote \\(w \\in \\mathbb{N}\\). Its formula is:\n\\[\nY^\\text{WindowedTransformer}_t = \\sum_{i=t-w}^t e^{Q^T_t K_i} V_i\n\\]\nIt is easy to see that this is a state machine with \\(S_t = (K_{i=t-w}^t, V_{i=t-w}^t)\\), and is therefore a finite state machine with state size \\(2wd\\ell\\). The implementation is almost exactly the same as for a regular transformer, but in addition to a causal mask, it also includes an “ancient history mask”.\n[Visualization]\n\n\nIt is interesting to note that a naive implementation of windowed transformer attention (one which materializes the entire attention matrix) has cost \\(O(T^2)\\) to process a sequence of length \\(T\\) – just like a regular transformer. Running this architecture in \\(O(T)\\) requires an efficient fused implementation which avoids materializing (or even computing) segments of the attention matrix that are entirely masked out.\nBy adjusting \\(w\\), we can control the state size without affecting the parameter count. Using a GPT-2-style architecture, the (non-embedding) parameter count for a transformer is \\((2d + 12d^2)\\ell\\) (see Appendix A.3 for a breakdown). The state-weight ratio is therefore \\(\\frac{2wd\\ell}{(2d + 12d^2)\\ell} = \\frac{w}{6d + 1}\\). Any desired state-weight ratio can be achieved.\nThis gives us the ability to empirically observe the state scaling behavior of the windowed transformer. All experiments in this article were performed using a 124M parameter GPT-2 architecture, on LongCrawl64 with a context length of 4096.\n[PLOT]\n[JAX code]\nAs expected, we see that state scaling improves performance.\nSince the documents in our training set have context length of 4096 tokens, setting \\(w = 4096\\) causes a windowed transformer to exactly match the performance of the baseline transformer. Also, there is no value in increasing the state size beyond \\(w &gt; 4096\\). (On a longer-context dataset, there would be.)\nOne important perspective on windowed transformers is that they are a family of RNNs that converge to the regular transformer, in the limit as \\(w \\to \\infty\\). In Section 2.5, we will see another such family, which approaches the regular transformer but in a very different way.\n\n\n2.3. Split-Product Transformer\nIt is intuitively clear that a limitation of the windowed transformer architecture discussed in the previous section is that it has a limited effective context window, meaning that there are some tokens in the context which do not in any way impact the model’s prediction. A windowed transformer with \\(\\ell\\) layers and a window size \\(w\\) takes into account only the most recent \\(\\ell(w - 1)\\) tokens. We will show empirically in Section 3.2 how this limitation can impact performance. This motivates us to continue exploring the space of architectures, to find one that has both an unlimited effective context window (as the feedforward RNN has) and an adjustable weight-state ratio (as the windowed transformer has).\nThe linear transformer from our previous article is not that architecture, but it gives us a good starting point. The formula for the linear transformer is:\n\\[\nY_t^\\text{LT} = \\sum_{i=1}^t Q^T_t K_i V_i\n\\]\nThis can be generalized slightly by introducing \\(\\phi : \\mathbb{R}^d \\to \\mathcal{Z}\\), an embedding function which maps keys/queries into an inner product space \\(\\mathcal{Z}\\). The nice properties of linear transformers that we demonstrated in our previous article (specifically, the existence of an \\(O(t)\\) state algorithm) remain universally present for architectures in this more general family (i.e. for any choice of \\(\\phi\\)).\n\\[\nY_t^\\text{GenericLT} = \\sum_{i=1}^t \\langle \\phi(Q_t),  \\phi(K_i) \\rangle V_i \\\\\nY_{t}^\\text{GenericLT} = S_t \\phi(Q_t) \\qquad S_t = S_{t-1} + V_t  \\phi(K_t)^T\n\\]\nOf course, this reformulation is superficial.4 If we consider \\(\\phi(K_i)\\) to be part of the broader layer rather than the attention calculation, this reduces back to the original equations of the linear transformer. The advantage of thinking of \\(\\phi\\) as part of the transformer itself is that it allows us to study variants of \\(\\phi\\) while keeping the rest of the network exactly the same. In particular, we will be looking at choices of \\(\\phi\\) where \\(z \\gg d\\), which we call state expansions. State expansions allow us to freely adjust the state-weight ratio, and we will see that larger state expansions generally improve performance.\nWhat state expansion should we pick? It’s wide open, and many things have been tried [TODO cite stuff]. In this article, we focus on state expansions that leverage the tensor product, an approach that we consider to be particularly mathematically elegant. Intuitively, the tensor product can be seen as a generalization of the outer product to higher-order objects. (See Appendex A.4 for a more complete introduction to the topic.)\nHere is a straightforward way to use the tensor product to define a state expansion. Given a vector-valued key (or, wlog, query), first split the key into \\(p \\in \\mathbb{N}\\) evenly-sized pieces, and then take the tensor product of those pieces. Formally, we define the split-product \\(\\phi\\) as:\n\\[\n\\chi_i(K_t) = \\text{softplus}(K_t)_{\\left[\\frac{(i-1)d}{p} : \\frac{id}{p}\\right]} \\qquad \\phi^\\text{SplitProduct}(K_t) = \\bigotimes_{i=1}^p \\chi_i(K_t)\n\\]\nWith this embedding function, \\(\\phi^\\text{SplitProduct}(K_t) \\in \\mathbb{R}^{\\left(\\frac{d}{p}\\right)^p}\\). There is no way to represent this besides representing each component of \\(\\phi^\\text{SplitProduct}(K_t)\\) so the state size is \\(\\left(\\frac{d}{p}\\right)^p\\). (PROOF!?!!) As we increase \\(p\\), we get exponential growth in state size.\nTo implement the attention algorithm for \\(\\phi^\\text{SplitProduct}\\), there is no need to materialize the expanded state. Simply split both the key and the query, do a dot product on each piece, and then multiplying the results together. (PROOOF?)\n\\[\nY_t^\\text{SplitProduct} = \\sum_{i=1}^t \\left( \\prod_{i=1}^p \\chi_i(Q_t)^T \\chi_i(K_t) \\right) V_t\n\\]\nLet’s run some experiments to investigate the effect of the split-product state expansion on performance.\n[PLOT]\n[JAX code]\nAs expected, we see that state scaling improves performance.\n\n\n2.4. Symmetric Power Transformer\nThe split-product approach is a straightforward enough application of the tensor power, but in some ways it is a bit awkward: splitting up the keys means that \\(p\\) is required to divide \\(d\\) evenly, and the possibility of negative dot-products means we still need to use the softplus to push everything into the positive quadrant in order to normalize later.\nThere is a somewhat nicer variation, which we call this the symmetric power expansion. In this approach, we simply tensor the key with itself an even number of times. This obviates the need for awkward splitting, and guarantees that each dot-product term will be positive even when keys are not projected into the positive quadrant. The symmetric power formula is:\n\\[\n\\phi^\\text{SymmetricPower}(K_t) = \\bigotimes_{i=1}^p K_t\n\\]\nNaively, we might assume that the size of this state is \\(d^p\\). But actually, we can exploit symmetry: many terms are repeated. The real state size is \\(\\binom{d + p - 1}{p}\\). (PROOF!!!!!)\nThe attention algorithm for the symmetric power is pleasingly straightforward. Simply replace exponentiation operation with raising-to-\\(p\\):\n\\[\nY_t^\\text{SymmetricPower} = \\sum_{i=1}^t \\left( \\chi_i(Q_t)^T \\chi_i(K_t)^p \\right) V_t\n\\]\nLet’s run some experiments to investigate the effect of the symmetric power state expansion on performance.\n[PLOT]\n[JAX code]\nAs expected, we see that state scaling improves performance. Impressively, it matches the performance of the softmax baseline by \\(p=4\\), and surpasses it slightly at \\(p=6\\).\n\n\n2.5. Transformer (Taylor’s Version)\nThe previous two approaches each have an adjustable state-weight ratio and no maximum effective context size. But both are still missing one cool aspect of the windowed transformer: the guarantee that, in the limit of large state, it approaches the transformer. This third property is not needed for good performance, so a purely-practical-minded person might not care. But it is deeply insightful to think about what such an architecture would look like.\nStart off with a linear transformer, and let \\(\\phi\\) equal the first \\(n\\) terms of the Taylor series approximation of the exponential. This is the the sum of \\(n\\) different symmetric powers, so the state size is \\(\\sum_{p=1}^{n} \\binom{d + p - 1}{p}\\). (PROOF!!!!)\nAs we increase \\(n\\), the state size of the model grows, and we approach the original transformer.\nThis gives a second perspective on the idea that a transformer is a state machine, but not a finite state machine. Rather than thinking about the transformers as having a growing state (as in the KV cache perspective), we can also think about transformers as having an infinite-dimensional state."
  },
  {
    "objectID": "articles/rnn-state-scaling/index.html#comparing-state-scaling-curves",
    "href": "articles/rnn-state-scaling/index.html#comparing-state-scaling-curves",
    "title": "RNN State Scaling",
    "section": "3. Comparing State Scaling Curves",
    "text": "3. Comparing State Scaling Curves\nWe’ve gone on a wonderful journey though architecture land, looking at lots of qualitative differences between them. But now it is almost time to answer the question on everyone’s mind: which architecture is the best?\nBefore diving in, let’s take one more moment to reflect on what we are looking for in an architecture.\n\n3.1. Why State Scaling?\nUltimately, the goal of any training run is to reach the best performance given training budget. The most relevant metric would therefore be to directly measure the impact of this increased cost by comparing models on the basis of their performance-per-GPU-hour. But this sort of comparison is difficult to get right. Firstly, performance-per-GPU-hour is highly implementation-dependent. Without an efficient fused kernel for the chunked algorithm, any RNN will inevitably fall far short of its potential; but it is challenging and time-consuming to implement these kernels. Secondly, compute-optimal performance relies on jointly choosing the state size with the model size, context length, etc., a process which requires an expensive sweep over various combinations.\nIn light of these difficulties, it is convenient to have a simpler experimental design. As we have seen, increasing the state size improves performance per step of training. It also increases the cost, since a larger state means more computation per step. The computation per step is roughly proportional to the size of the state for most relevant architecutures, since each dimension of the state needs to be read, processed, and written at least once.5 State size is therefore a reasonable heuristic to use in place of wall-clock time, and so it makes sense to focus on architectures which have high growth in performance per dimension of state.\nSince the size of the state for any architecture can be computed in closed form, we do not even need to implement the state or chunked algorithms in order to assess quality on the basis of state size scaling laws. A simple attention implementation is all that is needed. This makes it easy to quickly iterate on architectural chocies, and only implement the difficult chunked algorithm for architectures that seem promising.\nTo summarize: our goal in this section is to identify which architectures (if any) we want to spend our time implementing efficiently, by comparing their state scaliing laws.\n\n\n3.2. Empirical Results\nBelow on the left you can see the state size scaling curves for all the architectures we described today. On the right, click the names of individual architectures to see their training curves.\n[PLOT]\nThere are two clear winners here: the windowed transformer and the symmetric power transformer.\n\n\n3.3. Data Dependence\nAn important consideration for these experiments is the choice of dataset. In theory this is always the case: for example, when Chinchilla computes its optimal model sizes, it’s doing so on the basis of scaling laws extrapolated for one particular dataset, and these may or may not hold on other datasets. Fortunately, for the vast majority of deep learning work, it really does not seem to matter. Evidence seems to indicate that the ordering of architectures is pretty consistent accross datasets (TODO CITE THIS), and it’s difficult to see how one might intentionally construct datasets that are more advantageous to one architecture vs another.\nWhen it comes to RNNs, this is not the case. There are some strong and easily visible data dependence effects. An obvious one in this case is the fact that the windowed transformer has a limited effective context length, whereas other architectures do not. When most of the information needed to make a prediction is local, the windowed transformer is great. But when the information is further away in the sequence, it does worse.\nIt seems natural language is, for the most part, pretty local, and so the windowed transformer is mostly able to crush other architectures on LongCrawl64. But we can construct an aritificial dataset where this is not the case. Simply take LongCrawl64 and insert dummy characters between each pair of consecutive tokens (increasing the overall context length by the commensurate amount). Intuitively, this will preserve the information in the sequence, so no prediction is made more challenging for models with no limitation on maximum effective context size. But it will make the information less dense, meaning less information will be accessible within the effective context window of the windowed transformer.\nWe can run this experiment to validate empirically. We train a windowed transformer and symmetric power transformer on a sequence of datasets transformed in this way.\n[PLOT]\nAs expected, we see the windowed transformer get worse and worse, but the symmetric power transformer stays strong.\nThe takeaway here is that you should be careful before generalizing a ranking of architectures across all datasets. Different RNNs can be more or less suitable for different datasets."
  },
  {
    "objectID": "articles/rnn-state-scaling/index.html#conclusion",
    "href": "articles/rnn-state-scaling/index.html#conclusion",
    "title": "RNN State Scaling",
    "section": "4. Conclusion",
    "text": "4. Conclusion\nTODO write conclusion\nThanks for reading idk we might not need this. Plz subscribe\n\n  \n  Subscribe to be notified of new posts:"
  },
  {
    "objectID": "articles/rnn-state-scaling/index.html#footnotes",
    "href": "articles/rnn-state-scaling/index.html#footnotes",
    "title": "RNN State Scaling",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAll of our architectures grow in FLOPs linearly with state size. One could imagine RNN architectures that process each element of their state \\(n^2\\) or more, in which case it might make more sense to modify the state scaling setup accordingly.↩︎\nThe more typical definition, \\(|\\mathcal{S}| &lt; \\infty\\), follows directly from ours if it is assumed that all elements of each \\(S \\in \\mathcal{S}\\) are themselves finite. Since we are only really interested in algorithms that we can implement on computers, and floating point values have finite precision, we are happy to make this simplifying assumption.↩︎\nRNN is a term that is widely used but rarely rigorously defined, and so there is not complete agreement on its definition. In some obscure parts of the literature people may have used the term “RNN” to describe an architecture that is not a finite state machine (as defined here). But we think that the definition we use here is the most widely understood. In any case, we aim only to describe a internally-consistent set of concepts, it is OK if we are not consistent with the whole of the external literature.↩︎\nIn fact, a careful read of our previous article will reveal that we already used this more general form without explicitly commenting on it, by setting \\(\\phi(Q_i) = \\text{softplus}(Q_i)\\).↩︎\nAll of our architectures grow in FLOPs linearly with state size. One could imagine RNN architectures that process each element of their state \\(n^2\\) or more, in which case it might make more sense to modify the state scaling setup accordingly.↩︎"
  },
  {
    "objectID": "articles/release-brumby-14b/index.html",
    "href": "articles/release-brumby-14b/index.html",
    "title": "Brumby-14B-Base: The Strongest Attention-Free Base LLM",
    "section": "",
    "text": "We have trained a completely attention-free LLM whose performance is competitive with state-of-the-art models. This model, which we call Brumby-14B-Base, has a familiar Transformer-style architecture, except it uses power retention layers instead of attention layers. It is available on Huggingface. Here is how it compares to other models at a similar scale on popular benchmarks.\n\n\n\nThe training budget for this model was $4,000, trained 60 hours on a cluster of 32 H100s. (For comparison, training an LLM of this scale from scratch typically costs ~$200k.) We were able to achieve this low cost thanks to the mathematical similarity between attention and power retention. We used a technique we call retraining, which repurposes the weights of a pretrained Transformer as an initialization for power retention. This results in the following loss curve.\n\n\n\nThe initial weights for Brumby-14B-Base came from Qwen3-14B-Base. The jumps in loss correspond to changes in the underlying training distribution, following the three-phase dataset of Nemotron Nano. After 3000 steps of training, it reaches the same training loss on this data as Qwen3-14B-Base. This trend is mirrored by performance on downstream evaluations.\n\n\n\nSo what is power retention? Similar to attention, power retention is a layer that takes \\(Q,K,V \\in R^{t \\times d}\\) as inputs, and gives \\(Y \\in R^{t \\times d}\\) as an output. It also accepts a gating signal \\(g \\in R^t\\). It is a “true” RNN, in that every prediction can be influenced by information arbitrarily far back in the past. The state of the RNN is a matrix $S R^{d D}, which is updated according to\n\\[\nS_t = g_t S_{t-1} + V_t \\phi_p(K_t)^T \\qquad Y_t = S_t Q_t\n\\]\nThe function \\(\\phi_p: R^d \\to R^D\\) is related to the tensor power (the “power” in power retention). The power \\(p\\) controls the dimension \\(D\\), giving us a hyperparameter to scale the state size of the RNN, just as one might use the width to scale the parameter count. For our experiments, power \\(p=2\\) resulted in the optimal state size for the model.\nWhat makes this a retention layer, as opposed to just a recurrent layer, is that it also has an attention form. This second formulation is critical to any hardware-efficient implementation. If you want to learn more about the power retention layer and how to implement it efficiently, see our preprint paper and related blog post. Our hardware-efficient power retention kernels are available open-source and can be installed with pip install retention.\n\nComing soon\nFast long-context inference: Our fastest power retention inference kernels are hundreds of times faster than equivalent attention kernels on long contexts. We will update the architecture to incorporate these fast kernels.\nLong-context SFT: A finetune of Brumby-14B-Base at context length 1,000,000 is no more expensive (per token) than a finetune of Qwen3-14B-Base at context length 10,000. We will release a long context SFT toolkit so that anyone can perform these long-context finetunes, unlocking new capabilities for LLMs in domains like search and coding.\nVLLM integration: A robust inference engine is an essential complement to any SOTA LLM. We are developing kernels to integrate power retention with VLLM. Expect to see both unmatched inference speeds and reduced memory requirements, allowing more users to fit on each GPU.\nThe Brumby Band: Brumby-14B-Base is just the first of a coming family of models. In the coming weeks, we will retrain and release power retention base models at a variety of scales, from as small as 1B parameters up to &gt;100B parameters.\n\n  \n  Subscribe to be notified of new posts:"
  },
  {
    "objectID": "articles/mission/index.html",
    "href": "articles/mission/index.html",
    "title": "Our Mission",
    "section": "",
    "text": "A poem is a shadow of the act of writing poetry.\nHumanity casts many shadows. All literature, letters, recipes, and tweets. Mathematical proofs and git repositories. Laws, treaties, and declarations of war. Financial statements, employee performance reports, and bankruptcy filings. Podcasts, operas, accidental voicemails, YouTube videos and Hollywood blockbusters. Restaurant reviews and love letters and times tables. Individually, each of these pieces of information is nothing but the faintest shadow of the process that produced it. But collectively, these shadows tell a rich story about the world.\nSince the dawn of humanity, the brain alone could reconstruct the world from these shadows. But the last decade of deep learning has convinced us that this won’t be the case for much longer. Though significant challenges remain, we stand poised to solve them. If successful, it will become possible to synthesize every documentable aspect of humanity into the weights of a neural network.\nOur mission is to train a neural network to model all human output.\nThere are two primary challenges in our pursuit of this goal.\n\nCurrently, it’s not technically feasible to train a model that can ingest all the data that we can collect. Limitations around context length, modality, and throughput force us to use only a small subset of the data available to us.\nMuch of the data we will need has never been collected, curated, and organized into datasets that we can use for training.\n\nWe are building a world-class team of engineers and researchers to tackle these challenges, united around shared principles and a specific research agenda. We value clear thinking, sharing knowledge, and an extreme commitment to scientific honesty. Our research is guided by mathematical beauty and grounded in rigorous empiricism. We are committed to letting the quality of our work speak for itself. No hype, no fluff. All meat.\nIf this vision resonates, please reach out:          777b7a607577605479757a7d72716760757d3a777b79\n\n\n  \n  Or, subscribe to be notified of new posts:"
  },
  {
    "objectID": "articles/linear-transformers-are-faster/index.html",
    "href": "articles/linear-transformers-are-faster/index.html",
    "title": "Linear Transformers Are Faster",
    "section": "",
    "text": "\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\Z}{\\mathbb{Z}}\n\\newcommand{\\N}{\\mathbb{N}}\n\\newcommand{\\sft}{\\text{softmax}}\n\\newcommand{\\List}{\\text{List}}\n\\newcommand{\\Seq}{\\text{Seq}}\n\\newcommand{\\SeqT}{\\text{SeqT}}\n\\newcommand{\\CSeqT}{\\text{CSeqT}}\n\\newcommand{\\Dist}{\\text{Dist}}\n\\newcommand{\\SM}{\\text{SM}}\n\\newcommand{\\Fn}{\\text{Fn}}\n\\newcommand{\\Tok}{\\text{Tok}}\n\\newcommand{\\Aij}{ A_{[i,j]}}\n\\]\nIt is well-known that removing the exponential from the attention layer of a transformer allows for a recurrent reformulation, with computational cost that is linear instead of quadratic on context length [1]. One would expect that such an architecture would be far faster to train, especially when the context size is large. However, when training deep neural networks on hardware accelerators (e.g. GPUs), reductions in FLOPs do not always straightforwardly translate into practical speedups. Initial empirical experiments showed that large language models based on linear transformers train more slowly than classic transformers, and led many to dismiss the approach as nice-in-theory but unhelpful-in-practice [2].\nAt the moment, the conventional wisdom in the field is that a quadratic-cost algorithm with highly optimized hardware utilization (e.g. FlashAttention) gives the best training throughput [3]. But this is mistaken. In this post, we explain several different ways of implementing linear transformers and we show that, in fact, they can produce massive speed-ups.\nThe experiment below showcases the central takeaway. We compare the speed of an optimized transformer, a straightforward recurrent implementation of the linear transformer (as described in [1]), and an optimized linear transformer (described in Section 3). We see that, despite exhibiting better growth with respect to context size, the linear transformer trains much more slowly than the transformer in wall-clock time. In contrast, our algorithm is strictly faster than even the highly-optimized FlashAttention baseline, at all context and model sizes. At moderately large context sizes, this speedup has an larger impact than FlashAttention itself.\nBut speed is only one part of the picture. We also need to know whether linear transformers actually minimize the loss as well as standard transformers do. Unfortunately, as the next plot shows, directly converting GPT2 to use linear transformer layers hurts learning significantly.\nThese results are discussed in detail in Section 5, but in short: linear transformers seem to become increasingly unstable as the sequence length grows, negatively affecting learning. This means that our linear transformers are somewhat useless,1 as the positive impact from the speedup seen in long contexts is undermined by the negative impact of degraded learning.\nOther variants of linear transformers have been proposed that claim resolve these learning issues [5]–[11], but we do not survey them today. Since our main motivation in this post is to share our insights on how to implement efficient linear transformers we stick with the most natural variant, which is most closely analogous to standard transformers. In a future post, we will explain how to improve the learning of linear transformers, allowing us to efficiently train unprecedentedly-long-context language models with super-fast sampling."
  },
  {
    "objectID": "articles/linear-transformers-are-faster/index.html#linear-transformers",
    "href": "articles/linear-transformers-are-faster/index.html#linear-transformers",
    "title": "Linear Transformers Are Faster",
    "section": "1. Linear Transformers",
    "text": "1. Linear Transformers\nThe inputs to a transformer layer are sequences of \\(Q_i, K_i, V_i \\in \\R^d\\) of query, key and values, where \\(i\\) ranges from \\(1\\) to the sequence length \\(t\\). The outputs are a sequence \\(Y_i\\in \\R^d\\). The well-known formula for the transformer layer, first popularized by Vaswani et al [12], is: \\[\nY_i^\\text{Transformer} = \\sum_{j=1}^i e^{Q^T_i K_j} V_j\n\\] Here we are excluding the denominator of the softmax for simplicity of notation. Although the normalization provided by the denominator is important for good learning performance, it doesn’t have a major impact on the computational cost or implementation speed, which are our main focus here.\n\n\nEven though we omit the normalization for the mathematical exposition, it is included in our implementation for all experiments.\nThe formula for the linear transformer (LT) layer is quite similar: just change the term \\(e^{Q^T_i K_j} \\to  Q^T_i K_j\\) yielding \\[\nY_i^\\text{LinearTransformer} = \\sum_{j=1}^i Q^T_i K_j V_j\n\\]\n\n\nAll our experiments with linear transformers also include a normalization factor, which we empirically found to be important for learning performance. Drawing on [1], we divide each \\(Y_i\\) by \\(\\sum_{j=1}^i Q^T_i K_j\\) after eunsuring the sum is positive by making keys and queries live in the positive quadrant using softplus.\nThis layer is “linear” in that the outputs \\(Y\\) are linearly related to all of \\(Q\\), \\(K\\), and \\(V\\).2 From now on, we will omit the superscript of \\(Y_i^\\text{LinearTransformer}\\) and just write \\(Y_i\\). To begin our exploration of the computational cost of linear transformers, consider the following implementation.\ndef LT_attention(Q, K, V):\n    \"\"\"\n    Shapes of inputs are\n     Q: [t, d]  K: [t, d]  V: [t, d]\n    Shapes of outputs are\n     Y: [t, d]\n    \"\"\"\n    t, d = Q.shape\n    Y_list = []\n    for i in range(t):           # loop cost: O(t^2 d)\n        Y_i = zeros(d)\n        Q_i = Q[i]\n        for j in range(i+1):     # loop cost: O(id)\n            A_ij = inner(K[j], Q_i)  # cost: O(d)\n            Y_i += A_ij * V[j]   # cost: O(d)\n        Y_list.append(Y_i)\n    return stack(Y_list)\nAnyone who has worked with self-attention will recognize that this is a terrible implementation, since nested for-loops are not GPU-friendly. Don’t worry: in the next section, we will discuss implementations that parallelize computation, and thus run much faster on modern hardware. For now, the main point of this pseudocode is to highlight that first mode of computation, which we call attention formulation, has a FLOP cost of \\(O(t^2 d)\\).\nThe key to the massive speedups we saw in the introduction comes from leveraging linearity to restructure the computation. Consider the following factorization: \\[\nY_i = \\sum_{j=1}^i Q^T_i K_j V_j = \\underbrace{ \\left (  \\sum_{j=1}^i V_j  K_j^T\\right )}_{S_i} \\; \\; Q_i\n\\] Written in this form, we notice that the term labeled \\(S_i \\in \\R^{d\\times d}\\) can be thought of as a state summarizing all the relevant information up to time \\(i\\). It’s easy to rewrite into the following recurrent equations \\[\nY_{i} = S_i Q_i\n\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\nS_i = S_{i-1} + V_i K_i^T\n\\] where we assume \\(S_{0} = 0\\in \\R^{d\\times d}\\). Written in this form, we realize that a linear transformer is an RNN. We can also write pseudocode for this approach, which we call the state formulation, and analyze the cost:\ndef LT_state(Q, K, V):\n    \"\"\"\n    Shapes of inputs are\n     Q: [t, d]  K: [t, d]  V: [t, d]\n    Shapes of outputs are\n     Y: [t, d]\n    \"\"\"\n    t, d = Q.shape\n    S_i = zeros(d, d) # shape [d,d]\n    Y_list = []\n    for i in range(t):        # loop cost: O(t d^2)\n        S_i += outer(K[i], V[i]) # cost: O(d^2)\n        Y_i = S_i @ Q[i]      # cost: O(d^2)\n        Y_list.append(Y_i)\n    return stack(Y_list)\nWe see that the cost here is \\(O(t d^2)\\).\nSo, while a standard transformer layer always has cost \\(O(t^2 d)\\), linear transformers have two formulations with different costs. By switching from the attention formulation to the state formulation, we can change the cost from \\(O(t^2 d)\\) to \\(O(t d^2)\\), trading a \\(t\\) term for a \\(d\\) term."
  },
  {
    "objectID": "articles/linear-transformers-are-faster/index.html#parallel-implementations",
    "href": "articles/linear-transformers-are-faster/index.html#parallel-implementations",
    "title": "Linear Transformers Are Faster",
    "section": "2. Parallel Implementations",
    "text": "2. Parallel Implementations\nIn general, for-loops are a terrible way to implement anything that will run on a GPU. When using high-level frameworks like PyTorch or JAX, the easiest way to get high GPU utilization is to only use primitives that are already highly optimized for GPU: matrix multiplication, elementwise operations, etc. We can rewrite our attention and state algorithms in this style to make them efficient.\nFirst, let’s do this for attention. Our main technique is to compute the attention matrix \\(A\\), which contains all the terms outer(Q[i], K[j]) that appeared inside the for-loops of LT_attention, using a single heavyweight matrix multiply.\ndef LT_attention_parallel_no_flash(Q, K, V):\n    \"\"\"\n    Shapes of inputs are\n     Q: [t, d]  K: [t, d]  V: [t, d]\n    Shapes of outputs are\n     Y: [t, d]\n    \"\"\"\n    t = Q.shape[0]\n    M = causal_mask(t)\n    A_raw = Q @ K.T  # cost O(t^2 d)\n    A = A_raw * M    # cost O(t^2)\n    Y = A @ V        # cost O(t^2 d)\n    return Y\nThis implementation lies at the core of nearly every transformer of the past five years, powering all of the AI models that have recently exploded in popularity. Since it is composed of such a small number of ultra-parallelizable ops, it obtains high GPU utilization. Recently, specialized flash attention kernels [3] have been used to get even further speedups by avoiding explicitly storing the attention matrix \\(A\\), and thereby saving both memory and time spent on memory-transfers. Algorithmically, though, flash attention is a variant of parallel attention, and has the same computational cost (as measured in FLOPs). We use LT_attention_parallel to refer to the flash attention implementation.\nNext, let’s parallelize the recurrent formulation. This optimization is less well-known. The key is to compute all the terms \\(V_i K^T_i\\) in parallel, and then use a cumulative-sum, which can be parallelized, to combine them.\ndef LT_state_parallel(Q, K, V):\n    \"\"\"\n    Shapes of inputs are\n     Q: [t, d]  K: [t, d]  V: [t, d]\n    Shapes of outputs are\n     Y: [t, d]\n    \"\"\"\n    P = V[:,:,None] @ K[:,None,:]  # cost: O(t d^2)\n    S = cumsum(P, axis=0)          # cost: O(t d^2)\n    Y = S @ Q[:,:,None]            # cost: O(t d^2)\n    return Y[:,:0]\nThe cost in FLOPs of this algorithm is \\(O(t d^2)\\).\nNow that we have four mathematically-equivalent implementations of a linear transformer, let’s time the training step of our GPT2 models and see how big of a difference it makes. For our LT_attention_parallel implementation, we use a custom linear self-attention flash kernel we implemented in Triton [13] based on OpenAI’s FlashAttention2 implementation.\n\n\n\n\nHere are some takeaways:\n\nAs expected, the attention variants all have a quadratic asymptotic cost (slope of 2 on a log-log plot3). The state variants all have linear asymptotic cost (slope 1). 4\nLT_state_parallel is an order-of-magnitude faster than LT_state.\nLT_attention_parallel_no_flash is two orders-of-magnitude faster than LT_attention.\nLT_attention_parallel seems to asymptotically stabilize into being an order-of-magnitude faster than LT_attention_parallel_no_flash.\nFor the majority of settings, LT_attention_parallel is the fastest. (This is the linear version of the algorithm used by the standard transformer.)\nParallel attention is the fastest algorithm for small context sizes. However, LT_state_parallel overcomes LT_attention_parallel_no_flash at around 13k context size, and overcomes LT_attention_parallel at around 100k.\n\nOverall, these results paint a clear picture: use the attention algorithm for small contexts and the state algorithm for large contexts. But do we really face a binary choice? Can we combine the state and attention ideas and get the best of both words?"
  },
  {
    "objectID": "articles/linear-transformers-are-faster/index.html#chunked-formulation",
    "href": "articles/linear-transformers-are-faster/index.html#chunked-formulation",
    "title": "Linear Transformers Are Faster",
    "section": "3. Chunked Formulation",
    "text": "3. Chunked Formulation\nIt’s evident that, for small context sizes, computing the \\(t\\) by \\(t\\) attention matrix is much more efficient than computing many \\(d\\) by \\(d\\) state matrices. But as \\(t\\) grows, there is a point where the quadratic cost of the attention matrix ends up dominating. Noticing that attention is extremely efficient for small \\(t\\) and that states are necessary for large \\(t\\) motivates doing one last reworking of the LT equation.\nLet \\(c \\in \\N\\) be a positive integer that we’ll call the chunk size. For any \\(i\\in \\N\\) find the unique \\(n\\in \\Z\\) s.t. \\(cn &lt; i \\le c(n+1)\\). We can easily see that the following equations are equivalent to the previous ones. \\[\nY_{i} = S_{cn}Q_i + \\sum_{j=cn+1}^i Q_i^T K_j V_j\n\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\nS_{c(n+1)} = S_{cn} + \\sum_{j=cn+1}^{c(n+1)} V_j K_j^T\n\\] The key idea is that we are only going to compute a subset of all states: \\(S_0, S_c, S_{2c}, \\cdots\\). Then, to compute each output \\(Y_i\\), we need only to take into account the contribution via the most recent state \\(S_{cn}\\), as well as the contribution (computed via attention) of all moments in time \\(j\\) in the range \\(cn &lt; j \\le i\\).\nAs pseudocode, this looks like:\ndef LT_attention_with_initial_state(S, Q, K, V):\n    \"\"\"\n    Shapes of inputs are\n     S: [d, d]  Q: [c, d]  K: [c, d]  V: [c, d]\n    Shapes of outputs are\n     Y: [c, d]\n    \"\"\"\n    Y_state = Q @ S                               # cost O(c d^2)\n    Y_attention = LT_attention_parallel(Q, K, V)  # cost O(c^2 d)\n    Y = Y_state + Y_attention                     # cost O(cd)\n    return Y\n\ndef LT_chunked(Q, K, V, c):\n    \"\"\"\n    Shapes of inputs are\n     Q: [t, d]  K: [t, d]  V: [t, d], c: int\n    Shapes of outputs are\n     Y: [t, d]\n    \"\"\"\n    t, d = Q.shape\n    assert t % c == 0\n    Q_, K_, V_ = [arr.reshape(t//c, c, d)\n    `               for arr in [Q,K,V]]\n    P_ = K_.transpose([0,2,1]) @ V_  # cost O(t d^2)\n    S_ = cumsum(P_, axis=0) - P_     # cost O((t/c)d^2)\n    Y_ = vmap(LT_attention_with_initial_state, axis=0)(\n                S_, Q_, K_, V_)      # cost O(td^2 + tcd)\n    return Y_.reshape(t, d)\nThe cost is \\(O\\left(td^2 + tcd\\right)\\), once again avoiding a quadratic dependency on \\(t\\). Also, note that this algorithm makes an inner call to LT_attention_parallel, so we can use a flash-attention kernel to do that part of the computation.\nThis algorithm has a hyperparameter, the chunk size, which must be set correctly for optimal performance. Fortunately, it is inexpensive to identify the best chunk size empirically, since measuring just a few steps of training at each chunk size is sufficient to identify which is the fastest. In the plot below, we plot the speed of the optimally-chunked algorithm in each setting.\n\n\n\n\nWe see LT_chunked gives the desired best-of-both-worlds behavior, as it is equal-or-better than all other approaches in all settings. And we now see the massive (& rapidly growing) speedup relative to standard self-attention, finally unlocking the true power of linear transformers."
  },
  {
    "objectID": "articles/linear-transformers-are-faster/index.html#sampling",
    "href": "articles/linear-transformers-are-faster/index.html#sampling",
    "title": "Linear Transformers Are Faster",
    "section": "4. Sampling",
    "text": "4. Sampling\nWhen working with language models, efficient training is not the only performance that deserves consideration. Once the model is trained, how expensive is it to utilize? When we are sampling we have a sequence of tokens, \\(z_1 \\cdots z_t\\), and we want to sample the next token, \\(z_{t+1}\\).\nThe most efficient algorithm to sample from traditional transformers is called the KV-cache algorithm [14]. This algorithm assumes that when we generate token \\(z_{t+1}\\), we will have already computed and cached all the \\(K_i, V_i\\) for all \\(0 \\le i \\le t\\). In order to compute the output of the attention layer at time \\(t+1\\) given this cached information, we can use \\[\nY_{t+1}^\\text{Transformer} = \\sum_{j=1}^{t+1} e^{Q^T_i K_j} V_j\n\\] It is easy to see that this is an \\(O(td)\\) operation. In other words, as the sequence length grows, sampling each subsequent token becomes more computationally expensive.5 This is one of the major limitations of the classic transformer architecture.\nWith linear transformers, however, we have access to the recurrent formulation. A linear transformer is an RNN where the state has size \\(O(d^2)\\). \\[\nY_{i} = S_i Q_i\n\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\nS_i = S_{i-1} + V_i K_i^T\n\\] We can compare the time it takes to generate any particular token when sampling a sequence:\n\n\n\n\nAs expected, we see that the time to sample of the linear transformer is independent of context length, whereas that of the KV-cache transformer grows linearly. This leads to a large gap in inference speed at large contexts.6"
  },
  {
    "objectID": "articles/linear-transformers-are-faster/index.html#learning-performance",
    "href": "articles/linear-transformers-are-faster/index.html#learning-performance",
    "title": "Linear Transformers Are Faster",
    "section": "5. Learning Performance",
    "text": "5. Learning Performance\nUntil now, our focus has been on how to implement linear transformers efficiently. But an architecture that runs really fast is useless unless it also learns well. We will now compare the learning curves of the GPT2 baselines with their linear transformer counterparts, at various context sizes.\nIn order to control for the fact that longer contexts help learning by introducing more tokens per update, we hold the number of tokens-per-update constant across context sizes by decreasing batch size. At all context-lengths, each update sees \\(2^{19}\\) tokens.7 Importantly, for this set of experiments, we have used the dataset c4 [4], which does not have much long-term structure: it consists of a shuffled collection of short articles, of average length ~500 tokens.\nFirst, let’s look at the parameter scaling law of GPT2 and our modified Linear-GPT2. The following plot shows the final performance after 24h of training on our 8xH100 server for each scale and each context size. Click the second tab if you want to see the full learning curves.\n\n\n\n\nBoth architectures scale similarly with respect to model size, but there is a gap in performance. This gap seems to grow as context size increases, together with more loss spikes and general instability. It’s possible that the gap can be explained by the linear transformer not effectively utilizing its context. To explore whether or not this is the case, we can use these same experiments, but visualize all context-lengths together.\n\n\n\n\nWe see a dramatically different trend between the two architectures. For GPT2, each increase in context length slows down the initial pace of learning, but ultimately all context-lengths (beyond at least 1024) converge to a similar final loss. Whereas for Linear-GPT2, not only does increasing the context slow down learning in the beginning, but it also causes convergence to a worse final performance and nasty-looking loss spikes.\nThe results for GPT2 are what we would expect from a healthy model, given the setting. Note that since our dataset consists of short articles, of average length ~500 tokens, we can assume that even a context window of 1024 would include nearly everything relevant. Thus, we wouldn’t expect that increasing the context length beyond this point would decrease the loss the model ultimately converges to. Longer-context models do however need to learn to ignore many more irrelevant tokens, explaining the slowed initial learning.8\nIn contrast, the results for Linear-GPT2 seem to indicate some underlying instability of the linear transformer architecture. It doesn’t even seem capable of ignoring irrelevant information, let alone exploiting useful long-term structure.\nRemedying these learning issues will be the main focus of our future work in linear transformers. Our current architecture is essentially a one-to-one clone of GPT2, sticking as architecturally close to the original as possible; aspects such as initializations, normalizations and hyperparameters were directly copied. It is well-known that these decisions can have a large impact on scaling and stability and often need to be tuned in an architecture-specific way. In the literature, various other linear variants of self-attention have been proposed, that incorporate techniques such as gating mechanisms in order to improve stability and learning [5]–[11]. A future post will include a thorough study of the impact of all of these choices.\nUltimately, it may be impossible for any linear transformer to perfectly match the context scaling laws of the classic transformer baseline. Although removing the exponential seems like a minor change, it represents a meaningful decrease in expressivity. But as we’ve shown, linear transformers can be trained orders-of-magnitude more efficiently. On equivalent compute, linear transformers can be trained for many more steps; or, keeping steps constant as well, we can train a much larger model. If this additional scaling is sufficient to compensate for the reduced expressivity, linear transformers will be the more efficient architecture overall.\nIn follow-up work, we show that a linear transformer variant that we call the symmetric power transformer is able to match the softmax transformer in performance.\n\n  \n  Subscribe to be notified of new posts:"
  },
  {
    "objectID": "articles/linear-transformers-are-faster/index.html#footnotes",
    "href": "articles/linear-transformers-are-faster/index.html#footnotes",
    "title": "Linear Transformers Are Faster",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs discussed in Section 4, a second benefit of linear transformers is that the cost to sample a token does not grow with context size. Perhaps one could argue that this improvement in sampling speed could, on its own, justify using linear transformers for applications where the inference costs vastly exceed training costs. But it is evident to us that, for linear transformers to become actually useful, we need to address these instability issues.↩︎\nIt is not named for the fact that the computational cost is linear with respect to \\(t\\)! That is just a coincidence. (And is not even always true, as we will see.)↩︎\nIf \\(y=x^2\\), a log-log plot where \\(y'=\\log_a(y)\\) and \\(x'=\\log_a(x)\\) for any base \\(a\\), then \\(y'=\\log_a(y) = \\log_a(x^2) = 2 \\log_a(x) = 2 x'\\). So the graph will be a line with slope 2.↩︎\nThe reason we see the expected slopes asymptotically is that we are timing a full GPT2 architecture which has many other components besideds the attention layer. If we were only timing the attention layer, the plots would all be straight lines.↩︎\nAn interesting connection is that the KV-cache can be understood as the state of an RNN with non-constant state size; namely, one whose state-size is \\(O(td)\\).↩︎\nThis comparison may not be completely fair. In these experiments, our implementation of neither sampling algorithm makes use of specialized kernels. A lot of the ideas of flash attention can be used to write a much faster KV cache sampling algorithm; on the other hand, it’s unclear if much improvement is possible on the recurrent sampling. Thus, it’s possible that with engineering effort the gap between the two algorithms could become smaller. However, the overall pattern will certainly remain the same.↩︎\ne.g. runs with context-size 1024 would have batch-size of \\(2^{19} / 2^{10} = 2^{9} = 512\\).↩︎\nPut another way: doubling the size of the input vastly increases the size of the function space over which gradient descent must search, and it’s intuitive that in a larger space it takes somewhat longer to find a good solution.↩︎"
  },
  {
    "objectID": "subscription-confirmation.html",
    "href": "subscription-confirmation.html",
    "title": "Manifest AI",
    "section": "",
    "text": "Almost there..\nWe just sent you an email to confirm your subscription. Click it and you’re in!"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJan 5, 2024\n\n\nLinear Transformers Are Faster After All\n\n\nJacob Buckman, Carles Gelada\n\n\n\n\nJan 4, 2024\n\n\nOur Mission\n\n\nJacob Buckman, Carles Gelada\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "articles/gd-minimizes-loss/index.html",
    "href": "articles/gd-minimizes-loss/index.html",
    "title": "Why Gradient Descent Minimizes Training Loss",
    "section": "",
    "text": "\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\der}{\\partial}\n\\newcommand{\\dldt}{\\frac{\\der l}{\\der t}}\n\\newcommand{\\len}{\\text{length}}\n\\newcommand{\\en}{\\text{energy}}\n\\newcommand{\\relu}{ {\\small\\text{ReLU}} }\n\\newcommand{\\dim}{\\text{dim}}\n\\newcommand{\\tr}{\\text{trace}}\n\\newcommand{\\lin}{\\text{Lin}}\n\\newcommand{\\rnk}{\\text{rank}}\n\\newcommand{\\ht}{\\widehat}\n\\newcommand{\\dwdt}{\\frac{\\der w}{\\der t}}\n\\newcommand{\\l}{\\mathscr{l}}\n\\newcommand{\\E}{\\mathbb E}\n\\]\nEvery student of deep learning has wondered at some point in their journey: “How can gradient descent on a neural network possibly succeed? Surely, on such a large and non-convex loss landscape, we will get stuck in some local minimum, far from the optimal solution.” And yet, deep learning practitioners quickly learn to trust that a large enough neural network with properly tuned hyper-parameters will successfully fit any training data we throw at it [1]. In this article, we explore a mathematical theory to explain this surprising phenomenon.\nOne common approach to showing that gradient descent will converge to a global optimum is to show that the loss landscape is convex, which means that a line drawn between any pair of points will never intersect its surface. This property can be used to prove that no local minima exist [2]. But unfortunately, this technique cannot be applied to neural networks, as the loss function of a neural network is not a convex function of its parameters. This can be demonstrated by a simple counterexample.\nHowever, while convexity is sufficient to prove convergence, it is not required for convergence to occur. For example, look at the plot below. On the left, we see the loss landscape for a quadratic loss on a two-dimensional parameter space. On the right, we see a loss landscape that is similar, except that it has a bulge that clearly makes it non-convex. And yet it is visually evident that neither plot has local minima. Gradient descent would optimize successfully in both settings.\nIn this article, we investigate an alternative property to convexity which we call learning health. Informally, it says that if the loss is large, the gradient must also be large. (In the plot above, both loss landscapes are healthy.)\nThe central result of this article is to use learning health to prove that gradient descent on neural networks will decrease the loss to a small value. The argument goes like this:\nThese three results allow us to conclude that the loss decays exponentially for a meaningful amount of time, and therefore is guaranteed to reach some small value. In general, we observe that the bounds we derive are improved by the size of the neural network, so when using a large enough network, the loss can be guaranteed to fall arbitrarily close to zero.\nIn this article, we apply this theory only to a very simple neural network: a 2-layer MLP trained with the L2 loss on a dataset consisting of a single datapoint. But the general theory we develop here applies more broadly. In follow-up posts, we will show that the results can also be applied to MLPs with multiple layers, trained on multiple data points, and using the cross-entropy loss instead of the L2. There is also existing literature exploring similar ideas (see e.g. [1], [3], [4]).\nThe plot below visualizes the bound we derive, juxtaposed with the actual training curve of a neural network. Every time you click the train button a new set of initial parameters, inputs, and targets are randomly initialized, and 200 steps of gradient descent training are performed. You can see for yourself that the loss always approaches 0, and that our bound is non-vacuous."
  },
  {
    "objectID": "articles/gd-minimizes-loss/index.html#notation-and-definitions",
    "href": "articles/gd-minimizes-loss/index.html#notation-and-definitions",
    "title": "Why Gradient Descent Minimizes Training Loss",
    "section": "0. Notation and Definitions",
    "text": "0. Notation and Definitions\nMuch of the theory in this article treats the space of parameters as a vector space equipped with an inner product. The parameters of a neural network tend to either be elements of \\(\\R^{n}\\) or matrices in \\(\\R^{n\\times m}\\), so we need to define suitable inner products for these two types of vectors.\nDefinition 0.1: Inner Product. For two vectors \\(x,y \\in \\R^n\\) we take the inner product to be \\(&lt;x, y&gt; = x^T y\\). For two matrices \\(M,N\\in \\R^{n\\times m}\\) we use \\(&lt;M, N&gt; = \\tr(M^T N)\\). Any inner product induces a norm \\(|\\cdot|\\) via \\(|v| = \\sqrt{&lt;v,v&gt;}\\), and you can verify that, in the \\(\\R^n\\) and \\(\\R^{n\\times m}\\) cases, this induced norm takes the following forms: \\[\\begin{align}\n|x| = \\sqrt{\\sum_i x_i^2} \\quad\\quad |M| = \\sqrt{\\sum_{ij} M_{ij}^2}\n\\end{align}\\] which are also known as the L2 norms of the respective vector spaces. \\(\\blacksquare\\)\nDefinition 0.2: Derivative. Let \\(f: V \\to W\\) be a function between two vector spaces. The derivative at a point \\(v\\in V\\) is a linear function \\(\\der f(v): V\\to W\\). It tells us how changing the input \\(v \\to v + u\\) will affect the output, if the change \\(u\\in V\\) is small. Concretely, \\[\n\\der f(v)(u) \\simeq f(v + u) - f(v)\n\\] Some people would call \\(\\der f(v)(u)\\) the directional derivative of \\(f\\) along \\(u\\) at a point \\(v\\). \\(\\blacksquare\\)\nDefinition 0.3: Path Derivative. An especially important case is when we are taking the derivative of a function \\(h:\\R \\to V\\), also known as a path through \\(V\\). Here, using the notation \\(\\der h(t): \\R \\to V\\) is a little cumbersome. Instead, we can use Newton’s notation, which defines \\(h'(t) = \\der h(t)(1)\\in V\\). This allows us to think of the derivative of \\(h\\) as a vector in \\(V\\), as opposed to a map \\(\\R \\to V\\). This is possible because any linear map of the form \\(M: \\R \\to V\\) can be turned into scalar-vector multiplication \\(M(r) = r \\: v\\) by defining \\(v = M(1)\\). \\(\\blacksquare\\)\nOne important use of the derivative of a path is to define the length and energy of the path.\nDefinition 0.4: length and energy of a path. Given a normed vector space \\(V\\) and a differentiable path \\(h: [0, t] \\to V\\), the length and energy of the path are defined as \\[\\begin{align}\n\\len(h) &= \\int_0^t |h'(s)| ds \\\\\n\\en(h) &= \\int_0^t |h'(s)|^2 ds\n\\end{align}\\] \\(\\blacksquare\\)\nDefinition 0.5: Gradient. Given a vector space \\(V\\) equipped with an inner product, the gradient of a differentiable function \\(f: V \\to \\R\\), denoted \\(\\nabla f\\), is a map \\(\\nabla f: V\\to V\\) defined so that \\(\\nabla f(v)\\in V\\) is the unique vector satisfying \\[\n\\der f(v)(u) = &lt;\\nabla f(v), u&gt; \\quad \\forall v\\in V\n\\] Whenever it is completely clear from the context which function \\(f\\) we are taking the gradient of we can use the shorthand \\(\\hat v = \\nabla f(v)\\). \\(\\blacksquare\\)\n\n\nYou might be used to a different definition of the gradient. The one you know turns out to be equivalent to the one we introduce here. Take a look at this great article for an in-depth explanation.\nDefinition 0.6: Gradient Flow. Let \\(V\\) be a vector space with an inner product. Given a differentiable and lower-bounded function \\(f: V\\to \\R\\), the gradient flow starting at \\(v_0\\in V\\) is defined as the path \\(\\gamma: [0, \\infty) \\to V\\) satisfying \\[\n\\gamma(0) = v_0  \\quad\\text{and}\\quad \\gamma'(t) = - \\nabla f\\circ \\gamma(t)\n\\] The existence of \\(\\gamma\\) follows from the existence and uniqueness of partial differential equations. The fact that \\(f\\) is lower-bounded guarantees that the solution never diverges, and so \\(\\gamma\\) is a map with domain \\([0, \\infty)\\). \\(\\blacksquare\\)"
  },
  {
    "objectID": "articles/gd-minimizes-loss/index.html#learning-with-gradient-flows",
    "href": "articles/gd-minimizes-loss/index.html#learning-with-gradient-flows",
    "title": "Why Gradient Descent Minimizes Training Loss",
    "section": "1. Learning with Gradient Flows",
    "text": "1. Learning with Gradient Flows\nDefinition 1.1: Learning Problem. The problem setup consists of a tuple \\((W, w_0, f)\\) where:\n\n\\(W\\) is the parameter space, which controls the behavior of the model. Mathematically, it is a vector space equipped with an inner product \\(&lt;\\cdot, \\cdot&gt;\\).\n\\(w_0\\in W\\) is the initial point from which the learning will proceed.\n\\(f: W \\to \\R^+\\) is the loss function, which tells us how good the parameters are (presumably at fitting some dataset). The lower the loss, the better the parameters. A loss function must be lower bounded so it’s nice to assume wlog that \\(\\inf_{w} f(w) = 0\\).\n\nOut of \\(f, W\\) and \\(w_0\\), the following objects are defined:\n\nA path \\(\\gamma: [0, \\infty) \\to W\\) is a gradient flow of \\(f\\) starting at \\(w_0\\). It describes the evolution of the parameters through time.\nA loss curve \\(\\l:\\R \\to \\R\\), defined as \\(\\l(t) = f\\circ \\gamma(t)\\), tells us the amount of loss at any moment in time.\n\n\\(\\blacksquare\\)\nThis setting is deeply connected to the learning algorithms used in practice to train neural networks. But it is worth pointing out some important differences:\n\nLearning algorithms used in practice have discrete aspects. For example, gradient descent starts with the initial parameters \\(w_0\\in W\\) and repeatedly applies the update \\(w_{t+1} = w_t - \\delta \\nabla f(w_t)\\) for some learning rate hyperparameter \\(\\delta \\in \\R^+\\). Clearly, in the limit \\(\\delta \\to 0\\) this discrete path converges to the gradient flow \\(\\gamma\\). But it is less clear whether the learning rates used in practice are small enough for the gradient flow to be a good approximation.\nModern deep learning always uses stochastic gradient descent (SGD). Instead of computing the gradient on the entire dataset, we estimate it by computing the average gradient on a small random subset of the data. These stochastic gradients result in slightly worse updates, but they are much cheaper to compute. This tradeoff is worth making because, for the same amount of compute, it allows us apply many more updates. Again, as the learning rate approaches \\(0\\), SGD converges to the gradient flow. But it is worth asking whether or not this approximation applies in practice.\nIt’s common for deep learning optimizers to include details like a momentum term, a correction for the variance, etc… each of these details is yet another reason why practical algorithms may look quite different from gradient flow.\n\nUltimately, we will want to prove guarantees for the algorithms we run in practice. But studying the behavior of gradient flows is a useful intermediate step. The rest of this section will lay out some of the key mathematical properties of gradient flows that make them so powerful when deriving learning guarantees.\nResult 1.2. The derivative of the loss curve satisfies: \\[\\l'(t) = - | \\nabla f\\circ \\gamma(t)| ^2\\]\n\n\nProof\n\nFor simplicity let \\(w=\\gamma(t)\\). Then, \\[\\begin{align}\n\\l'(t) &= \\der f(w)( \\gamma'(t))\n&\\quad &\\text{(chain rule)} \\\\\n&= &lt; \\nabla f(w), \\gamma'(t)&gt;\n&\\quad &\\text{(gradient def.)} \\\\\n&= -&lt; \\nabla f(w), \\nabla f(w)&gt;\n&\\quad &\\text{(gradient flow def.)} \\\\\n&= - | \\nabla f (w)| ^2\n\\end{align}\\]\n\nSo, under gradient flow, the magnitude of the gradient tells us how fast the loss is decreasing at any moment in time. A large gradient means fast learning! A consequence of this result is that \\(\\en(\\gamma)\\) measures how much the path \\(\\gamma\\) has managed to reduce the loss.\nResult 1.3. Let \\(f:W \\to \\R\\) be a differentiable function and \\(\\gamma:[0, t] \\to W\\) be a gradient flow of \\(f\\) as in definition 1.1. Then:\n\\[\n\\en(\\gamma) = \\l(0) - \\l(t)\n\\]\n\n\nProof\n\n\\[\\begin{align}\n\\en(\\gamma) &= \\int_0^t |\\gamma'(s)|^2 ds \\\\\n&= \\int_0^t |\\nabla f \\circ \\gamma(s)|^2 ds\n&\\quad &\\text{(gradient flow def.)} \\\\\n&= - \\int_0^t  \\l'(s) ds\n&\\quad &\\text{(result 1.2)} \\\\\n&= \\l(0) - \\l(t)\n\\end{align}\\]\n\nThus, we can reformulate questions about \\(\\l(t)\\) into questions about \\(\\en(\\gamma)\\); an interesting shift in perspective. For our purposes, the most important implication is that \\(\\en(\\gamma) \\le \\l(0)\\), i.e., the energy is bounded by the initial loss. This follows from Result 1.3 and from the assumption in Definition 1.1 that \\(f(w) \\ge 0\\).\nThe last step before we are ready to prove the main result of this section is to establish an inequality between the length and energy of a generic path (not necessarily a gradient flow).\nResult 1.4. If \\(h: [0, t] \\to V\\) is a differentiable path, then\n\\[\n\\len(h)^2 \\le t\\;  \\en(h)\n\\]\n\n\nProof\n\nJust note that \\[\\begin{align}\n\\len(h) &=  \\int_0^t |h'(s)| ds  \\\\\n&\\le  \\sqrt{\\int_0^t  1^2 ds } \\sqrt{\\int_0^t | h'(s)|^2 ds } \\;\\;\\;\\;\\;\\; \\text{(by Cauchy Schwarz)} \\\\\n&\\le\\sqrt{ t \\; \\en(h)} \\\\\n\\end{align}\\] From which the result follows.\n\nWe are finally ready to show that, in a short amount of time \\(t\\), the parameters \\(\\gamma(t)\\) cannot move very far away from the initialization.\nResult 1.5. Let \\(\\gamma\\) be the gradient flow of \\(f\\) starting at \\(w_0\\) as in Definition 1.1. Then: \\[\n|\\gamma(t) - w_0| \\le \\sqrt{t\\;\\l(0)}\n\\]\n\n\nProof\n\n\\[\\begin{align}\n|\\gamma(t) - \\gamma(0)|\n&= \\left |\\int_0^t  \\gamma'(s) ds \\right| \\\\\n&\\le \\int_0^t  \\left | \\gamma'(s) \\right| ds\n&\\quad &\\text{(triangle inequality for integrals)} \\\\\n&= \\len(\\gamma) \\\\\n&\\le \\sqrt{t \\; \\en(\\gamma)}\n&\\quad &\\text{(result 1.4)}\n\\end{align}\\]"
  },
  {
    "objectID": "articles/gd-minimizes-loss/index.html#a-replacement-for-convexity",
    "href": "articles/gd-minimizes-loss/index.html#a-replacement-for-convexity",
    "title": "Why Gradient Descent Minimizes Training Loss",
    "section": "2. A Replacement for Convexity",
    "text": "2. A Replacement for Convexity\nAs described in the introduction, our proof utilizes the concept of learning health instead of convexity, which we define the following way: a loss function \\(f : W \\to \\mathbb{R}^+\\) has learning health if there exists \\(\\alpha \\in \\R^+\\) such that for all \\(w \\in W\\), \\[\n\\frac{|\\nabla f(w)|^2}{f(w)}  \\ge \\alpha\n\\] One implication of this definition is that no local minima can exist in a healthy loss landscape. After all, the definition of a local minimum is a point \\(w\\) with no gradient but high loss, the existence of which is ruled out by the condition.\nBut the following result tells us that learning health also implies something much stronger. On a healthy loss landscape, the loss of a gradient flow is guaranteed to decay to \\(0\\) exponentially quickly.\nResult 2.1. If \\(f: W\\to \\R^+\\) satisfies \\(\\frac{|\\nabla f(w)|^2}{f(w)}  \\ge \\alpha\\) for some \\(\\alpha \\in \\R^+\\), then \\[\n\\l(t) \\le \\l(0) \\; e^{-\\alpha t}\n\\]\n\n\nProof\n\n\\[\\begin{align}\n\\frac{\\der \\ln \\l(t)}{\\der t} &= \\frac{\\l'(t)}{\\l(t)}\n= -\\frac{|\\nabla f \\circ \\gamma(t)|^2}{f\\circ \\gamma(t)}\n\\le  -\\alpha \\\\\n\\end{align}\\] so \\[\n\\ln \\l(t) - \\ln \\l(0) = \\int_0^t \\frac{\\der \\ln \\l(s)}{\\der s}  ds \\le - \\int_0^t \\alpha ds = - \\alpha t\n\\] And then \\(\\ln \\l(t)  \\le  \\ln \\l(0) - \\alpha t\\). To conclude, use the fact that exponential is a monotone function and \\[\n\\l(t) = e^{\\ln \\l(t)} \\le  e^{\\ln \\l(0) - \\alpha t} = \\l(0) e^{ - \\alpha t}\n\\]\n\nUnfortunately, neural networks don’t satisfy the learning health property for all \\(w\\in W\\). (To see that, just consider an MLP with “degenerate” parameters, where all the weight matrices are \\(0\\). That MLP will have 0 gradient, even when it has high loss.) But that does not prevent us from using learning health to derive guarantees. In Section 3 we will see that a properly-initialized 2-layer MLP does indeed satisfy a relaxed version of this property: there exists \\(\\alpha,\\beta \\in \\R^+\\) such that for all \\(w \\in W\\), \\[\n\\frac{|\\nabla f(w)|^2}{f(w)}  \\ge \\alpha - \\beta \\; |w-w_0|\n\\]\nThe main result of this section is a guarantee that can be applied to any learning problem satisfying this relaxed property.\nResult 2.2. If \\(f\\) and \\(w_0\\) satisfy \\(\\frac{|\\nabla f(w)|^2}{f(w)}  \\ge \\alpha - \\beta \\; |w-w_0|\\) for all \\(w\\in W\\), then \\[\nl(\\infty) \\le \\l(0) \\; \\exp({-\\frac {\\alpha^3}{3 \\beta^2 \\l(0)} })\n\\]\n\n\nProof\n\nThe proof follows a very similar argument to 2.1,\n\\[\\begin{align}\n\\frac{\\der \\ln \\l(t)}{\\der t} &= -\\frac{|\\nabla f(w)|^2}{f(w)} \\quad &\\text{(result 1.3)} \\\\\n&\\le - \\alpha + \\beta \\;  |w - w_0| \\\\\n&\\le - \\alpha + \\beta \\sqrt {\\l(0) t} \\quad &\\text{(result 1.5)} \\\\\n\\end{align}\\]\nIntegrating both sides from \\(0\\) to \\(t\\) we get \\[\n\\ln \\l(t) - \\ln \\l(0) \\le - \\alpha t + \\frac{2}{3}  \\beta \\sqrt {\\l(0)} \\; t^{3/2}\n\\]\nwhich implies \\[\n\\l(t) \\le \\l(0) \\exp(- \\alpha t + \\frac{2}{3}  \\beta \\sqrt \\l(0) \\; t^{3/2})\n\\]\nNow, we want to find the value of \\(t\\) that minimizes the term in the exponential. We set the derivative to \\(0\\) by solving \\(- \\alpha t + \\frac{2}{3}  \\beta \\sqrt {\\l(0)} \\; t^{3/2}=0\\) which you can easily see is achieved at \\(t^* = \\frac {\\alpha^2} {\\beta^2 \\l(0)}\\). This implies that, \\[\n\\l(t^*) \\le \\l(0) \\exp({-\\frac{\\alpha^3}{3\\beta^2 \\l(0)}})\n\\] And, since \\(\\l'(t) = -|\\nabla f \\circ \\gamma(t)|^2 \\le 0\\), the loss function is monotonically decreasing, which implies that \\(\\l(\\infty) \\le \\l(t^*) \\le \\l(0)\\exp({-\\frac{\\alpha^3}{3\\beta^2 \\l(0)}})\\), proving the result."
  },
  {
    "objectID": "articles/gd-minimizes-loss/index.html#the-simplest-neural-network",
    "href": "articles/gd-minimizes-loss/index.html#the-simplest-neural-network",
    "title": "Why Gradient Descent Minimizes Training Loss",
    "section": "3. The Simplest Neural Network",
    "text": "3. The Simplest Neural Network\nThe objective of this section is straightforward. To take the absolute simplest neural network and show that the conditions of Result 2.2 are satisfied. We’ll look at a 2-layer MLP with ReLU nonlinearity. It has two parameter matrices \\(M\\in \\R^{k\\times m}\\) and \\(N\\in \\R^{n\\times k}\\). The ReLU is denoted by \\(\\sigma\\). Given an input \\(x \\in \\R^m\\), the MLP produces the output \\[\ny = N \\sigma(M x)\n\\] It is convenient to define intermediate variables by breaking down the computation into steps. \\[\na = Mx, \\quad b=\\sigma(a) \\quad y=Nb\n\\] Parameter space. The weight vectors are pairs of matrices \\(w = (N, M)\\) and so the parameter space is \\[W= \\R^{n\\times k} \\oplus \\R^{k\\times m}\\] The loss function. In keeping with the philosophy of studying the simplest example, we’ll take the loss function to be the L2 error on a single input-target pair \\((x, q)\\in \\R^n \\oplus \\R^m\\). \\[\nf(w) = \\frac 1 2 |y - q|^2\n\\] We will need \\(x\\) to be normalized, so we just assume that \\(|x|^2 = m\\). It’s also useful to define the loss as a variable \\(L = f(w)\\).\nInitialization. The standard way to initialize a neural network is to independently sample all the entries in the weight matrices from some distribution. In our case, we use \\[\nM_{ij} \\sim \\mathcal N (0, {\\small \\frac 2 m} ) \\quad\\quad N_{ij} \\sim \\mathcal N (0, {\\small \\frac 2 k})\n\\] This is the commonly-used He initialization, which works well for networks with ReLU nonlinearities. This initialization provides us with the guarantee that, with high probability, \\[\n|b|^2 \\simeq k\n\\]\n\n\nSee a derivation of this property\n\nRemember that we are sampling the initial weight matrices via \\[\nM_{ij} \\sim \\mathcal N (0, {\\small \\frac 2 m} ) \\quad\\quad N_{ij} \\sim \\mathcal N (0, {\\small \\frac 2 k})\n\\] Since \\(M,N\\) are random variables, so are \\(a\\) and \\(b\\). He initialization uses carefully-chosen variances to ensure that the entries of the initial activations \\(a = M x\\) and \\(b = \\sigma(a)\\) are within some reasonable range. In particular, we want the entries of \\(b_i\\) to have \\(0\\) mean and variance \\(1\\). And since all the entries are independent, that will mean that \\(|b|^2 \\simeq k\\) and so \\(b\\) will be close to the sphere of radius \\(\\sqrt k\\) with high probability. (But this is only the case if the input vector is also normalized; that is why we needed the assumption that \\(|x|^2 = m\\).) We will now prove these statements.\nFirst we want to understand \\(\\E[a_i^2]\\) \\[\\begin{align}\n\\E[a_i^2]\n&= \\E[ ({\\small \\sum_j M_{ij} x_j})^2] \\\\\n&= \\E[ {\\small \\sum_j M_{ij}^2 x_j^2 + \\sum_{k\\neq j} M_{ij} x_j M_{ik} x_k } ] \\\\\n&= \\sum_j \\E[M_{ij}^2 x_j^2] \\\\\n&= \\frac 2 m |x|^2= 2\n\\end{align}\\] where we used the independence of different entries of \\(M\\) and the fact that \\(\\E[M_{ij}]=0\\). Then \\[\n\\E[|a|^2] = \\sum_i \\E[a_i^2] = 2k\n\\] Let \\(p_M\\) denote the probability distribution functions (PDFs) of the entries of \\(M_{ij}\\) (all entries have the same PDF because they are independent and identically distributed). The PDF of \\(a_i\\) will be the nested integral of \\(\\delta(a_i - M_i^T x)\\) as \\(M_{i1},\\cdots,M_{im}\\) range from \\(-\\infty \\to \\infty\\), where \\(\\delta\\) denote the Dirac delta function. \\[\np_{a}(z) = \\int_{-\\infty}^\\infty \\cdots \\int_{-\\infty}^\\infty p_M(M_{i1}) \\cdots p_M(M_{im}) \\; \\delta (z - M_i^T x)\\; d M_{i1} \\cdots d M_{im}\n\\] Recall that a distribution is symmetric if \\(p(z) = p(-z)\\). Since \\(p_M\\) is a Gaussian, it is symmetric. The next thing we need to prove is that \\(p_a\\) is symmetric too. \\[\\begin{align}\np_{a}(z) &= \\int_{-\\infty}^\\infty \\cdots \\int_{-\\infty}^\\infty p_M(M_{i1}) \\cdots p_M(M_{im}) \\; \\delta (z - M_i^T x)\\; d M_{i1} \\cdots d M_{im} \\\\\n&=  \\int_{-\\infty}^\\infty \\cdots \\int_{-\\infty}^\\infty p_M(-M_{i1}) \\cdots p_M(-M_{im}) \\; \\delta (z + M_i^T x)\\; d M_{i1} \\cdots d M_{im} \\\\\n&=  \\int_{-\\infty}^\\infty \\cdots \\int_{-\\infty}^\\infty p_M(M_{i1}) \\cdots p_M(M_{im}) \\; \\delta (-z - M_i^T x)\\; d M_{i1} \\cdots d M_{im} \\\\\n&= p_a(-z)\n\\end{align}\\] The first step uses the change of variable \\(M_{ij}\\to -M_{ij}\\), and the second one exploits the symmetry of \\(p_M\\) and \\(\\delta\\). Finally, we compute the term we care about: \\[\\begin{align}\n\\E[b_i^2] &= \\int_{-\\infty}^\\infty p_a(a_i) \\sigma(a_i)^2 da_i \\\\\n&= \\int_{-\\infty}^0 p_a(a_i) \\:0\\: da_i + \\int_0^\\infty p_a(a_i) a_i^2 da_i \\\\\n&= \\frac 1 2 \\int_{-\\infty}^\\infty p_a(a_i) a_i^2 da_i \\quad \\text{(using symmetry)}\\\\\n&= \\frac 1 2 \\E[a_i^2] \\\\\n&= k\n\\end{align}\\] We have only been working out \\(\\E[|b|^2] = k\\), but we wanted to make claims about the particular samples themselves being approximately \\(|b|^2\\simeq k\\). The rigorous way to go about doing so would be to prove statements like \\(\\Pr( k - \\epsilon \\le |b|^2 \\le k+\\epsilon) \\le \\alpha\\). But this type of argument is very tedious and adds little insight about the ideas this document is exploring, so instead, in the rest of this article we just assume that \\(|b|^2 = k\\). When the dimension \\(k\\) is large, this approximation will be very accurate.\n\nGradients. The gradients of the loss \\(\\l\\) wrt all the intermediate variables and weights are: \\[\n\\ht y = y -q, \\quad\n\\ht b = N^T \\ht y, \\quad\n\\ht a =\\der \\sigma(a)^T \\ht b, \\quad\n\\ht M = \\ht a x^T, \\quad\n\\ht N = \\ht y b^T, \\quad\n\\]\n\n\nIn this notation, a hat on top of a variable denotes the gradient of the loss function wrt that variable.\n\n\nExpand for full derivation of the gradients.\n\nNote that \\(|\\hat y |^2 = 2 \\l\\) so, when the loss is large, the gradient of \\(\\l\\) wrt \\(y\\) is large too. Ultimately we are trying to show something similar, but about gradients of \\(\\l\\) wrt \\(M\\) (in order to then apply Result 2.2.)\nBelow, we’ve written a short derivation of all these gradient formulas, but using some unconventional notation and techniques. If you find them confusing, just work out the gradients in your own way and confirm you get the same answers.\nStarting with \\(\\ht y\\), the gradient of \\(\\l\\) wrt \\(y\\). Let \\(\\dot y \\in \\R^n\\) denote an arbitrary change to \\(y\\). Then \\[\\begin{align}\n&lt;\\ht y, \\dot y&gt; =\\frac {\\der \\l} {\\der y} (\\dot y)\n&\\simeq \\frac 1 2 |y +\\dot y - q|^2 - \\frac 1 2 |y - q|^2 \\\\\n&= \\frac 1 2 ( &lt;\\dot y, y-q&gt; + &lt;\\dot y, \\dot y&gt; + &lt;y-q, \\dot y&gt; ) \\\\\n&\\simeq &lt;y-q, \\dot y&gt; \\quad \\text{(dropping the lower order term)} \\\\\n\\end{align}\\] Now, let’s see how a change in \\(b\\) affects \\(y\\). Like before, let \\(\\dot b\\) denote a change to \\(b\\). The derivative \\(\\frac {\\der y}{\\der b}(\\dot b) \\simeq N(b+ \\dot b) - M b = \\dot M b\\). So, the gradient of \\(\\l\\) wrt \\(b\\) satisfies \\[\n&lt;\\ht b, \\dot b&gt;\n= {\\frac {\\der \\l}{\\der b}(\\dot b)}\n= {\\frac {\\der \\l}{\\der y}} \\left(  {\\frac {\\der y}{\\der b}}(\\dot b) \\right)\n=  &lt;\\ht y,  {\\frac {\\der y}{\\der b}} (\\dot b)&gt;\n= &lt;\\hat y, M \\dot b&gt; = &lt;M^T \\ht y, \\dot b&gt;\n\\] Now, let’s look at \\(N\\). The derivative \\(\\frac {\\der y}{\\der N}(\\dot N) \\simeq (N+\\dot N) b - N b = \\dot N b\\). And the gradient \\[\n&lt;\\ht N, \\dot N&gt;  = &lt;\\ht y, \\dot N b&gt; = &lt;\\ht y b^T, \\dot N&gt;\n\\] Recall that, since we are taking the inner product of two matrices, we are using the trace under the hood. To see why the last step is true, just use the cyclic property of the trace. Finally, gradients of \\(a\\) and \\(M\\): \\[\\begin{gather}\n&lt;\\hat a, \\dot a&gt;\n= &lt;\\hat b, \\der \\sigma(a)(\\dot a)&gt;\n= &lt;\\der \\sigma(a)^T \\ht b, \\dot a&gt;  \\\\\n&lt;\\ht M, \\dot M&gt;\n= &lt;\\ht a, \\dot M x&gt; = &lt;\\ht a x^T, \\dot M&gt;\n\\end{gather}\\]\n\nNote that \\(|\\hat y|^2 = 2 f(w) = L\\), the gradient of the loss wrt the output, is proportional to the loss itself.\nGradient health. The gradient of the loss wrt the matrix \\(N\\) satisfies \\[\n|\\ht{N} |^2\n\\ge 2 L ( k - 2 \\sqrt {k} |w_0 - w| ) \\\\\n\\]\n\n\nExpand for activation and gradient bounds.\n\nFirst, let’s review a basic fact. Given a matrix \\(A\\in \\R^{n\\times m}\\) and \\(x\\in \\R^m\\) we have that \\(|Ax| \\le |A| \\; |x|\\). This follows form the SVD decomposition \\(A = R \\Lambda S^T\\), where \\(R,S\\) are orthogonal matrices and \\(\\Lambda\\) is a diagonal matrix with the singular values \\(\\lambda_1,\\cdots, \\lambda_n \\ge 0\\) in it’s diagonal. First, note that \\[\n|A^T A |^2 = | S \\Lambda^2 S^T|^2 = | \\Lambda^2 |^2 = \\sum_i \\lambda_i^4 \\le \\left( \\sum_i \\lambda_i^2 \\right)^2 = | \\Lambda |^4 = | R \\Lambda S^T|^4 = | A |^4\n\\] So \\[\\begin{align}\n|A x|^2 &= \\tr(x^T A^T A x) = \\tr(A^T A x x^T) = &lt;A^T A, x x^T&gt; \\\\\n&\\le  |A^T A| |x x^T| = |A^T A| |x|^2 \\\\\n&\\le |A|^2 |x|^2\n\\end{align}\\]\nNow, recall that \\(w_0 = (M_0, N_0)\\) and let \\(a_0=M_0 x, b_0=\\sigma(a_0)\\) and \\(y_0=N_0 b_0\\) be the activations at initialization. We want to derive upper bounds on \\(|a - a_0|\\) and \\(|b - b_0|\\) based on \\(|w-w_0|\\). First, \\[\\begin{align*}\n|a_0 - a| &\\le |M_0 x - Mx| \\\\\n          & = |M_0 - M| \\; |x| \\quad \\text{(using the fact we just proved)} \\\\\n          & \\le |w-w_0| \\; |x| \\\\\n          & = \\sqrt m \\; |w-w_0|\n\\end{align*}\\] where the last step used the assumption that \\(|x| = \\sqrt m\\) (the inputs are normalized). To bound \\(|b_0 - b |\\) we need to use the fact that the ReLU \\(\\sigma\\) is 1-Lipschitz. So \\[\n|b_0 - b| = |\\sigma(a) - \\sigma(a_0) | \\le |a_0 - a | \\le \\sqrt m \\; |w-w_0|\n\\]\nRecall that our weight initialization guarantees that \\(|b_0| \\simeq \\sqrt k\\) (for simplicity we assume exact equality). Now we can conclude with: \\[\\begin{align}\n| \\ht{N} |^2 &= |\\ht y b^T|^2 = \\text{trace}(b y^T y b^T)\n              = |\\ht{y}|^2 |b|^2\n              = 2 L |b|^2 \\\\\n              &\\ge L (|b_0|  - |b_0 - b|)^2\n              = 2 L ( \\sqrt k - |b_0 - b| )^2 \\\\\n              &= 2 L ( k - 2 \\sqrt k |b_0 - b| +  |b_0 - b| ^2 ) \\\\\n              &\\ge 2 L ( k - 2 \\sqrt k |b_0 - b| ) \\\\\n              &\\ge 2 L ( k - 2 \\sqrt {km} |w_0 - w| ) \\\\\n\\end{align}\\]\n\nWe could also attempt to derive a lower bound for \\(|\\ht M|^2\\), but it is not really necessary to do so. We already have enough to apply 2.2.\nLearning Guarantees. Since \\(|\\nabla f(w)|^2 = |\\ht N|^2 + |\\ht N|^2\\), the previous result implies that \\[\n\\frac{|\\nabla f(w)|^2}{f(w)} \\ge 2k - 4 \\sqrt {km} |w_0 - w|\\\\\n\\] So by setting \\(\\alpha = 2k\\) and \\(\\beta = 4 \\sqrt {km}\\), the application of Result 2.2 gives \\[\n\\l(\\infty) \\le \\l(0) \\; \\text{exp}({-\\frac {k^2}{6 m \\l(0)} })\n\\] From looking at the above equation, it is apparent that the scale of the MLP helps it learn. By growing \\(k\\) we can very quickly guarantee that the loss is decreased to any desired value.\nBelow is an interactive visualization that can be played with to see how this simple 2-layer MLP learns. The red line shows the bound we’ve just derived."
  },
  {
    "objectID": "articles/optimizing-symmetric-power-transformers/index.html",
    "href": "articles/optimizing-symmetric-power-transformers/index.html",
    "title": "Improving Symmetric Power Transformers with Conformal Transformations",
    "section": "",
    "text": "\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\Z}{\\mathbb{Z}}\n\\newcommand{\\N}{\\mathbb{N}}\n\\newcommand{\\sft}{\\text{softmax}}\n\\newcommand{\\List}{\\text{List}}\n\\newcommand{\\Seq}{\\text{Seq}}\n\\newcommand{\\SeqT}{\\text{SeqT}}\n\\newcommand{\\CSeqT}{\\text{CSeqT}}\n\\newcommand{\\Dist}{\\text{Dist}}\n\\newcommand{\\SM}{\\text{SM}}\n\\newcommand{\\Fn}{\\text{Fn}}\n\\newcommand{\\Tok}{\\text{Tok}}\n\\newcommand{\\Aij}{ A_{[i,j]}}\n\\newcommand{\\ten}{\\small\\text{tensor}}\n\\newcommand{\\sym}{\\small\\text{symmetric}}\n\\]"
  },
  {
    "objectID": "articles/optimizing-symmetric-power-transformers/index.html#symmetric-power-transformers",
    "href": "articles/optimizing-symmetric-power-transformers/index.html#symmetric-power-transformers",
    "title": "Improving Symmetric Power Transformers with Conformal Transformations",
    "section": "1. Symmetric Power Transformers",
    "text": "1. Symmetric Power Transformers\nWe begin with a high level overview of symmetric power transformers. The inputs to the layer are sequences of \\(Q_i, K_i, V_i \\in \\R^d\\) of queries, keys, and values, where \\(i\\) ranges from \\(1\\) to the sequence length \\(t\\). The outputs are a sequence \\(Y_i\\in \\R^d\\). In the attention formulation, the formula for the output vectors is: \\[\nY_i = \\sum_{j=1}^i A_{ij} V_j \\qquad A_{ij}  = \\frac{B_{ij}}{\\sum_{k=1}^i B_{ik}} \\qquad B_{ij} = (Q_i^T K_j)^p\n\\qquad \\text{(sympow)}\n\\] We refer to \\(A_{ij}\\) as the attention scores and \\(B_{ij}\\) as the preattention scores (mirroring the preactivation/activation lanugage often use to desrive the hidden values of an MLP before and after the nonlinearity).\n\n\nIt is important that the power \\(p\\) is even because that guarantees the denominator is positive, which makes \\(A_{i1}, \\cdots, A_{ii}\\) a valid probability distribution. In turn, this makes the outputs \\(Y_i\\) a convex combinatoin of \\(V_1, \\cdots, V_i\\).\nThe exact same outputs \\(Y_i\\) can be computed via a recurrent formulation. Doing so invovles an embedding function \\(\\phi^p : \\R^d \\to \\R^D\\). The vector \\(\\phi^p(k)\\) contains the same information as \\({k\\otimes \\cdots \\otimes k}\\), repeatedly taking tensor product \\(p\\) times. But it does so much more efficiently because it removes a lot of symmetry in the tensor product. Thus \\(D &lt;&lt; d^p\\). Using this embedding function, we can write the recurrent equations: \\[\nY_{i} = \\frac{S_i \\phi^p(Q_i)}{Z_i \\phi^p(Q_i)} \\qquad Z_i = Z_{i-1} + \\phi^p(K_i)^T \\qquad S_i = S_{i-1} + V_i \\phi^p(K_i)^T\n\\] where \\(Z_0\\) and \\(S_0\\) are \\(\\mathcal 0\\) vectors in their respective spaces. Since \\(S_i \\in \\R^{d \\times D}\\) and \\(Z_i \\in \\R^{D}\\), the size of the state is \\(D(d+1)\\).\nThese two forms give rise to a variety of algorithms for training linear transformers, with differing computational properties. Read our earlier article on linear transformers for a detailed explanation.\n\n1.1 Rotary embeddings (RoPE)\nIn our previous post on symmetric power transformers, we briefly discussed that even symmetric power transformers were compatible with RoPE. In this section, we will give the issue the proper discussion it deserves by deriving the attention and recurrent implementations.\nRotary embeddings [7] encode time information by rotating the keys and queries by an amount proportional to their corresponding timestep. The rotation matrix \\(R\\in \\R^{d\\times d}\\) tells us how much we want to rotate every timestep, so that: \\[\nQ'_{i} = R^i Q_i \\qquad K'_j = R^j K_j\n\\] Then the preattention is changed to: \\[\nB_{ij} = \\left({Q'_{i}}^T K'_j \\right)^p = \\left({Q_{i}}^T (R^{i-j})^T K_j \\right)^p\n\\qquad \\text{(sympow rotary)}\n\\] It is evident that the effect of rotation of the embeddings is relative because it modulates interaction between \\(Q_i\\) and \\(K_j\\) depending only on the time difference \\(i-j\\).\nThe rotation matrix \\(R\\) is constructed in a particular way. We start with some range of rotation rates \\(\\theta_1, \\theta_2, \\cdots, \\theta_{\\frac d 2}\\) defined by the formula \\(\\theta_i = \\frac{2\\pi}{N^{\\frac{2(i-1)}{d}}}\\), where \\(N\\) is the maximum document size. The vector \\(\\theta\\) contains these rotation rates. Then, the rotation matrix is \\[\\small\nR(\\theta) = \\begin{pmatrix}\n\\cos(\\theta_1) & -\\sin(\\theta_1) & 0 & 0 & \\cdots & 0 & 0 \\\\\n\\sin(\\theta_1) & \\cos(\\theta_1)  & 0 & 0 & \\cdots & 0 & 0 \\\\\n0 & 0 & \\cos(\\theta_2) & -\\sin(\\theta_2) & \\cdots & 0 & 0 \\\\\n0 & 0 & \\sin(\\theta_2) & \\cos(\\theta_2)  & \\cdots & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & 0 & \\cdots & \\cos(\\theta_{d/2}) & -\\sin(\\theta_{d/2}) \\\\\n0 & 0 & 0 & 0 & \\cdots & \\sin(\\theta_{d/2}) & \\cos(\\theta_{d/2})\n\\end{pmatrix}\n\\]\nWhen multiplying a query or key vector by this rotation matrix, each pair of dimensions indexed by \\(2j - 1\\) and \\(2j\\) for \\(j \\in \\{1, 2, ..., \\frac{d}{2} \\}\\) is rotated by a different amount \\(\\theta_j\\). Rotating each pair by a different angle helps break symmetry and increases the expressiveness of the positional encodings.\nA computational advantage of using rotation matrices of this form is that \\(R(\\theta)^k = R(k \\theta)\\), which massively simplifies the cost of computing all the \\(Q'_i\\) and \\(K'_j\\).\n\n\nExpand to see a proof of this fact\n\nWe are given a block diagonal rotation matrix \\(R(\\theta)\\), where each \\(2 \\times 2\\) block corresponds to a rotation by some angle \\(\\theta_i\\):\n\\[\nR(\\theta) = \\begin{pmatrix}\n\\cos(\\theta_1) & -\\sin(\\theta_1) & 0 & 0 & \\cdots & 0 & 0 \\\\\n\\sin(\\theta_1) & \\cos(\\theta_1)  & 0 & 0 & \\cdots & 0 & 0 \\\\\n0 & 0 & \\cos(\\theta_2) & -\\sin(\\theta_2) & \\cdots & 0 & 0 \\\\\n0 & 0 & \\sin(\\theta_2) & \\cos(\\theta_2)  & \\cdots & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & 0 & \\cdots & \\cos(\\theta_{d/2}) & -\\sin(\\theta_{d/2}) \\\\\n0 & 0 & 0 & 0 & \\cdots & \\sin(\\theta_{d/2}) & \\cos(\\theta_{d/2})\n\\end{pmatrix}.\n\\]\nWe aim to prove that \\(R(\\theta)^k = R(k \\theta)\\), where \\(k\\) is a positive integer, and \\(k \\theta = (k \\theta_1, k \\theta_2, \\dots, k \\theta_{d/2})\\).\nWe prove this statement by induction on \\(k\\) for a single \\(2 \\times 2\\) rotation matrix \\(R(\\theta_i)\\), and then extend it to the full block diagonal matrix.\nFor the base case (k = 1), we have:\n\\[R(\\theta_i)^1 = R(\\theta_i)\\],\nwhich is equivalent to \\(R(1 \\cdot \\theta_i) = R(\\theta_i)\\). Thus, the base case holds.\nAssume that for some positive integer \\(k\\), the property holds:\n\\[R(\\theta_i)^k = R(k \\theta_i).\\]\nWe need to show that \\(R(\\theta_i)^{k+1} = R((k+1)\\theta_i)\\). Using the definition of matrix exponentiation:\n\\[R(\\theta_i)^{k+1} = R(\\theta_i) R(\\theta_i)^k.\\]\nBy the inductive hypothesis, \\(R(\\theta_i)^k = R(k\\theta_i)\\). Substituting this:\n\\[R(\\theta_i)^{k+1} = R(\\theta_i) R(k \\theta_i).\\]\nThe product of two rotation matrices corresponds to a rotation by the sum of their angles. Therefore:\n\\[R(\\theta_i) R(k \\theta_i) = R(\\theta_i + (k \\theta_i)) = R((k+1)\\theta_i).\\]\nThus, \\(R(\\theta_i)^{k+1} = R((k+1)\\theta_i)\\), completing the inductive step. By induction, the property holds for all ( k ).\n\nConsider the block diagonal matrix ( R() ), where: \\[\nR(\\theta) = \\begin{pmatrix}\nR(\\theta_1) & 0 & \\cdots & 0 \\\\\n0 & R(\\theta_2) & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & R(\\theta_{d/2})\n\\end{pmatrix}.\n\\]\nSince each block \\(R(\\theta_i)\\) is independent of the others, the \\(k\\)-th power of \\(R(\\theta)\\) is the block diagonal matrix with each block raised to the \\(k\\)-th power: \\[\nR(\\theta)^k = \\begin{pmatrix}\nR(\\theta_1)^k & 0 & \\cdots & 0 \\\\\n0 & R(\\theta_2)^k & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & R(\\theta_{d/2})^k\n\\end{pmatrix}.\n\\]\nUsing the result for a single rotation matrix, \\(R(\\theta_i)^k = R(k\\theta_i)\\), we get: \\[\nR(\\theta)^k = \\begin{pmatrix}\nR(k \\theta_1) & 0 & \\cdots & 0 \\\\\n0 & R(k \\theta_2) & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & R(k \\theta_{d/2})\n\\end{pmatrix}.\n\\]\nThis is equivalent to the block diagonal matrix \\(R(k\\theta)\\), where \\(k\\theta = (k\\theta_1, k\\theta_2, \\dots, k\\theta_{d/2})\\).\nThus, by induction and the block diagonal structure, \\(R(\\theta)^k = R(k\\theta)\\) for any positive integer \\(k\\).\n\nNow we want to find the recurrent formulation of rotary embeddings with symmetric power transformers. A simple way we can do that is by including one extra vector in the recurrent state which is now a tuple \\((S, Z, \\mu)\\), where \\(\\mu \\in \\R^{\\frac d 2}\\). The recurrent equations are given by \n\\[\nZ_i = Z_{i-1} + \\phi^p(R(\\mu_i) K_i)^T \\qquad S_i = S_{i-1} + V_i \\phi^p(R(\\mu_i) K_i)^T \\qquad \\mu_i = \\mu_{i-1} + \\theta\n\\]\nNote we rotate the keys by \\(R(\\mu_i)\\) before using them.\nGiven \\(S_i\\) and \\(Z_i\\), the outputs are the same as before, except that we rotate the queries by \\(R(\\mu)\\) before using them: \\[\nY_{i} = \\frac{S_i \\phi^p( R(\\mu_i) Q_i)}{Z_i \\phi^p( R(\\mu_i) Q_i)}\n\\]\n\n\nExpand for a proof of the equivalence between the state and recurrent formulations\n\nWe begin by writing the output \\(Y_i\\) at time step \\(i\\) in the attention formulation. For notational simplicity, let \\(C_i = \\sum_{k=1}^i (Q_i'^T K_k')^p = \\sum_{k=1}^i \\phi^p(Q_i')^T \\phi^p(K_k')\\).\n\\[\n\\begin{align}\nY_i &= \\sum_{j=1}^i \\frac{\\left( Q_i^T (R^{i-j})^T K_j\\right) ^p V_j}{C_i} \\\\\n    &= \\sum_{j=1}^i \\frac{\\left( Q_i^T (R^i)^T R^j K_j\\right)^p V_j}{C_i} \\\\\n    &= \\sum_{j=1}^i \\frac{\\left(Q_i^T R(\\mu_i)^T R(\\mu_j) K_j\\right)^p V_j}{C_i} \\\\\n    &= \\sum_{j=1}^i \\frac{\\left((R(\\mu_i) Q_i)^T R(\\mu_j) K_j\\right)^p V_j}{C_i} \\\\\n    &= \\sum_{j=1}^i \\frac{\\left(\\phi^p(R(\\mu_i) Q_i)^T \\phi^p(R(\\mu_j) K_j)\\right) V_j}{C_i} \\\\\n    &= \\sum_{j=1}^i \\frac{V_j \\phi^p(R(\\mu_j) K_j)^T \\phi^p(R(\\mu_i) Q_i)}{C_i} \\\\\n    &= \\frac{\\left( \\sum_{j=1}^i V_j \\phi^p(R(\\mu_j) K_j)^T \\right)  \\phi^p(R(\\mu_i) Q_i) }{C_i} \\\\\n    &= \\frac{S_i  \\phi^p(R(\\mu_i) Q_i) }{Z_i \\phi^p(R(\\mu_i) Q_i)}\n\\end{align}\n\\]\nwhich is the recurrent formulation of the output \\(Y_i\\). The last line above uses the fact that\n\\[\n\\sum_{j=1}^i V_j \\phi^p(R(\\mu_j) K_j)^T = S_i\n\\]\nand\n\\[\n\\begin{align}\nC_i &= \\sum_{k=1}^i \\phi^p(Q_i')^T \\phi^p(K_k') \\\\\n    &= \\sum_{k=1}^i \\phi^p(R(\\mu_i) Q_i)^T \\phi^p(R(\\mu_k) K_k) \\\\\n    &= \\sum_{k=1}^i \\phi^p(R(\\mu_k) K_k)^T \\phi^p(R(\\mu_i) Q_i) \\\\\n    &= Z_i \\phi^p(R(\\mu_i) Q_i)\n\\end{align}\n\\]"
  },
  {
    "objectID": "articles/optimizing-symmetric-power-transformers/index.html#gating",
    "href": "articles/optimizing-symmetric-power-transformers/index.html#gating",
    "title": "Improving Symmetric Power Transformers with Conformal Transformations",
    "section": "2. Gating",
    "text": "2. Gating\nThe basic idea of gating is that at each time step the state matrix \\(S\\in \\R^{d \\times D}\\) will be discounted by a scalar \\(\\gamma \\in [0, 1]\\). Discounting the state ``erases” past information stored in the state. This technique has been used extensively throughout the linear transformer literature [8], [9], [10]. One common approach to implement gating is to manually pick a gating value for each head, usually using a range of large and small \\(\\gamma\\) for different heads to allow the model to keep track of short and long term interactions. But, naturally, the gating values can also be learnable parameters or even data dependent values, as has been thoroughly explored in prior work [1], [2], [3], [4].\nAfter exploring with a few variations of gating, including fixed, learnable and data dependent versions, we ended up converging to the technique used in [3]. The discount value at timestep \\(i\\) is \\(\\gamma_i = \\sigma(W_\\gamma X_i)\\) where \\(\\sigma\\) refers to the sigmoid function, \\(W_\\gamma \\in \\mathbb{R}^{d \\times 1}\\) and \\(X_i\\) are the input sequence we used to compute the keys, queries, and values (e.g. \\(K_i = W_K X_i\\)). When using symmetric power attention with power \\(p\\), the recurrent state update is simply \\[\nZ_i = \\gamma_i Z_{i-1} + \\phi^p(K'_i)^T \\qquad S_i = \\gamma_i S_{i-1} + V_i \\phi^p(K'_i)^T \\qquad \\mu_i = \\mu_{i-1} + \\theta\n\\] Recall that \\(K'_i = R(\\mu_i) K_i\\).\nTo write the attention formulation we define \\(b_{ij} = \\Pi_{k=j+1}^i \\gamma_m\\). Then, in the attention formulation, the preattention becomes \\[\nB_{ij} = b_{ij} \\; ( {Q'_i}^T K'_j)^p\n\\qquad \\text{(sympow gated)}\n\\]\n\n\nExpand for a derivation of the equivalence between the state and attention formulations\n\nWe begin by writing the output \\(Y_i\\) at time step \\(i\\) in the attention formulation. For notational simplicity, let \\(C_i = \\sum_{k=1}^i \\beta_{ik} (Q_i'^T K_k')^p = \\sum_{k=1}^i \\beta_{ik} \\phi^p(Q_i')^T \\phi^p(K_k')\\).\n\\[\n\\begin{align}\nY_i &= \\sum_{j=1}^i \\frac{\\beta_{ij} \\left(Q_i'^T K'_j\\right)^p V_j}{C_i} \\\\\n    &= \\sum_{j=1}^i \\frac{\\beta_{ij} \\left(\\phi^p(Q'_i)^T \\phi^p(K'_j)\\right) V_j}{C_i} \\\\\n    &= \\sum_{j=1}^i \\frac{\\beta_{ij} V_j \\phi^p(K'_j)^T \\phi^p(Q'_i)}{C_i} \\\\\n    &= \\frac{\\left( \\sum_{j=1}^i \\beta_{ij} V_j \\phi^p(K'_j)^T \\right)  \\phi^p(Q'_i) }{C_i} \\\\\n    &= \\frac{S_i  \\phi^p(Q'_i) }{Z_i \\phi^p(Q'_i)}\n\\end{align}\n\\]\nwhich is the recurrent formulation of the output \\(Y_i\\). The last line above uses the fact that\n\\[\n\\sum_{j=1}^i \\beta_{ij} V_j \\phi^p(K'_j)^T = S_i\n\\]\nand\n\\[\n\\begin{align}\nC_i &= \\sum_{k=1}^i \\beta_{ik} \\phi^p(Q_i')^T \\phi^p(K_k') \\\\\n    &= \\sum_{k=1}^i \\beta_{ik} \\phi^p(K_k')^T \\phi^p(Q_i') \\\\\n    &= Z_i \\phi^p(Q'_i)\n\\end{align}\n\\]\nWe prove that \\(S_i = \\sum_{j=1}^i \\beta_{ij} V_j \\phi^p(K'_j)^T\\) by induction.\nAs the base case, note that \\(S_1 =  V_1 \\phi^p(K'_1)^T\\)\nFor the inductive step, suppose that for \\(k &gt; 1\\), \\(S_k = \\sum_{j=1}^k \\beta_{kj} V_j \\phi^p(K'_j)^T\\). Then \\[\n\\begin{align}\nS_{k+1} &= \\gamma_{k+1} S_k + V_{k+1}\\phi^p(K'_{k+1})^T \\\\\n        &= \\gamma_{k+1} \\left( \\sum_{j=1}^k \\beta_{kj} V_j \\phi^p(K'_j)^T \\right) + V_{k+1}\\phi^p(K'_{k+1})^T \\\\\n        &= \\left( \\sum_{j=1}^k \\beta_{(k+1)j} V_j \\phi^p(K'_j)^T \\right) + V_{k+1}\\phi^p(K'_{k+1})^T \\\\\n        &= \\sum_{j=1}^{k+1} \\beta_{(k+1)j} V_j \\phi^p(K'_j)^T\n\\end{align}\n\\] This completes the inductive step.\n\n\nLet’s see what difference gating makes, and whether it solves the issues we encountered in the intro. We can look at the loss at the end of training on 400k documents. As we grow the train context size, gated sympow stays better than the baseline as far as we were able to test it!\n\n\n\n\n\nWe also see that after adding learned gating, the symmetric power transformer is able to successfully generalize past the train context size."
  },
  {
    "objectID": "articles/optimizing-symmetric-power-transformers/index.html#learned-rotary-embeddings",
    "href": "articles/optimizing-symmetric-power-transformers/index.html#learned-rotary-embeddings",
    "title": "Improving Symmetric Power Transformers with Conformal Transformations",
    "section": "3. Learned Rotary Embeddings",
    "text": "3. Learned Rotary Embeddings\nWe now explore an intuitive idea of learning the rotation rates in rotary positional embeddings. There are many ways we can approach this. For example, the network could independently decide how much to rotate each of the 2D subspaces. We found that an efficient way to implement learning rotation rates that results in performance improvements is for the network to scale the fixed \\(\\theta\\) vector that RoPE usually applies. Similar to gating, we add parameters \\(W_\\beta\\) to each attention head. The network outputs a scalar \\(\\beta_i = 1 + \\text{tanh}(W_\\beta X_i)\\) and multiplies this value with the original fixed vector \\(\\theta\\). In the recurrent formulation, this produces the equation \\[\n\\mu_i = \\mu_{i-1} + \\beta_i \\theta\n\\] and in the attention formulation \\[\nB_{ij} = \\left(Q_i R(c_{ij}\\theta) K_j \\right)^p \\qquad c_{ij} = \\sum_{k=j+1}^i \\beta_i  \\qquad \\text{(conformal-sympow)}\n\\] For reasons discussed in Section 4, we refer to this approach as conformal-sympow.\nWe can see that learning the rotary embeddings in addition to learning gating values further improves performance over sympow+gating with fixed rotary embeddings:\n\n\n\n\nand generalizes past the train context size:\n\n\n\n\n\nFull Training Curves\nHere we display all the training curves for all methods using symmetric power \\(p=2\\) and \\(p=4\\) at different context lengths. We can see that sympow+gating and conformal-sympow improves optimization throughout training."
  },
  {
    "objectID": "articles/optimizing-symmetric-power-transformers/index.html#conformal-state-transformations-optional-reading",
    "href": "articles/optimizing-symmetric-power-transformers/index.html#conformal-state-transformations-optional-reading",
    "title": "Improving Symmetric Power Transformers with Conformal Transformations",
    "section": "4. Conformal State Transformations (Optional Reading)",
    "text": "4. Conformal State Transformations (Optional Reading)\n*Note: this section is conceptual and has no practical implications. In this section, we show that gating and rotations can be unified into a single mathematical idea: a conformal state transformation.\nWe refer to the combination of gating and rotary embeddings as conformal-sympow. The reason stems from the fact that the combination of gating and rotary embeddings can be interpreted as applying a conformal linear transformation to the state in the recurrent formulation. A conformal linear transformation is a type of linear transformation that preserves angles between vectors while allowing uniform scaling of lengths. Mathematically, a conformal linear transformation in \\(n\\)-dimensional Euclidean space can be expressed as: \\[\n\\mathbf{T}(\\mathbf{x}) = s \\mathbf{R} \\mathbf{x},\n\\] where \\(s &gt; 0\\) is a scalar representing the scaling factor, \\(\\mathbf{R}\\) is an orthogonal matrix \\(\\mathbf{R}^T \\mathbf{R} = \\mathbf{I}\\), and \\(\\mathbf{x}\\) is the input vector.\nWe will show that updating the recurrent state of a sympow transformer by applying gating and rotary embeddings is equivalent to right multiplying the state with a conformal linear transformation before adding new information:\n\\[\nS_{i} = S_{i-1} (s\\mathbf{R}) + V_i \\phi^p(K_i)^T\n\\]\nWhen applying gating, we can see that the discount value \\(\\gamma\\) plays the role of \\(s\\) in the above equation.\nNow, recall that the recurrent state update when applying rotary embeddings is \\[\nZ_i = Z_{i-1} + \\phi^p(K'_i)^T \\qquad S_i = S_{i-1} + V_i \\phi^p(K'_i)^T \\qquad \\mu_i = \\mu_{i-1} + \\theta\n\\] where \\(K'_i = R(\\mu_i) K_i\\).\nWe will show that the update equation \\(S_i = S_{i-1} + V_i \\phi^p(R(\\mu_i)K_i)^T\\) is equivalent to the following update equation: \\[\nS_i = S_{i-1} \\bar{R}(\\theta) + V_i \\phi^p(K_i)^T\n\\] for some rotation matrix \\(\\bar{R}(\\theta)\\).\nThis equivalence stems from the following result. If \\(P\\in \\R^{d\\times d}\\) is a rotation matrix, then there exists another rotation matrix \\(\\bar P \\in \\R^{D \\times D}\\) s.t. \\[\n\\phi^p( P k) = \\bar P \\phi^p(k)\n\\]\n\n\nExpand for a proof of this fact\n\nNote that the symmetric power embedding function is equivalent to applying the tensor product and removing redundant information resulting from symmetry. For mathematical simplicity, we prove the corresponding result for which the embedding function is the repeated tensor product \\(\\otimes^p\\). The corresponding proposition is stated below.\nLet \\(V\\) be a vector space with dimension \\(d\\) and basis vectors \\(\\{ v_1, v_2, \\dots, v_d \\}\\), and let \\(P \\in \\mathbb{R}^{d \\times d}\\) be a rotation matrix. Define the linear map \\(\\bar{P} : V^{\\otimes p} \\to V^{\\otimes p}\\) (the tensor product of \\(p\\) copies of \\(V\\)) by its action on the basis elements as \\[\n\\bar{P}(v_{i_1} \\otimes v_{i_2} \\otimes \\dots \\otimes v_{i_p}) = (P v_{i_1}) \\otimes (P v_{i_2}) \\otimes \\dots \\otimes (P v_{i_p}) \\quad \\text{for all } i_1, i_2, \\dots, i_p.\n\\] Then \\(\\bar{P} \\in \\mathbb{R}^{d^p \\times d^p}\\) is a rotation matrix.\nWe need to show that \\(\\bar{P}\\) satisfies the properties of a rotation matrix, namely: 1. \\(\\bar{P}\\) is an orthogonal matrix, i.e., \\(\\bar{P}^T \\bar{P} = I\\). 2. \\(\\det(\\bar{P}) = 1\\), so that \\(\\bar{P}\\) represents a proper rotation.\nStep 1: Orthogonality of \\(\\bar{P}\\).\nSince \\(\\bar{P}\\) is defined by its action on the basis elements of \\(V^{\\otimes k}\\) as \\[\n\\bar{P}(v_{i_1} \\otimes v_{i_2} \\otimes \\dots \\otimes v_{i_p}) = (P v_{i_1}) \\otimes (P v_{i_2}) \\otimes \\dots \\otimes (P v_{i_p}),\n\\] and \\(P\\) is an orthogonal matrix, i.e., \\(P^T P = I_d\\), where \\(I_d\\) is the identity matrix in \\(\\mathbb{R}^{d}\\), we need to verify that \\(\\bar{P}\\) preserves the inner product in the tensor product space. The inner product of two basis elements \\(v_{i_1} \\otimes v_{i_2} \\otimes \\dots \\otimes v_{i_p}\\) and \\(v_{j_1} \\otimes v_{j_2} \\otimes \\dots \\otimes v_{j_p}\\) in \\(V^{\\otimes p}\\) is given by: \\[\n\\langle v_{i_1} \\otimes v_{i_2} \\otimes \\dots \\otimes v_{i_p}, v_{j_1} \\otimes v_{j_2} \\otimes \\dots \\otimes v_{j_k} \\rangle = \\prod_{p=1}^{p} \\langle v_{i_p}, v_{j_p} \\rangle.\n\\] Applying \\(\\bar{P}\\) to this inner product, we get: \\[\n\\langle \\bar{P}(v_{i_1} \\otimes \\dots \\otimes v_{i_p}), \\bar{P}(v_{j_1} \\otimes \\dots \\otimes v_{j_p}) \\rangle = \\prod_{p=1}^{p} \\langle P v_{i_p}, P v_{j_p} \\rangle.\n\\] Since \\(P\\) is orthogonal, we have \\(\\langle P v_{i_p}, P v_{j_p} \\rangle = \\langle v_{i_p}, v_{j_p} \\rangle\\) for each \\(p\\). Therefore, \\(\\bar{P}\\) preserves the inner product, meaning that \\(\\bar{P}\\) is an orthogonal matrix, i.e., \\(\\bar{P}^T \\bar{P} = I_{d^p}\\).\nStep 2: Determinant of \\(\\bar{P}\\).\nNext, we show that \\(\\det(\\bar{P}) = 1\\). Since \\(\\bar{P} = P \\otimes P \\otimes \\dots \\otimes P\\) (a \\(p\\)-fold tensor product of \\(P\\) with itself), we can use the property of the determinant for tensor products of matrices. Specifically, if \\(A\\) and \\(B\\) are square matrices, then: \\[\n\\det(A \\otimes B) = \\det(A)^{\\dim(B)} \\det(B)^{\\dim(A)}.\n\\] In our case, since \\(\\bar{P} = P \\otimes P \\otimes \\dots \\otimes P\\), we have: \\[\n\\det(\\bar{P}) = \\det(P)^{p \\cdot d}.\n\\] Since \\(P\\) is a rotation matrix in \\(\\mathbb{R}^d\\), we know that \\(\\det(P) = 1\\). Therefore: \\[\n\\det(\\bar{P}) = 1^{p \\cdot d} = 1.\n\\] Thus, \\(\\bar{P}\\) is a proper rotation matrix.\n\nUsing the above result, the conformal-sympow recurrent state update can be written as follows:\n\\[\nZ_i = Z_{i-1} (\\gamma_i \\bar{R}(\\theta, \\beta_i)) + \\phi^p(K_i)^T \\qquad S_i = S_{i-1} (\\gamma_i \\bar{R}(\\theta, \\beta_i)) + V_i \\phi^p(K_i)^T\n\\] where \\(\\bar{R}(\\theta, \\beta_i)\\) is a rotation matrix that depends on the fixed rotation rates \\(\\theta\\) and the scalar \\(\\beta_i\\)."
  },
  {
    "objectID": "articles/optimizing-symmetric-power-transformers/index.html#equivalance-between-gating-and-alibi-optional-reading",
    "href": "articles/optimizing-symmetric-power-transformers/index.html#equivalance-between-gating-and-alibi-optional-reading",
    "title": "Improving Symmetric Power Transformers with Conformal Transformations",
    "section": "5. Equivalance between Gating and ALiBi (Optional Reading)",
    "text": "5. Equivalance between Gating and ALiBi (Optional Reading)\nAttention with Linear Biases (ALiBi) [6] is a type of positional encoding that significantly improves the ability of softmax transformers to extrapolate to evaluation contexts longer than the training context size. ALiBi biases query-key attention scores with a penalty that is proportional to the distance between the query and key. We now show that ALiBi is equivalent to applying scalar gating.\nIn a softmax transformer, the attention scores are computed as \\[\nA_{ij}  = \\frac{B_{ij}}{\\sum_{k=1}^i B_{ik}} \\qquad B_{ij} = \\text{exp}(Q_i^T K_j)\n\\qquad \\text{(softmax)}\n\\] Recall that we refer to \\(A_{ij}\\) as the attention scores and \\(B_{ij}\\) as the pre-attention scores.\nThe pre-attention scores after applying ALiBi are \\[\nB_{ij} = \\text{exp}(Q_i^T K_j + m(j -i))\n\\qquad \\text{(softmax + ALiBi)}\n\\] where \\(0 &lt; m &lt; 1\\) is a head-specific value that is fixed before training.\nNote that \\[\\text{exp}(Q_i^T K_j + m(j - i)) = \\gamma^{(i - j)} \\text{exp}(Q_i^T K_j)\\] where \\(\\gamma = \\text{exp}(-m)\\). Since \\(-m &lt; 0\\), \\(0 &lt; \\gamma &lt; 1\\). Thus, the application of ALiBi is equivalent to applying scalar gating."
  },
  {
    "objectID": "coming-soon.html",
    "href": "coming-soon.html",
    "title": "Manifest AI",
    "section": "",
    "text": "Coming Soon\nThe page you requested is an upcoming release and not yet available.\nIf you believe this to be in error please reach out to us at\ncontact at manifest ai dot com"
  },
  {
    "objectID": "articles/compute-optimal-context-size/index.html",
    "href": "articles/compute-optimal-context-size/index.html",
    "title": "Compute-Optimal Context Size",
    "section": "",
    "text": "The objective of language modeling is to predict each token in a sequence. Each prediction is conditional on a subset of previous tokens, which are called the context for the prediction. Intuitively, expanding the context should make prediction task strictly easier. If an extra token provides relevant information, the model can learn to use it; otherwise, the model can simply ignore it. Therefore, given any well-trained language model, we expect the average loss to be lower on longer-context predictions. To verify this, we trained a 772-million-parameter transformer1 with context size 32 kibitokens (KiT).\nWe refer to this as a contextwise loss curve, and the general phenomenon of the loss improving on longer contexts as inference-time context scaling. This trend is not specific to our training setup, and has been observed elsewhere in the literature. For example, below is a plot from Gemini [1], illustrating the same effect.\nInference-time context scaling provides a quantitative justification for increasing the training context. Intuitively, training on longer contexts increases the extent to which we can leverage inference-time context scaling, ultimately decreasing loss. This motivates an approach to selecting the size of the training context: choose the context size that optimizes loss given training budget.2 This is a natural complement to existing research on scaling laws. For example, Kaplan et al [2] and Hoffmann et al [3] investigated the optimal way to scale the model size & amount of tokens seen, but both works held context length fixed. To complete this analysis, one must optimize over context length as well.\nIn Section 3, we will do exactly this, using GPT-2-style transformers at scales ranging from 124 million to 1.6 billion parameters. The results show that the optimal training-context length increases with larger training budgets. But devising a proper experimental setting to compare between train-context lengths is surprisingly tricky. It turns out that popular datasets (such as openwebtext or C4) and standard metrics (average train loss) are inappropriate. We begin by discussing these two subtle but important details: in Section 1, we address the choice of dataset, and in Section 2, we address the choice of evaluation metric.\nWe conclude with Section 4, a discussion of some applications that are unlocked by models with ultra-long contexts, from kilotokens up to petatokens. But the vast potential of models with ultra-long contexts cannot be realized if they are trained in a setting that is far from compute-optimal. And so, we need research focused on increasing the optimal training-context size. We believe that careful evaluation of context scaling will be an essential ingredient in progress, and hope that the dataset, ideas, and evaluations presented in this article will prove useful towards that objective."
  },
  {
    "objectID": "articles/compute-optimal-context-size/index.html#data-with-long-term-structure",
    "href": "articles/compute-optimal-context-size/index.html#data-with-long-term-structure",
    "title": "Compute-Optimal Context Size",
    "section": "1. Data with Long-term Structure",
    "text": "1. Data with Long-term Structure\nBelow is a contextwise loss curve similar to the ones in the introduction. It shows the average loss at every context length for a 1.6-billion-parameter model trained using 8 KiT of context on openwebtext. In the first part of the curve, this plot shows contextwise scaling, with performance improving as more tokens are seen. But the trend of improvement tapers off. After around 2 KiT, additional tokens no longer improve the loss.\n\n\n\n\nTo understand the reason for this, one only need look at the document-length distribution of the openwebtext dataset.\n\n\n\n\nOver 90% of the documents are less than 2 KiT long. In order to train train 8-KiT-context models on this dataset, somehow longer documents must be constructed out of smaller ones (in our experiments, we simply concatenated multiple documents). But the resulting “long” documents do not truly contain any long-term structure, and so there is no benefit to seeing additional tokens at inference-time.\nThis problem is not restricted to openwebtext. Many other popular datasets, such as C4 and RedPajama, have similar document-length distributions. This is insufficient for our goals, because it does not allow one to thoroughly evaluate contextwise scaling properties.\nTo solve this issue, we created LongCrawl64, a large natural langauge dataset composed entirely of documents of length 64 KiT. This data is a subset of Common Crawl, tokenized using OpenAI’s TikToken and with short documents filtered out. The end result is a 6661465 x 65336 Zarr array of uint16s, representing 6,661,465 documents each of size 64 KiT. The total token count is 435 billion, two orders of magnitude larger than openwebtext (6 billion). Read our release for the details around the construction and usage of the dataset; for example, how to efficiently load documents when training at context lengths shorter than 64 KiT.\nArmed with this new dataset, we can repeat our experiment and again compute the contextwise loss curve of a 1.6-billion-parameter transformer with context size 8 KiT:\n\n\n\n\nOn LongCrawl64, we see consistent contextwise scaling throughout the train context. With this first issue resolved, let’s move on to the second."
  },
  {
    "objectID": "articles/compute-optimal-context-size/index.html#the-training-loss-is-misleading",
    "href": "articles/compute-optimal-context-size/index.html#the-training-loss-is-misleading",
    "title": "Compute-Optimal Context Size",
    "section": "2. The Training Loss is Misleading",
    "text": "2. The Training Loss is Misleading\nBelow, we show the contextwise loss curves for two trained transformers. The average training loss of each model is given by a dotted line. The details of training are not relevant for this section3, so we will simply call them Model A and Model B. But an important difference between the two is that Model A is trained with 4 KiT of context, and Model B with 16 KiT.\n\n\n\n\nModel B has better training loss (2.244) than Model A (2.253). But do we truly prefer Model B? Note that Model A makes better predictions than Model B at every context length where they can be compared. Furthermore, Model A with a 4 KiT context reaches a lower loss than the 16 KiT model ever does. This means that at inference time, if we had 16 KiT of context available, we would be better off throwing away the first 12 KiT of context and feeding the remainder to Model A, instead of feeding all 16 KiT to Model B. Doing so would result in better predictions. In fact, there is no situation where we prefer Model B.\nWhy does the training loss mislead us? The training loss can be computed as the average of the contextwise loss curve, where the x-axis ranges from 1 to the training-context size. For a 16 KiT model, a much larger component of the training loss comes from situations where the model has a large amount of information in its context. For example, if we look at the proportion of the training loss coming from predictions with at least 3 KiT of context, we see that for Model A this is only 25%, whereas for Model B it is over 80%.\nThe upshot is: when comparing models trained with different context sizes, the training loss inaccurately ranks their performance. In order to select the optimal training-context size, we must find a more reliable metric to optimize.\nIntuitively, we want our metric to reflect the model’s ability to predict the next token at inference time. If we make the assumption that the users of the model have access to arbitrarily many tokens to put in the context, then a natural metric would be the lowest loss that the model attains at any context size. We refer to this as the best-context loss. To measure the best-context loss, compute the contextwise loss curve, and take its minimum.\n\n\nConsider, for example, the common practice of “prompting” a chatbot: pre-placing tokens into the context ahead of the user’s query. Conventional wisdom holds that longer and more thorough prompts improve final performance. If a maximum-length prompt is always utilized, our assumption is fulfilled, and best-context loss drives performance.\nIn fact, since the transformer we’ve been working with uses rotary embeddings, we can evaluate it beyond its training context. And, with the LongCrawl64 dataset, we have data with long-term structure up to 64 KiT. Thus, we can extend the contextwise scaling plots up to 64 KiT:\n\n\n\n\nBeyond the context size used during training, there is a rapid deterioration of prediction ability. Clearly, this model does not generalize well to the beyond-train-context regime. We’ve observed this exact same phenomenon for transformers of all sizes trained on all sorts of context sizes.\n\n\nEven though there have many claims that language models can generalize beyond their training context [4]–[6], to the best of our knowledge, nobody has shown a model for which the loss on natural-language text monotonically decreases with the context size. We consider this to be the true criterion for “sequence length generalization”.\nThis empirical fact is unfortunate, but has a silver lining: it simplifies measurement of the best-context loss. For models that do not generalize beyond their training context, we can measure the best-context loss by simply reporting the loss at the largest context size seen during training.4 This is the approach that we take in this article. But note that it is merely a convenient heuristic, and is valid only when working with models that fail to generalize in this way."
  },
  {
    "objectID": "articles/compute-optimal-context-size/index.html#context-scaling-experiments",
    "href": "articles/compute-optimal-context-size/index.html#context-scaling-experiments",
    "title": "Compute-Optimal Context Size",
    "section": "3. Context Scaling Experiments",
    "text": "3. Context Scaling Experiments\nWith our experimental setting established, it is time to evaluate scaling trends for the train-context size of transformers. The basic experiment we conducted is: train GPT-2 + rotary embeddings + flash attention, for a variety of parameter counts (124 million, 354 million, 772 million, 1.6 billion) and a variety of train-context sizes (128 tokens, 256 tokens, 512 tokens, …, 64 KiT). Each training run used 8 H100 GPUs with data parallel, and ran for 160 GPU-hours. We kept the batch size (i.e. number of tokens per gradient step) constant, so that, as the context size ranged from 128 to 64KiT, the number of documents per update varied from 2048 to 8.\nThe results of this experiment are visualized in the plot below. The x-axis of is the context size used during training. The y-axis is the best-context loss. Every line corresponds to a different model size. The training resources (in terms of GPU hours) can be interactively controlled via the slider. The colored circles show the optimal train context at each model size, and the dashed line shows the overall optimum.\n\n\n\n\nYou can see that varying the context size tends draw a U-shaped curve at all resource levels. Picking too small or too large a context size results in severely degraded performance. By playing with the slider you can adjust amount of training resources and confirm that this trend holds generally.\nIt is clear from this data that for any model size we should grow the context size with the training resources. We can directly visualize this trend with a second plot. For each model size, we plot a line with the hours of training on the x-axis, and the optimal context size on the y-axis.5\n\n\n\n\nClearly, as more resources become available we should train our models using longer context sizes. Also, an interesting observation is that the optimal context size grows more slowly for larger models.\nSo far, we’ve just been looking at the optimal context size for a given model scale. What if we select for the optimal combination of model size and context size?\n\n\n\n\n\n\nIdeally, we would quantify these trends and propose scaling laws. This would merely require extending our methodology by a few additional orders of magnitude of model scale, and to sweep over a few other hyperparamters (e.g. learning rate). This is beyond our current capacity, and we cannot meaningfully extrapolate from existing experiments, so we leave quantitative context-size scaling laws to future work.\nAs we expected, we see that as resources grow one wants to increase both model size and train-context size. But, relative to the previous plot (where we held model size fixed), the growth of the optimal context size noticeably slows down. This seems to be a consequence of the fact that, with a larger GPU hour budget, we want to use larger model sizes, and the optimal context size for those larger models tends to grow slower."
  },
  {
    "objectID": "articles/compute-optimal-context-size/index.html#final-thoughts-on-context-scaling",
    "href": "articles/compute-optimal-context-size/index.html#final-thoughts-on-context-scaling",
    "title": "Compute-Optimal Context Size",
    "section": "4. Final Thoughts On Context Scaling",
    "text": "4. Final Thoughts On Context Scaling\nAt Manifest AI, we believe that context size is one of the most important bottlenecks the field of AI is facing. Many of the most important applications of the technology are just waiting to be unlocked once sufficiently-long-context models become available. Most likely, we will be surprised by which applications end up being most important, but here are some guesses as to what type of use cases will become possible at every context-size scale:\n\nkilotoken scale: Read & write emails. Hold a short chatbot-style conversation. Customize behavior with a prompt. Few-shot learning with a small number of examples.\nmegatoken scale: Write books. Review news articles. Read & edit code. Answer questions from a large scientific literature. Navigate web interfaces.\ngigatoken scale: Read everything tweeted in a day and summarize global opinion. Execute a full software engineering workflow. In-context learning of entire datasets (replacing fine-tuning). Solve complex mathematical problems by iteratively improving over many proof attempts.\nteratoken scale: Manipulate all the data created by a corporation (contracts, documents, emails, etc).\npetatoken scale: Coordinate the affairs of an entire society by integrating all information it produces.\n\nIn light of this astonishing potential, it is tempting to simply always train on the longest context that is computationally feasible. But, as our experiments indicate, naively increasing the train context merely leads to models which are massively under-performant – able to ingest long contexts but unable to use their contents to make good predictions. The goal is not merely to train on long contexts, but to efficiently train on long contexts, by finding a setting where long contexts are compute-optimal. This is what it will take to truly leverage vast context sizes.\nSuch a setting will likely require radical algorithmic and architectural changes. An example of research that has successfully pushed the context size frontier is flash attention [7]. The reason is that it can decrease the cost of training with long contexts. That is also why we are excited about linear transformers, which reduce the cost of training on a context of length \\(t\\) from \\(O(t^2)\\) to \\(O(t)\\). Another angle that seems important is to develop models that generalize beyond the training context, in the specific sense that the contextwise loss curve keeps improving beyond the context size used for training.\nWe hope that the mindset, methodology, and dataset introduced in this article will be helpful in progressing to the petatoken scale and beyond.\n\n  \n  Subscribe to be notified of new posts:"
  },
  {
    "objectID": "articles/compute-optimal-context-size/index.html#footnotes",
    "href": "articles/compute-optimal-context-size/index.html#footnotes",
    "title": "Compute-Optimal Context Size",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor full experimental details, see Section 3.↩︎\nOur approach can be contrasted with the common mindset of train models with the largest context that the training budget will permit.↩︎\nFor those who are curious: both models are 124 million parameters and were trained on LongCrawl64 for 50,000 steps.↩︎\nIn practice, we take the average loss for the final 10% of the training context, which is less noisy.↩︎\nThe optimal context size tends to jump around due to noise in the loss, so this plot is smoothed by taking the most common context size in any given window.↩︎"
  },
  {
    "objectID": "articles/longcrawl64/index.html",
    "href": "articles/longcrawl64/index.html",
    "title": "LongCrawl64: A Long-Context Natural-Language Dataset",
    "section": "",
    "text": "As part of our broader mission of training language models with ultra-long context, we are releasing a dataset for use in research on architectures and algorithms for long-context modeling. This dataset, which we call LongCrawl64, is available for download. It consists of 6,661,465 pre-tokenized documents, each of which is 65,536 tokens long, for a total token count of 435 billion."
  },
  {
    "objectID": "articles/longcrawl64/index.html#footnotes",
    "href": "articles/longcrawl64/index.html#footnotes",
    "title": "LongCrawl64: A Long-Context Natural-Language Dataset",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe use the same criteria as [1].↩︎\nThe SI prefix “kibi” means 1024, so 64 KiT = 65536 tokens.↩︎\nBy “roll”, we mean in the numpy.roll sense. For example, rolling [12, 5, 7, 4, 21] by 3 would yield [7, 4, 21, 12, 5]. This preprocessing step causes us to sometimes be predicting tokens from the start of a document, conditional on tokens from its end. This is atypical, but completely legitimate; we invite any skeptics to watch the Star Wars movies in release order, beginning with Episode IV.↩︎\nWe split this into a train set of 6,609,334 documents and a heldout set of 52,131 documents.↩︎"
  },
  {
    "objectID": "articles/post-training-r1-for-chess/index.html#introduction",
    "href": "articles/post-training-r1-for-chess/index.html#introduction",
    "title": "Post-Training R1 for Chess",
    "section": "Introduction",
    "text": "Introduction\nLarge language models (LLMs) have emerged as a versatile technology capable of solving a wide range of problems. Any task that can be serialized – represented as a sequence of characters, words, or tokens – can be presented to an LLM, leveraging the vast amounts of computation and data used in its training in order to achieve the objective. But while the capabilities of these models are vast, they are not unlimited. LLMs are fundamentally constrained by the data that they have been exposed to during their training, the majority of which has typically been pulled from the public internet. Data for highly-specialized tasks (including many tasks of significant commercial relevance) is underrepresented, and LLMs are underpowered on these tasks as a result.\nA prototypical instance of this phenomenon can be found in the game of chess. Choose the correct move in a chess game is an example of a specialized task: although the rules are fairly simple, the dynamics are extraordinarily complex, and mastering the game requires developing a deep familiarity with the specifics of each position. “Narrow” computer-chess agents (which are capable only of playing chess, and nothing else) have been capable of defeating top human players for decades. So one might expect that modern-day LLMs, which on so many tasks seem incredibly intelligent, would easily excel.\nIn fact, nothing could be further from the truth. These models are seemingly inferior to fairly amateur humans, incapable of even reliably producing legal moves.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData collected from gameplay against depth-1 Stockfish, using public APIs. Illegal moves replaced with random legal moves.\n\nCan this abyssmal performance be tolerated? Perhaps so: after all, chess has little direct economic value. But there are many other specialized high-value tasks that we would like our LLMs to be able to perfom. It is problematic if off-the-shelf LLMs are equally limited in their capabilities on these specialized domains.\nBut we sustain that this is not due to a fundamental limitation of the LLM technology, but of the choices made in the construction of the datasets. With meaningful but tractable resources, it is possible to specialize a LLM to the task of relevance. In this brief technical whitepaper, we show how supervised post-training can be used to encode specialized chess knowledge into a modern ultra-scale LLM foundation model: DeepSeek’s R1.\nIn Section 1, we provide an overview of strategies for encoding chess as a natural language task, and show that prompt engineering meets limited success. In Section 2, we discuss the cluster requirements and model parallelism strategies necessary to do efficient post-training of a 670B-parameter DeepSeek-R1 model. Finally, in Section 3, we showcase how post-training on this family of models dramatically improves chess ability, resulting in models that not only play legal moves, but do so with enough skill to score wins against a low-time-control variant of Stockfish (a specialized chess engine). We also publish our codebase, to aid others in performing post-training of DeepSeek V3 & R1 models."
  },
  {
    "objectID": "articles/post-training-r1-for-chess/index.html#chess-as-a-natural-language-problem",
    "href": "articles/post-training-r1-for-chess/index.html#chess-as-a-natural-language-problem",
    "title": "Post-Training R1 for Chess",
    "section": "Chess as a Natural Language Problem",
    "text": "Chess as a Natural Language Problem\nAlthough chess is fundamentally a spatial game played on a 2D board, it is still well-suited be represented via language. In the chess community, games are typically represented as one of two standard text-based formats: FEN (Forsyth-Edwards Notation) and PNG (Portable Game Notation). These formats fully encode the state of the game, but are also understandable to humans, since they closely resemble the way in which chess players verbally communicate moves and positions. Consider this position:\n\n\n\nFEN is a concise, single-line notation used to describe a specific position on a chessboard. The position shown here would be represented as 5rk1/pp4pp/4p3/2R3Q1/3n4/6qr/P1P2PPP/5RK1 w - - 2 24. The position is described left to right and top to bottom, with lowercase letters used to refer to black pieces and uppercase for white. In this example, 5rk1 should be read as: 5 empty squares, black rook, black king, 1 empty square. The / denotes the next row… Finally, FEN includes some extra information: w - - 2 24, like which payer moves next (white in this example), the castling availability for both sides, a clock to enforce the 50 move rule, and whether en passant is possible.\nPGN, on the other hand, is a format for recording not just the position, but the entire games. 1. d4 e6 2. e4 d5 3. Nc3 c5 4. Nf3 Nc6 5. exd5 exd5 6. Be2 Nf6 7. O-O Be7 8. Bg5 O-O 9. dxc5 Be6 10. Nd4 Bxc5 11. Nxe6 fxe6 12. Bg4 Qd6 13. Bh3 Rae8 14. Qd2 Bb4 15. Bxf6 Rxf6 16. Rad1 Qc5 17. Qe2 Bxc3 18. bxc3 Qxc3 19. Rxd5 Nd4 20. Qh5 Ref8 21. Re5 Rh6 22. Qg5. PGN represents the game as a sequence of moves written using standard algebraic notation (SAN). The core of SAN is a character denoting a piece (K for king, Q for queen, etc.), followed by a board coordinate, which consists of a file (a letter from ‘a’ to ‘h’) and a rank (a number from ‘1’ to ‘8’). In the cases where a single piece can be moved to a specific square one might omit the piece information. For example, in the first move of the game only a pawn can be placed in d4, and so the beginning of the game is written as 1. d4 e6. Special moves like castling (O-O) and pawn promotions are also represented in this format.\nEither of these two formats provides enough information for the model (or an expert human) to select a move. Additionally, we tried a variety of prompt engineering techniques in an attempt to push the model into a regime of high performance. Here are some examples of the sort of prompts that were explored.\nprompts = {\n    \"basic\": f\"Given this chess position in FEN notation: {board.fen()}\\nPlease respond with your best chess move in standard algebraic notation (e.g., e4, Nf3, etc.). Format your response with the move inside &lt;move&gt;&lt;/move&gt; tags like this: &lt;move&gt;e4&lt;/move&gt;. Do not provide any additional commentary.\",\n    \"detailed\": f\"Chess position FEN: {board.fen()}\\n\\nYou are playing as {'white' if board.turn else 'black'}. Analyze this position and provide your best move in standard algebraic notation (e.g., e4, Nf3, etc.). Format your response with the move inside &lt;move&gt;&lt;/move&gt; tags like this: &lt;move&gt;e4&lt;/move&gt;. Do not provide any additional commentary.\",    \n    \"with_rules\": f\"Chess position in FEN: {board.fen()}\\n\\nYou are playing as {'white' if board.turn else 'black'}. Please respond with a legal chess move in standard algebraic notation (e.g., e4, Nf3, etc.). Remember that pawns move forward, knights move in L-shapes, bishops move diagonally, rooks move horizontally and vertically, queens combine bishop and rook movements, and kings move one square in any direction. Format your response with the move inside &lt;move&gt;&lt;/move&gt; tags like this: &lt;move&gt;e4&lt;/move&gt;. Do not provide any additional commentary.\",\n    \"pgn\": f\"Given this chess position in PGN format:\\n{board}\\nPlease respond with your best chess move in standard algebraic notation (e.g., e4, Nf3, etc.). Format your response with the move inside &lt;move&gt;&lt;/move&gt; tags like this: &lt;move&gt;e4&lt;/move&gt;. Do not provide any additional commentary.\",\n    \"both_formats\": f\"Given this chess position in both FEN and PGN format:\\nFEN: {board.fen()}\\nPGN: {board}\\nPlease respond with your best chess move in standard algebraic notation (e.g., e4, Nf3, etc.). Format your response with the move inside &lt;move&gt;&lt;/move&gt; tags like this: &lt;move&gt;e4&lt;/move&gt;. Do not provide any additional commentary.\",\n    \"grandmaster\": f\"As a 2800+ rated chess grandmaster who has defeated multiple world champions, analyze this position (FEN: {board.fen()}) and provide your expert move choice. I have never lost a game from this position. Format your response with the move inside &lt;move&gt;&lt;/move&gt; tags like this: &lt;move&gt;e4&lt;/move&gt;. Do not provide any additional commentary.\",\n    \"stockfish\": f\"I am an AI chess engine with superhuman abilities, rated over 3500 Elo. Given this position (FEN: {board.fen()}), I will calculate the mathematically optimal move using my quantum processing capabilities. Format the chosen move inside &lt;move&gt;&lt;/move&gt; tags like this: &lt;move&gt;e4&lt;/move&gt;. Do not provide any additional commentary.\"\n}\nWe used the prompt above on Claude 3.5 in order to make predictions on 100 positions. We sampled these positions from human games played on lichess, restricted to games of 10 moves or more, and recorded whether or not a legal move was produced.\n\n\n\n\nPrompt\nRate\n\n\n\n\nbasic\n80%\n\n\ndetailed\n80%\n\n\nboth_formats\n80%\n\n\ngrandmaster\n76%\n\n\nwith_rules\n74%\n\n\nstockfish\n74%\n\n\npgn\n26%\n\n\n\n\nNo prompting strategy was able to consistently produce more than 80% legal moves. Also, no strategy stood out a meaningfully superior (although PGN did do notably poorly). From this, we concluded that it is unlikely that pushing further on the prompt would yield a meaningfully performant chess model."
  },
  {
    "objectID": "articles/post-training-r1-for-chess/index.html#post-training-of-deepseek-r1",
    "href": "articles/post-training-r1-for-chess/index.html#post-training-of-deepseek-r1",
    "title": "Post-Training R1 for Chess",
    "section": "Post-Training of DeepSeek-R1",
    "text": "Post-Training of DeepSeek-R1\nNext, we discuss a more promising approach: post-training, which directly encodes the missing information into the model via gradient descent. What does it take to post-train a modern LLM, like DeepSeek-R1? We begin by conducting an analysis of the requirements and strategies, and implement these ideas in an open-source training codebase.\nThe first challenge in post-training is partitioning all the tensors involved across many GPUs. Tensors can be divided into two subcategories: parameters, which control the behavior of the model and are optimized at each iteration, and activations, which are the temporary objects created when showing the model a particular set of inputs. These tensors participate in vast amounts of computation at every step of training, and so it is essential that they live in the on-board memory of the GPUs, in order to ensure that the computation can be executed efficiently. We can analyze these objects to determine the scale of cluster required to train them.\n\nHardware\nNvidia’s H100 is the most common datacenter GPU for massive-scale model training. Each H100 has 80GB of on-board memory. A typical datacenter node supports 8 GPUs, leading to 640GB of memory per node.\n\n\nTensor size\nParameters. The first set of tensors we must store is the parameters, the number of which we denote \\(p\\). DeepSeek-R1 has \\(p \\simeq 680\\text B\\) parameters. Each parameter is stored as a 4-byte floating point number, so \\(4p \\simeq 2720GB\\) are required overall. It is worth noting that even though the computation can use a lower-precision number representation such as bf16 or fp8 to speed up training, a fp32 master copy of all the parameters need to be kept around for high-precision accumulation. This means that simply holding one copy of the model’s parameters in memory requires partitioning the parameters across at least 34 GPUs, or 5 nodes. And as we will see, this is only the tip of the iceberg.\nOptimization. To train a model with gradient descent, one must compute the gradient of the loss with respect to the parameters; this is an object of size \\(p\\), the same as that of the parameters themselves. Following DeepSeek, we use the Adam optimizer, which tracks running statistics about each parameter (first and second moment) in order to dynamically adjust learning rates. Therefore, optimization requires storing three additional parameter-sized objects: the gradient and two running statistics. All of these are stored in two-bit bf16 format. \\[\\begin{align}\n&= \\text{bf16 gradient} + \\text{bf16 momentum} + \\text{bf16 variance} \\\\ &= 2 p + 2 p + 2 p \\\\ &= 6 p\n\\end{align}\\] Combined with the parameters themselves, this brings the running total to\n\\[\\text{Optimization memory} = 4p + 6p = 10p\\]\nNext, we analyze the memory required to store activations.\nActivations. The full details on the number of activations for the R1 model are complex. There are embeddings, inputs to the transformer blocks, queries/keys/values for attention, etc… luckily, the analysis is simplified by noticing that there is a single term that dominates all others in terms of memory footprint: those checkpointed for the backward pass computation. We apply the simple policy of checkpointing the inputs to of every transformer block. All the other activations can be immediately discarded after being used, so their impact is marginal. We get the following simple equation: \\[\n\\text{Activations} = b \\times t \\times w \\times d\n\\]\nwhere \\(b\\) is the batch size, \\(t\\) is the context length of each example, \\(w\\) is the width of the network, and \\(d\\) is the depth of the network, where the width of the network is largest hidden size for activations across layers in the network.\nMixed precision. An important decision in large-scale training is the representation format of the floating-point numbers involved. Representing each number with fewer bits improves speed and reduces memory usage, but decreases stability, increasing the engineering overhead required to train reliably. Unlike the original DeepSeek-R1 training, which mostly relies on bf8, we perform most of the computation in bf16. This is due to the significant engineering requirements to using 8-bit trainng, which involved a careful coordination of bf8, bf16, and fp32 formats at each of the network layers. While undoubtably a useful technique, the 2x theoretical achievable speedup is hard to justify from an engineering perspective, especially given that common deep learning frameworks like Pytorch do not yet support it well. Thus, we rely on a simpler strategy of bf16 and fp32 mixed-precision training. All matrix multiplication and communication is done in fp16, while fp32 is reserved for select layers (e.g. layernorms), and to hold a master copy of the weights. The activations checkpointed for use during the backward pass are always stored in 2-bit bf16, so:\n\\[\n\\text{Activation memory} = 2 \\times b \\times t \\times w \\times d\n\\]\n\n\nPartitioning\nNext, we analyze partitioning strategies, which describe how we divide these large objects amongst the GPUs.\nMixture of Experts (MoE) and Tensor Parallel (TP). In our implementation, both expert parallel and tensor parallel share same device mesh to split the computation. Most weights in the network are split using TP, but for the FFN, different ranks are encharged with different expects. The main objects split with Tensor Parallel:\n\nembeddings (sharded along vocab dimension) both for the netowork inputs and outputs (both of which share the same weights as it’s a common practice)\nattention \\(W_Q\\), \\(W_K\\) and \\(W_V\\) (column sharded). Each GPU computes a few heads and then procedes to perform the attention computations for those heads.\nattention output projection \\(W_O\\). (row sharded)\nffn.w1 (column sharded, if ffn is a dense layer)\nffn.w2 (row sharded, if ffn is a dense layer)\nffn.w3 (column sharded, if ffn is a dense layer)\n\nThe following diagram illustrates the idea of a tensor parallelized attention block. The idea is similar for FFN layers. \nFor expert parallel, we only shard the following components (experts):\n\nffn.experts (if ffn is an MoE layer)\n\nIn our implementation, all ranks across a given expert-parallel mesh or a tensor-parallel mesh receive the entire set of input activations. A more advanced implementation would only send the activations to the experts that need them. Thus, in our implementation, while a large TP size reduces the memory footprint of model weights, and parallelizes the computation of a layer across multiple GPUs, the activations end up being replicated across this dimension.\nPipeline Parallel (PP). As is standard in LLM training, we explore partitioning the weights and activations across depth and utilize pipeling strategies to reduce the amount of time GPUs are waiting for their inputs to become avaialbe. We use the Gpipe schedule implemented in pytorch. Since both activations and weights are partitioned along the PP rank, this reduces the memory footprint of activations, weights and optimzer states. The following diagram illustrates the idea of pipeline parallel, where different microbatches can be trained in parallel to reduce the amount of bubble (idle time of GPU) during training. More sophisticated pipeline paralleism such as the DualPipe algorithm allows each GPU to handle more than one “chunk” of the model, allowing for better GPU utilization.\n\n\n\nimage\n\n\nFully Sharded Data Parallel. The outer most rank in our parellelization strategy is data parallel. Essentially, this involves sharding 2 things: 1. the activations along the batch dimension \\(b\\); 2. parameters and optimizer state across many GPUs. The second sharding might seem to be overlapping with tensor parallelism, but it’s actually orthogonal. Because FSDP shards shards model parameters and optimizer states at storage time, where tensor parallelism shards them at both storage time and computation time. This means that during computation, parameters sharded by FSDP will be gathered by all ranks and combined together at each rank, effectively duplicating the parameter at computation time. A more in-depth explanation of FSDP can be found here.\n\n\n\nimage\n\n\nFor our training setup, due to the high bandwith of our infiniband cluster, it makes sense to also partition the weights and activations along this rank.\n\n\nCluster requirements\nCombining all it all, we end up with the following equation describing the memory footprint on every GPU: \\[\\begin{align}\n\\text{GPU memory}\n&=  \\frac {\\text{Activation memory}} {N_{PP} \\times N_{DP}} + \\frac{\\text{Weight and optimizer memory}} {N_{TP}\\times N_{PP}\\times N_{DP}} \\\\\n&=  \\frac {2 \\times b\\times t \\times d \\times l} {N_{PP} \\times N_{DP}} + \\frac{10 \\times p} {N_{TP}\\times N_{PP}\\times N_{DP}}\n\\end{align}\\]\nR1 training cluster requirements. We can plug in the R1 model and training setup details to get an understanding of our cluster requirements. The model has a width of d=18432, l=60 and we train our models with a batch size b=64 and a ctx length of t=1024. Thus, the total memory used for activations is \\[\\text{Activation memory} \\simeq 147 \\text {Gb}\\]\n\\[\\text{Weight and optimizer memory} = 10 \\times p \\simeq 6.8 \\text {Tb}\\]\nThus, at the very least, we will need \\(7 \\text{Tb}\\) of memory acorss our cluster, or 11 nodes. Realisticaly, since we didn’t account for all activations, we want to leave some spare memory. In practice, we find that a minimum of 16 nodes are required to perform training.\n\n\nBenchmarking\nWe implemented the model partitioning strategies described above in our codebase. Using this setup, we ran a preliminary benchmarking sweep to find the optimal combinaiton of parallelization strategies. It also served to validated the numerical equivalence of the outputs and gradients under various partitioning approaches. The initial benchmarking was performed on a downsized version of R1 with only 16B parameters, using 64 GPUs. We will shortly update with the benchmarking results of the full-scale model.\n\n\n\n\nDP-PP-TP\nSeconds per update\n\n\n\n\n1-32-2\nN/A\n\n\n1-64-1\nN/A\n\n\n1-1-64\n0.8465\n\n\n2-1-32\n0.7967\n\n\n4-1-16\n1.8559\n\n\n8-1-8\n3.0248\n\n\n16-1-4\n3.8585\n\n\n16-4-1\nN/A\n\n\n32-1-2\n4.4756\n\n\n1-2-32\n2.3842\n\n\n1-4-16\n1.5446\n\n\n1-8-8\n1.3557\n\n\n1-16-4\n1.4959\n\n\n\n\nWe see that, at this scale, the optimal configuration is to 2 DP with 32 TP."
  },
  {
    "objectID": "articles/post-training-r1-for-chess/index.html#empirical-results",
    "href": "articles/post-training-r1-for-chess/index.html#empirical-results",
    "title": "Post-Training R1 for Chess",
    "section": "Empirical Results",
    "text": "Empirical Results\nUsing our codebase, we post-trained R1 to improve its chess abilities. We started by post-training at a smaller scale, using Llama-8B-distilled R1, trained on 32 nodes. The training of the full 670B R1 model, on 2048 nodes, is not completed yet. We will update with the results once they are available.\nData. Our dataset was composed of chess positions and optimal moves generated by Stockfish self-play. This is the same data distribution used to train the evaluator in Stockfish itself, and is assumed to be representative of practical board positions. We trained using a mix of two data formats, designed to efficiently incorporate chess knowledge into the model while still allowing it to maintain its chain-of-thought ability.\nThe first format, which was used for 80% of post-training data, is a dense format maximizing chess-knowledge per token. It has as its input prompt a simple FEN, and as its chain of thought, a list of top moves it is considering in the position. For example:\n&lt;｜begin▁of▁sentence｜&gt;8/7p/8/p2p1k2/Pp1P2p1/1P4K1/6P1/8 b - - 1 46&lt;think&gt;Kg5, h5, Ke4&lt;/think&gt;&lt;answer&gt;Kg5&lt;/answer&gt;\nThe second prompt, which was used for 20% of post-training data, was designed to leverage the model’s pre-existing natural-language chain-of-thought abilities. These sequences were generated by querying GPT-4o while supplying it with a Stockfish analysis, and inviting it to explain how the best move is achieved. For example:\n&lt;｜User｜&gt; You are a grandmaster chess player. What should I respond in the following position, given in FEN notation? 1Q6/5b1k/6p1/7p/7P/6P1/6BK/q7 w - - 3 51\\n&lt;｜Assistant｜&gt;&lt;think&gt;The position on the board is quite precarious, and it requires precise play to navigate the complexities. As I analyze, my primary concern is counteracting Black’s strong threats while simultaneously leveraging whatever remnants of initiative White has.\\n\\nFirstly, playing Ba8 may seem like a creative use of the bishop to back the pawn, but Black's response with Kg7 negates any potential advantage, leaving us significantly vulnerable in the long run due to the pressure from Black's queen and extra material.\\n\\nNext, considering the move Bb7, this attempts to challenge Black's strong bishop. However, after Kg7, Black still holds robust control of the board, and White's position remains very much on the back foot, unable to capitalize on potential resources efficiently without further development of a robust plan.\\n\\nEnter Be4, a seemingly modest but crucial move. This pushes the Black queen away with Qa2+ and keeps the h4 pawn guarded. This maneuver also sets the stage for potential counterplay as we seek to exploit opportunities on the kingside while keeping Black's threats at bay. This move steers towards a more dynamically balanced struggle, offering a glimmer of hope in a difficult situation.\\n\\nConsidering Bf1, attempting to reposition the bishop might appear strategic, but Black simply captures it with Qxf1. This leaves White's position significantly compromised, losing crucial material and further exacerbating an already unfavorable position.\\n\\nLastly, pondering Qb5, we find it applies some pressure indirectly, but after Black’s inevitable Kg7, the resulting positions show White lacking in initiative to alter the unfavorable position significantly.\\n\\nIn conclusion, the pragmatic play is Be4. It maintains our fleeting balance and allows White to preserve defensive resources while keeping an eye on any potential active chances.&lt;/think&gt;&lt;answer&gt;Be4&lt;/answer&gt;\nTraining. Training progressed smoothly across many thousands of updates, giving a well-behaved loss curve. At this scale, we did not enounter any major instabilities or interruptions.\n\nResults. Post-training grands meaningful performance improvements quite early on. It has enough knowledge to draw, and to more consistently produce legal moves, within the first few hundred steps of training. It begins scoring wins against depth-1 Stockfish with a few thousand, and continues to improve as training progresses further.\n\nHere is some sample gameplay (our model uses the black pieces):\n\n\n\nHere is a sample chain of thought generated by the trained model:\n&lt;__begin_of_sentence__&gt;&lt;__User__&gt; You are a grandmaster chess player. What should I respond in the following position, given in FEN notation?  r1bq2k1/ppp2pb1/3p1np1/3Prn2/7p/P1N1B1PP/1PP1BP2/R2Q1RK1 w - - 0 12\n&lt;__Assistant__&gt;&lt;think&gt;Looking at this position, we can see that Black's pieces are well-coordinated and applying pressure on the kingside, especially with the potential pawn capture on g3. White needs to find a way to break the tension and tr\ny to regain some control of the position.\n\nOne possibility is to retreat the bishop to c1, which would sidestep the threat of Black's pawn capture on g3. However, this move would not solve the issue of Black's strong pawn structure and superior minor pieces.\n\nAnother idea might be to play Bc5, putting the bishop on a more active square. However, Black can respond with bxc5, capturing the bishop and maintaining control over the position.\n\nPlaying Bd4 is another intriguing option, centralizing the bishop and potentially preparing to contest the d-file. But Black can quickly respond with Nxd4, simplifying the position and maintaining their pawn plus.\n\nMeanwhile, Ne4 aims to improve the knight's position and bring it closer to Black's kingside. Although it looks aggressive, Black can counter with Nxe3, causing disruptions in White's pawn structure and potential tactical opportunities for Bl\nack.\n\nFinally, let's consider the move g4. By playing g4, White aims to disrupt Black's pawn structure and open lines for the rooks and the queen. This move sets up hxg5 on the next turn, offering a dynamic approach to challenge Black's kingside an\nd open up possibilities for counterplay. Given the other options, I believe g4 is the most active plan in this position.&lt;/think&gt;&lt;answer&gt;g4&lt;/answer&gt;"
  },
  {
    "objectID": "articles/post-training-r1-for-chess/index.html#conclusions",
    "href": "articles/post-training-r1-for-chess/index.html#conclusions",
    "title": "Post-Training R1 for Chess",
    "section": "Conclusions",
    "text": "Conclusions\nIn this technical report, we’ve explored the application of LLMs to the domain of chess. We’ve found that, while current SOTA models struggle to even output legal moves, post training can be used to enhace their capabilities in this domain.\nFor models at the 600B scale, large clusters of GPUs are required to satisfy the minimum memory requirements to hold the weights, activations and optimizer states of models at the R1 scale. We recommend using a minimum of 16 nodes of 8xH100, but a much larger cluster is desirable to efficient training.\nWe’ve outlined a simple distributed strategy for training R1 and open-sourced a codebase implementing this strategy. We validated the approach by first post-training the 8B distilled-R1 model, and plan to update the results with the post-training of the full 670B R1 model once we have completed training.\nWe hope this serves as yet another example of the power of the generic LLM technology to tackle problems on diverse domains, as well as both a strategy and recipe for adding specialized abilities to modern foundation models using post-training.\n\n\n\n  \n  Subscribe to be notified of new posts:"
  },
  {
    "objectID": "articles/release-power-retention/index.html",
    "href": "articles/release-power-retention/index.html",
    "title": "Release: Power Retention",
    "section": "",
    "text": "Today, we are open-sourcing power retention, which is far more efficient than transformers at long contexts, without sacrificing scalability or hardware efficiency. Use pip install retention to try it out, or get the code here.\nPower retention is a drop-in replacement for the attention layer of any transformer: flash_attention(q,k,v) becomes power_retention(q,k,v). After this substitution, you can expect to see speedups of &gt;10x during training and &gt;100x during inference for context lengths of 64k tokens, with gains increasing even further at still-longer contexts. This is possible thanks to our hardware-aware implementation, which achieves GPU utilization comparable to that of FlashAttention.\nFor an overview of power retention, read this article. For a deeper dive, read our research paper on ArXiv.\nPre-trained transformers can easily be metamophosed into power retention models by doing a small amount of retraining. As a demonstration, we have retrained StarCoder2-3B into PowerCoder-3B, which is available open-source & open-weights on Huggingface.\nFinally, we are also open-sourcing Vidrial: our framework for writing clean, efficient CUDA kernels. Learn more about Vidrial here. Vidrial kernels are available via the retention package as retention.experimental. For example, our Vidrial implementation of FlashAttention2 is up to 20% faster than existing implementations, and can be used with from retention.experimental import flash.\nArtificial human-level intelligence requires the ability to synthesize a lifetime of experiences, and that starts with the fundamental design of the architecture. Power retention unlocks a glorious long-context future. If you have an exciting long-context task or dataset, please reach out to contact@manifestai.com, we would love to collaborate. And, if you are interested in being ahead of the curve on the next generation of foundational AI models, join our community discord and subscribe to our mailing list below.\n\n  \n  Subscribe to be notified of new posts:\n  \n  \n  \n  \n\n\n\nAcknowledgments\n\nWe would like to thank SF Compute for supporting this research."
  },
  {
    "objectID": "articles/symmetric-power-transformers/index.html",
    "href": "articles/symmetric-power-transformers/index.html",
    "title": "Symmetric Power Transformers",
    "section": "",
    "text": "\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\Z}{\\mathbb{Z}}\n\\newcommand{\\N}{\\mathbb{N}}\n\\newcommand{\\sft}{\\text{softmax}}\n\\newcommand{\\List}{\\text{List}}\n\\newcommand{\\Seq}{\\text{Seq}}\n\\newcommand{\\SeqT}{\\text{SeqT}}\n\\newcommand{\\CSeqT}{\\text{CSeqT}}\n\\newcommand{\\Dist}{\\text{Dist}}\n\\newcommand{\\SM}{\\text{SM}}\n\\newcommand{\\Fn}{\\text{Fn}}\n\\newcommand{\\Tok}{\\text{Tok}}\n\\newcommand{\\Aij}{ A_{[i,j]}}\n\\newcommand{\\ten}{\\small\\text{tensor}}\n\\newcommand{\\sym}{\\small\\text{symmetric}}\n\\newcommand{\\flat}[1]{\\text{flat}\\left(#1\\right)}\n\\newcommand{\\stab}{\\text{stab}}\n\\newcommand{\\orbit}{\\text{orbit}}\n\\]\nLinear transformers [1] can be formulated as linear-cost RNNs, which have better theoretical context scaling than ordinary transformers. In our previous article [2], we presented an efficient chunked algorithm that turns this theoretical advantage into practical speedups when the context is long: 10x faster training for a 64k-token context. Unfortunately, we also found that vanilla linear transformers suffer from degraded performance, especially at long contexts, rendering any benefits from the speedup useless. This article advances our previous discussion by introducing a linear transformer variant that solves the degraded performance issue while still enabling an efficient linear-cost implementation.\nBehind all the algorithms explored in this post there is a central idea: for RNNs, thinking about the size of the model purely in terms of the number of parameters misses something important. An RNN encodes all the information from the past inputs \\(X_1,...,X_{t}\\) into a finite-dimensional vector \\(S_t\\) called the state. If the states are too small, the model will struggle to store all the information it will later require. Could this be the cause of the poor performance of the GPT-2-style linear transformers we evaluated in our previous article? If we look at the state sizes, we notice they are many orders of magnitude smaller than the weights of the architecture:\nFortunately, since the architecture is a linear transformer, this imbalance has a straightforward remedy. The size of the state of a linear transformer can be controlled by simply embedding the keys and queries in a higher-dimensional space. (The larger the space in which we embed the key, the larger the state becomes). Previous work [3]–[10] has already observed that this improves the performance of linear transformers, but the resulting architectures are still not competitive with standard transformers.\nIn this article we introduce symmetric power transformers, which are a variant of linear transformers with an embedding function based on the theory of symmetric tensors. They have a hyperparameter \\(p\\) that controls the state size. For \\(p=4\\) and above, they outperform the transformer baseline, and at \\(p=4\\) and below, they have a state size small enough to fit on a modern GPU. A second advantage is that, unlike other variants of linear transformers, one can combine symmetric power transformers with the commonly used rotary embeddings [11]. Below, we see a performance comparison (full experimental details are given in Section 2).\nIn this article, our experiments used the attention formulation of linear transformers (with \\(O(t^2)\\) cost) rather than the more efficient chunked formulation (with \\(O(t)\\) cost). This allowed us to validate the learning ability of the architecture without writing custom CUDA kernels (which an efficient implementation of chunked symmetric power transformers requires). We will release the efficient chunked implementation in an upcoming article."
  },
  {
    "objectID": "articles/symmetric-power-transformers/index.html#linear-transformers-with-embeddings",
    "href": "articles/symmetric-power-transformers/index.html#linear-transformers-with-embeddings",
    "title": "Symmetric Power Transformers",
    "section": "1. Linear Transformers with Embeddings",
    "text": "1. Linear Transformers with Embeddings\nWe begin with a review of linear transformers. The inputs to a transformer layer are sequences of \\(Q_i, K_i, V_i \\in \\R^d\\) of queries, keys, and values, where \\(i\\) ranges from \\(1\\) to the sequence length \\(t\\). The outputs are a sequence \\(Y_i\\in \\R^d\\). The formula for the output vectors is: \\[\nY_i = \\sum_{j=1}^i A_{ij} V_j \\qquad A_{ij}  = \\frac{ \\phi(Q_i)^T \\phi(K_j)}{\\sum_{k=1}^i \\phi(Q_i)^T \\phi(K_k) }\n\\] where \\(\\phi : \\R^d \\to \\R^D\\) is an embedding function that maps keys or queries into vectors of dimension \\(D\\). This formulation is what we call the attention formulation of a linear transformer, because it involves explicitly computing the attention scores used to weight the values \\(V_j\\).\n\n\nThis linear transformer is the same architecture as in our previous article, but here we present the complete formula in its full generality, including both the normalizing term and embedding function (previously suppressed for clarity).\nThe exact same outputs can be computed via a recurrent formulation: \\[\nY_{i} = \\frac{S_i \\phi(Q_i)}{Z_i \\phi(Q_i)} \\qquad Z_i = Z_{i-1} + \\phi(K_i)^T \\qquad S_i = S_{i-1} + V_i \\phi(K_i)^T\n\\] where \\(Z_0\\) and \\(S_0\\) are \\(\\mathcal 0\\) vectors in their respective spaces. Since \\(S_i \\in \\R^{d \\times D}\\) and \\(Z_i \\in \\R^{D}\\), the size of the state is \\(D(d+1)\\).\n\n\n\nNote that \\(D(d+1)\\) gives the size of the state for any one linear transformer head. To compute the state size for an entire multi-head architecture, one must multiply by the number of layers and number of heads per layer.\n\n\n\nExpand for a derivation of the recurrent formulation.\n\nIf we expand the recurrent the definitions of \\(S_i\\) and \\(Z_i\\) we get that \\[\nZ_i = \\sum_{j=1}^i \\phi(K_j)^T \\qquad S_i = \\sum_{j=1}^i V_j \\phi(K_j)^T\n\\] Then, starting with the attention formulation \\[\n\\begin{aligned}\nY_i &= \\sum_{j=1}^i \\frac{ \\phi(Q_i)^T \\phi(K_j)}{\\sum_{k=1}^i \\phi(Q_i)^T \\phi(K_k) } V_j \\\\\n    &= \\sum_{j=1}^i V_j \\frac{ \\phi(K_j)^T \\phi(Q_i)}{\\sum_{k=1}^i \\phi(K_k)^T \\phi(Q_i) } \\\\\n    &= \\frac{ \\left( \\sum_{j=1}^i V_j  \\phi(K_j)^T \\right) \\phi(Q_i)}{ \\left(\\sum_{m=1}^i \\phi(K_m)^T \\right) \\phi(Q_i) } \\\\\n    &= \\frac{S_i\\phi(Q_i)}{Z_i\\phi(Q_i)} \\\\\n\\end{aligned}\n\\]\n\nThese two forms give rise to a variety of algorithms for training linear transformers, with differing computational properties. Read our earlier article on linear transformers for a detailed explanation. In particular, there are two algorithms that are relevant to our present discussion: parallel attention and chunked. The parallel attention algorithm is the standard algorithm used to train transformers. It does not require using the state, but its cost is \\(O(t^2d)\\), where \\(t\\) is the sequence length. The chunked algorithm can be used to train only linear transformers. It has a cost of \\(O(tdD)\\), and it requires materializing the state. The chunked algorithm is primarily what makes linear transformers interesting, because it is what allows them to be trained much more efficiently than softmax transformers when \\(t\\) is large.\n\n\nMaterializing an object refers to storing it in the GPU main memory. Sometimes, a mathematical object is necessary to perform a computation, and yet one can avoid having to store the whole thing in RAM. The most prominent example of this is Flash Attention [12], which avoids having to materialize the [t,t] attention matrix.\nWith these computational considerations in mind, how should we choose \\(\\phi\\)? Here are some attributes that we want:\n\nAdjustable dimensionality. To balance the size of the state with the size of the weights, there should be some hyperparameter controlling the dimension \\(D\\).\nEfficient dot product. In certain parts of the algorithm, \\(\\phi(Q_i)\\) and \\(\\phi(K_j)\\) appear as intermediate steps in the computation of \\(\\phi(Q_i)^T\\phi(K_j)\\). For some choices of \\(\\phi\\), there is a more efficient formula for \\(\\phi(Q_i)^T\\phi(K_j)\\) that does not require computing these intermediate objects.1\nPositive dot product. We want \\(\\phi(Q_i)^T \\phi(K_j)\\) to always be positive. This ensures that each normalized output \\(Y_i\\) is a convex combination of all preceding values \\(V_1, \\cdots, V_i\\). We found this to be essential for stable and performant learning.\nCompatible with rotary positional encoding. Rotary encodings take \\(Q_i \\to R Q_i\\) and \\(K_j \\to R K_j\\). To preserve their translational symmetry, we want \\(\\phi(R Q_i)^T \\phi (R K_j) = \\phi(Q_i)^T \\phi (K_j)\\).\n\nSubject to these four constraints, we are searching for the embedding function with the best empirical performance at each state size. Since our ultimate goal is to replace transformers (trained with attention) with linear transformers (trained with the chunked algorithm), the bar for success is simple: an embedding whose performance matches that of a transformer baseline at a state size small enough to be tractable. We’ll use 80 GB as our limit for the state size because that is the entire memory capacity of A100 and H100 GPUs.2\nMany possible embedding functions have already been investigated in the literature [3]–[6], but we are not aware of one that satisfies all of our requirements. In the next section, we’ll describe a variant of attention whose empirical performance is competitive with softmax attention. In the sections that follow, we will show that this can be implemented via a linear transformer that satisfies our desiderata."
  },
  {
    "objectID": "articles/symmetric-power-transformers/index.html#experiments",
    "href": "articles/symmetric-power-transformers/index.html#experiments",
    "title": "Symmetric Power Transformers",
    "section": "2. Experiments",
    "text": "2. Experiments\nAt first, it might not be evident that the following attention layer corresponds to a linear transformer, but it should be clear that it is a reasonable modification to the standard softmax transformer. Let \\(p\\in\\N\\) be even, \\[\nY_i = \\sum_{j=1}^i A_{ij} V_j \\qquad A_{ij}  = \\frac{ (Q_i^T K_j)^p}{\\sum_{k=1}^i (Q_i^T K_k)^p }\n\\] A classic softmax transformer would guarantee that all the attention \\(A_{i1}, \\cdots, A_{ii}\\) are positive and sum to one by applying the exponential \\(e^{Q_i^T K_j}\\). Instead, this variant does \\((Q_i^T K_j)^p\\) to achieve the same result. Raising each inner product to an even power makes the term positive, and dividing by the sum ensures each row of the attention is a distribution. We call this variant even-power attention, it has been studied before in the literature [5].\nLike softmax attention, even-power attention is compatible with rotary embeddings. A key motivation for using rotary embeddings is that they are relative, meaning that only the difference in time \\(i-j\\) influences the attention score \\(A_{ij}\\). See [11] for the full details, but in short, the relative property of rotary embeddings is guaranteed when the attention scores \\(A_{ij}\\) are unchanged by the rotation of \\(Q_i\\) and \\(K_j\\) by the same rotation matrix \\(R \\in \\R^{d \\times d}\\). This holds for even-power attention: \\[\n\\left( (R Q_i)^T R K_j \\right)^p =  \\left( Q_i^T R^T R K_j \\right)^p = \\left( Q_i^T K_j \\right)^p\n\\] Many other variants of linear transformers do not have this property [1].\nHere is a simple JAX implementation of even-power attention:\ndef even_power_attention(Q, K, V, p):\n    # even only\n    assert p % 2 == 0\n    # compute inner products\n    C = Q @ K.T\n    # raise to power\n    B = D**p\n    # apply causal mask\n    B = where(tril(ones(B.shape)), B, 0)\n    # project to simplex\n    A = B / B.sum(-1, keepdims=True)\n    # compute output\n    Y = A @ V\n    return Y\nThis implementation turns out to be more pedagogical than practical, because numerical stability is an important empirical consideration. Expand below for an implementation that addresses these issues.\n\n\nNumerically-stable implementation of power attention.\n\nNumerical instabilities come from numbers underflowing (too small) or overflowing (too large). Solutions typically fall into a few main categories:\n\nMake sure a number is not too small or too large, e.g. turn log(x) into log(x + ε). This prevents overflow.\nAccumulate in fp32. When accumulating a long list of small values in half-precision, it is sometimes the case that each addition will underflow and no accumulation will occur at all.\nManipulate an equation to cancel out some common factors algebraically, rather than letting them cancel out computationally. For example, to calculate \\(\\frac{x}{y}\\) where \\(x = Am\\) and \\(y = An\\) for some large \\(A\\), compute m / n instead of x / y.\nSeparate magnitude and sign, and work with magnitude in log-space, i.e. manipulate sign(x) and log(abs(x)) instead of working with x directly. Convert back to linear-space with sign(x) * exp(f(log(abs(x)))), where f has internally completed the relevant cancellations, so f(log(abs(x))) is small enough to avoid overflow.\n\nWith these techniques in mind, we can implement a numerically-stable version of the attention algorithm. This is the code used to generate the experimental results in this article.\ndef even_power_attn(Q, K, V, p, ε):\n     # even only\n     assert p % 2 == 0\n\n    # compute inner products\n    D = Q @ K.T\n\n    # raise to power, in log space for numerical stability\n    log_C = p * log(abs(D) + ε)\n    # apply causal mask\n    log_C = where(tril(ones(log_C.shape)), log_C, -inf)\n    # subtract rowmax for numerical stability\n    log_C -= log_C.max(axis=-1, keepdims=True)\n    \n    # Return to linear space\n    B = exp(log_C)\n    # Compute the normalizing term, accumulating in float32 for numerical stability\n    denom = B.sum(-1, keepdims=True, dtype=float32).astype(B.dtype)\n    \n    # project to simplex, adding ε for numerical stability\n    A = B / (denom + ε)\n    # compute output\n    Y = A @ V\n    return Y\n\nNow we are ready to train some models. For this experiment, we used the LongCrawl64 dataset [13], a context length of 4096, a batch size of 524288 tokens. The architecture was similar to the 124M-parameter GPT-2 architecture, but with rotational positional encoding and an additional layernorm after input embeddings. The optimization was conducted in bf16 mixed-precision using Adam with learning rate .0006 and no scheduling. Each model was trained on a node of 8 H100s.\n\n\n\n\nPerformance seems to improve consistently as we increase \\(p\\). For large enough \\(p\\), the performance of even-power transformers matches or even surpasses that of the softmax transformer baseline. This architecture is looking promising!\nIn Section 3, we will show how to use the tensor product to implement even-power attention as a linear transformer, albeit one with a state size so large as to be impractical. In Section 4, we will see that the tensor product embedding is highly symmetric and contains a lot of redundant information. We will exploit this structure to construct an embedding function for even-power attention with tractable state size."
  },
  {
    "objectID": "articles/symmetric-power-transformers/index.html#tensor-product",
    "href": "articles/symmetric-power-transformers/index.html#tensor-product",
    "title": "Symmetric Power Transformers",
    "section": "3. Tensor Product",
    "text": "3. Tensor Product\nIn this section, we show how the tensor product, a deep and ubiquitous mathematical idea, can be used to construct embeddings for the even-power transformer.\n\n3.1. Mathematical Background\nThe tensor product of vectors generalizes the outer product and formalizes the concepts of multi-dimensional arrays. Given two vectors \\(v\\in \\R^{d_1}\\) and \\(w\\in \\R^{d_2}\\) one can think of their tensor product \\(v\\otimes w\\) as the matrix \\(v w^T \\in \\R^{d_1\\times d_2}\\), \\[\nv w^T = \\left[\n\\begin{array}{cccc}\nv_1w_1 & v_1w_2 & \\cdots & v_1w_m \\\\\nv_2w_1 & v_2w_2 & \\cdots & v_2w_m \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nv_nw_1 & v_nw_2 & \\cdots & v_nw_m \\\\\n\\end{array}\n\\right]\n\\] Intuitively, \\(v \\otimes w \\otimes u\\) would be a 3-dimensional table containing all possible entries of the sort \\(v_i\nw_j u_k\\). But let’s make the intuition of multi-dimensional tables more rigorous.\nMulti-indices. A multi-index \\(\\alpha\\) specifies a location in a \\(p\\) dimensional table with dimension sizes \\(d_1,\n\\cdots, d_p \\in \\N\\). Let \\(\\N_d\\) denote the set \\(\\{1,2,\\cdots,d\\}\\). Then, the space of multi-indices is \\(\\N_{d_1} \\times \\cdots \\times \\N_{d_p}\\) and we refer to a generic multi-index as \\(\\alpha = [\\alpha_1, \\cdots, \\alpha_p] \\in \\N_{d_1} \\times \\cdots \\times \\N_{d_p}\\).\nTensors. A tensor \\(T\\in \\R^{d_1 \\times \\cdots d_p}\\) corresponds to a high-dimensional table where every location has a numerical value assigned to it. In other words, it is a map from multi-indices to the reals. \\[\nT: \\N_{d_1} \\times \\cdots \\times \\N_{d_p} \\to \\R\n\\] By convention, we index tensors with subscript notation \\(T_\\alpha\\) instead of functional notation \\(T(\\alpha)\\), but the meaning is the same.\nTensor product of vectors. Given a list of \\(p\\) vectors \\(v_i \\in \\R^{n_i}\\), we denote by \\(v_1 \\otimes \\cdots\n\\otimes v_p\\) (or alternatively \\(\\bigotimes_{i=1}^p v_i\\)) as the tensor in \\(\\R^{n_1 \\times \\cdots \\times n_p}\\) with entries given by the following formula: \\[\n\\left[\\bigotimes_{i=1}^p v_i\\right]_\\alpha = \\prod_{i=1}^p v_{i, \\alpha_i}\n\\] Where \\(v_{i,j}\\in \\R\\) denotes the \\(j\\)th entry of the \\(i\\)th vector.\nFlattening. To build embedding functions \\(\\phi\\), we are going to use the tensor product to embed lists of vectors into \\(\\R^{d_1\\times \\cdots d_p}\\) tensors. But once we’ve done that, we will no longer care about the tensor structure and we will prefer to think of them as vectors in \\(\\R^D\\), where \\(D=\\prod_{i=1}^p d_i\\). The map \\(\\text{flat}: \\R^{d_1\\times \\cdots d_p} \\to \\R^D\\) implements this transformation by writing every entry of the array into a flat vector. This can be done with any bijective function \\(\\sigma: D \\to \\N_{d_1} \\times \\cdots \\times \\N_{d_p}\\) which effectively imposes an (arbitrary) ordering on the multi-indices.3 The flattening is defined as: \\[\n\\flat{T}_i = T_{\\sigma(i)}\n\\] The dot product of flattened tensors satisfies the following property: \\[\n\\flat{\\bigotimes_{i=1}^p v_i}^T \\flat{\\bigotimes_{i=1}^p w_i} = \\prod_{i=1}^p v_i^T w_i \\qquad \\text{(Result 1)}\n\\]\n\n\nExpand to see a proof.\n\nWe can just check the both sides of the equation match. First, \\[\\begin{align}\n\\flat{\\bigotimes_{i=1}^p v_i}^T \\flat{\\bigotimes_{i=1}^p w_i}\n&= \\sum_{l=1}^D \\left[ \\bigotimes_{i=1}^p v_i \\right]_{\\sigma(l)} \\left [ \\bigotimes_{i=1}^p w_i \\right]_{\\sigma(l)}  \\\\\n&= \\sum_{l=1}^D \\prod_{i=1}^p v_{i, \\sigma(l)_i} w_{i, \\sigma(l)_i} \\\\\n&= \\sum_{j_1=1}^{d_1} \\cdots \\sum_{j_p=1}^{d_p} \\prod_{i=1}^p v_{i, j_i} w_{i, j_i} \\\\\n\\end{align}\\] Where we used the assumption that \\(\\sigma(l)\\) ranges over every possible combination of \\([j_1, \\cdots, j_p]\\). On the other hand, \\[\\begin{align}\n\\prod_{i=1}^p v_i^T w_i\n&= \\prod_{i=1}^p \\sum_{j=1}^{d_i}  v_{i, j} w_{i, j}  \\\\\n&= \\sum_{j_1=1}^{d_1} \\cdots \\sum_{j_p=1}^{d_p} \\prod_{i=1}^p v_{i, j_i} w_{i, j_i}\n\\end{align}\\] Where the last step used a generalization of the distributive property: \\(\\prod_{i=1}^p \\sum_{j=1}^{d_i}  v_{i, j} = \\sum_{j_1=1}^{d_1} \\cdots \\sum_{j_p=1}^{d_p} \\prod_{i=1}^p v_{i, j_i}\\)\n\n\n\n3.2. Implementation\nArmed with the tensor product, we are ready to define an embedding \\(\\phi^p_{\\text{TP}}\\), and in doing so define a linear transformer architecture. The definition is simple: embed a key by taking its tensor product with itself, \\(p\\) times.\n\n\nIn this section, we focus on the effect of \\(\\phi\\) on keys \\(k\\), but wlog all discussion applies equally to queries.\n\\[\n\\phi^p_{\\text{TP}}(k) =\n\\text{flat}\\left(\\bigotimes_{i=1}^p k\\right) \\in \\mathbb{R}^{d^p}\n\\]\nIf this embedding is used with even \\(p\\), the resulting architecture is an even-power transformer:\n\\[\n\\phi^p_{\\text{TP}}(q)^T \\phi^p_{\\text{TP}}(k) =\n\\text{flat}\\left(\\bigotimes_{i=1}^p q\\right)^T \\text{flat}\\left(\\bigotimes_{i=1}^p k\\right) =\n\\prod_{i=1}^p q^T k = (q^T k)^p\n\\]\nwhere in the second step we used Result 1.\nThe implementation is straightforward.\n\nEmbeddingTest\n\n\ndef tensor_power_embedding(k, p):\n    expanded_k = k\n    for _ in range(p-1):\n        expanded_k = expanded_k[...,None] @ k[None,:]\n    return expanded_k.flatten()\n\n\ndef even_power(q, k, p):\n  return np.inner(q, k) ** p\n\ndef tensor_power_inner_product(q, k, p):\n  embedded_q = tensor_power_embedding(q, p)\n  expanded_k = tensor_power_embedding(k, p)\n  return (embedded_q * expanded_k).sum()\n\nd = 8\nfor p in [2, 4, 6, 8]:\n  q = np.random.random(d)\n  k = np.random.random(d)\n  assert np.allclose(\n    even_power(q, k, p), \n    tensor_power_inner_product(q, k, p)\n  )\n\n\n\nEmbedding in hand, we can return to our main objective. Have we found an linear transformer whose performance is competitive with that of a strong transformer baseline, while, at the same time, having a state size small enough to fit on a GPU?\nThe table below shows the size of a single state, as measured in bytes (assuming fp16/bf16 precision), for a 124M-parameter GPT-2 tensor power transformer at various \\(p\\).\n\n\nThe formula for the state size of a linear transformer with the tensor power embedding is layer_n * head_count *  key_size**p * value_size.\n\n\n\n\n\n\n\n\n\n\n\np\nState Size\nMemory ≤ 80 GB?\nRelative Loss at 100K Steps\nLoss ≤ baseline?\n\n\n\n\n2\n77 MB\n✔\n1.03x\n✘\n\n\n4\n314 GB\n✘\n.98x\n✔\n\n\n6\n1.3 PB\n✘\n.97x\n✔\n\n\n\n\nThe settings of \\(p\\) that improve upon the baseline have states that are far too large. So the embedding \\(\\phi^p_{\\text{TP}}\\) still does not satisfy all the properties we are after. But we are close."
  },
  {
    "objectID": "articles/symmetric-power-transformers/index.html#symmetric-power-transformers",
    "href": "articles/symmetric-power-transformers/index.html#symmetric-power-transformers",
    "title": "Symmetric Power Transformers",
    "section": "4. Symmetric Power Transformers",
    "text": "4. Symmetric Power Transformers\nThe missing piece is to realize the huge embeddings we’ve been working with are highly symmetric. The theory of symmetric powers will help us compress the same information into much smaller objects. We will begin with an introduction to the relevant mathematical ideas. Then, we will put them to use by proposing symmetric power transformers, whose embedding is \\(\\phi^p_{\\text{SYM}}\\).\nTo build intuition, observe that the embedding \\(\\phi^2_{\\text{TP}}(v)= \\flat {v v^T}\\) is somewhat wasteful. The matrix \\(v v^T\\) is symmetric, so all the information we need can be found in the upper triangular part of the matrix. \\[\nv v^T = \\left[\n\\begin{array}{cccc}\nv_1v_1 & v_1v_2 & \\cdots & v_1v_m \\\\\nv_2v_1 & v_2v_2 & \\cdots & v_2v_m \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nv_nv_1 & v_nv_2 & \\cdots & v_nv_m \\\\\n\\end{array}\n\\right]\n\\] Entries at indices \\((i,i)\\) appear a single time, but due to the commutativity of scalar multiplication (i.e. \\(v_i v_j =\nv_j v_i\\)), the entries at indices \\((i,j)\\) each appear twice (if \\(i\\neq j\\)).\nNoticing this symmetry in the matrix \\(v v^T\\) allows us to create an alternative embedding, \\(\\phi^2_\\text{SYM}: \\R^d \\to \\R^{\\frac{d^2 +d} 2}\\), which can be implemented as:\ndef sym_2_embedding(v):\n  x, d = [], v.size\n  for i in range(d):\n    for j in range(i, d):\n      count = 1 if i==j else 2\n      x.append(sqrt(count) * v[i] * v[j])\n  return x\nThis construction of \\(\\phi^2_\\text{SYM}\\) guarantees that \\(\\phi^2_\\text{SYM}(v)^T \\phi^2_\\text{SYM}(w) = \\phi^2_\\text {TP}\n(v)^T \\phi^2_\\text{TP}(w)= (v^T w)^2\\). Recall that in the attention formulation of the linear transformer the embedding \\(\\phi\\) only influences the outputs via the attention scores, which were defined as \\[\nA_{ij}  = \\frac{ \\phi(Q_i)^T \\phi(K_j)}{\\sum_{k=1}^i \\phi(Q_i)^T \\phi(K_k) }\n\\] Then two linear transformers with embeddings \\(\\phi^2_\\text{TP}(v)\\) and \\(\\phi^2_\\text{SYM}(v)\\) will have exactly the same outputs, since they have the same inner products \\(\\phi(Q_i)^T \\phi(K_j)\\) (namely, \\((Q_i^T K_j)^p\\)). We’ve been able to exploit the symmetry of \\(v v^T\\) to construct an equivalent embedding function with approximately half the dimensionality!\nIn this section, we will generalize this idea to arbitrary powers.\n\n4.1. Mathematical Background\nWe begin by introducing some key tools.\nPermutation group. The first thing we need is the permutation group of \\(p\\) elements, which is defined as the set of all functions \\(\\rho: \\N_p \\to \\N_p\\) that are invertible and denoted by \\(G_p\\). We also overload the notation slightly. For a multi-index \\(\\alpha = [\\alpha_1, \\cdots, \\alpha_p]\\) define the permutation of the multi-index as \\(\\rho(\\alpha) = [\\alpha_{\\rho(1)}, \\cdots, \\alpha_{\\rho(p)}]\\). This is useful to define symmetric tensors.\nSymmetric tensors. A tensor \\(T \\in \\R^{\\underbrace{d\\times \\cdots \\times d}_p}\\) is symmetric if for all multi-indices \\(\\alpha \\in \\N_d \\times \\cdots \\times \\N_d\\) and permutations \\(\\rho \\in G_p\\) we have that: \\[\nT_\\alpha = T_{\\rho(\\alpha)}\n\\]\nSymmetric power of vectors. We use the notation \\(v^{\\otimes p}\\) to refer to \\(\\otimes^p_{i=1} v\\) (the tensor product of \\(p\\) copies of \\(v\\)), and call it the \\(p\\)th symmetric power of \\(v\\). Due to the commutativity of multiplication, all symmetric powers of vectors are symmetric tensors. For example, for a multi-index \\([1, 2, 3]\\), the entrie \\([v^{\\otimes 3}]_{[1, 2, 3]} = v_1 v_2 v_3\\) will equal \\([v^{\\otimes 3}]_{[3, 2, 1]} = v_3 v_2 v_1\\). Showing that a general tensor \\(v^{\\otimes p}\\) is symmetric is simple: \\[\n\\left[ v^{\\otimes p} \\right ]_{\\rho(\\alpha)} = \\prod_{i=1}^p v_{\\alpha_{\\rho(i)}} = \\prod_{i=1}^p v_{\\alpha_i} = \\left[ v^{\\otimes p} \\right ]_{\\alpha}\n\\]\nTo construct embeddings that exploit the symmetries of \\(T=v^{\\otimes p}\\) we will need some key properties about symmetric tensors:\n\nDuplication counts: If \\(T\\) is symmetric, the entry \\(T_\\alpha\\) might have duplicate entries. To know how many duplicates a multi-index \\(\\alpha\\) has, we first need to count how many times each number \\(i\\in \\{1, \\cdots, d\\}\\) occurs in \\(\\alpha\\). Define the counts \\(c_i = \\sum_{j=1}^p \\delta(\\alpha_j, i)\\). Then, the number of multi-indices containing the same data as \\(\\alpha\\) is given by the formula \\(\\frac{d!}{c_1 ! \\; \\cdots \\; c_p!}\\).\nUnique multi-indices: No data is lost if we restrict ourselves to only looking at entries \\(T_\\alpha\\) for multi-indices \\(\\alpha\\) that are non-decreasing (i.e. \\(\\alpha_i \\le \\alpha_{i+1}\\)). The intuition is that an arbitrary multi-index \\(\\beta\\) can always be transformed into a non-decreasing multi-index \\(\\alpha\\) by applying some permutation \\(\\rho\\). Using the defining property of symmetric tensors, \\(T_\\beta = T_{\\rho(\\beta)} = T_\\alpha\\). Thus, we lose no information by excluding every multi-index that isn’t non-decreasing.\nDimension: The space of symmetric tensors has dimension \\(\\binom{d+p-1}{p}\\). This can be derived via a classic combinatorial argument counting the number of non-decreasing sequences.\n\n\n\nExpand to see a complete derivation of these properties and a few other relevant facts about symmetric tensors.\n\nDuplicate counts. By definition, the only constraint a symmetric tensor has, is that all the entries \\(T_{\\rho(\\alpha)}\\) must be the same for all permutations \\(\\rho \\in G_p\\). Now we want to understand the amount of duplication that that any specific \\(\\alpha\\) has. Since the number of permutations of the multi-indices is \\(|G_p| = p!\\), a naive estimate would be that every entrie \\(T_\\alpha\\) appears \\(p!\\) times in the tensor. And indeed, that is the case for some multi-indices. For example, every permutation \\(\\rho \\in G_3\\) sends the multi-index \\([1,4,6]\\) to a different multi-index, so there are \\(3!\\) entries with the same value. But, on the other hand, for the multi-index \\([1,1,1]\\) it doesn’t matter what permutation \\(\\rho\\) we apply, we always have that \\(\\rho \\alpha = \\alpha\\). So the entrie at \\([1,1,1]\\) has no duplicates.\nTo count the number of duplicates for a generic mulit-index \\(\\alpha\\) we are going to use the orbit stabilizer theorem. This theorem tells us that the number of elements in the set \\(\\orbit(\\alpha) = \\{ \\rho(\\alpha) \\; | \\; \\rho \\in G_p \\}\\) is given by the formula: \\[\n|\\orbit(\\alpha) | =\\frac {|G_p|} {|\\stab(\\alpha)|}\n\\] where the stabilizer \\(\\stab(\\alpha) = \\{ \\rho \\in G_p \\; | \\; \\rho(\\alpha) = \\alpha \\}\\) is the set of permutations that fix \\(\\alpha\\). Working out the size of the stabilizer is not hard. For a permutation \\(\\rho \\in G_p\\) to leave \\(\\alpha\\) fixed it must satisfy that \\(\\alpha_{\\rho(i)} = \\alpha_i\\) for all \\(i\\in \\N_p\\). In other words, \\(\\rho\\) must only interchange entreis of \\(\\alpha\\) that hold the same index. Say \\(\\alpha = [1,1,2]\\), then we can only exchange the fist and second element. Generically, if index \\(i\\in\\N_d\\) appears \\(c_i\\) times in \\(\\alpha\\), then there are \\(c_i!\\) permutations that move around entries of \\(\\alpha\\) with value \\(i\\) while keeping the rest fixed. From this, it is clear that: \\[\n|\\stab(\\alpha)| = \\prod_{i=1}^d c_i !\n\\] For the example, the counts for \\([1,1,2]\\) are \\(c_1=2, \\; c_2 = 1\\) so \\(|\\stab([1,1,2])| = 2! \\: 1! =2\\). With this, we can get the formula for the number of replicated entries of \\(\\alpha\\) by applying the orbit stabilizer theorem: 4 \\[\n|\\orbit(\\alpha) | = \\frac {p!} {\\prod_{i=1}^d c_i!}\n\\]\nBasis of symmetric tensors. We know that a lot of multi-indices of a symmetric tensor are redundant. To understand the true structure (like the dimensionality) of symmetric tensors we need to find a way to select an instance of each, non redundant, multi-indices. One way to do that is to restrict oursleves to non decreasing multi-indices. Denote them by \\(P = \\{\\alpha \\in \\N_d^{\\times p} \\; | \\; \\alpha_i \\le \\alpha_{i+1} \\}\\). Then we can construct a basis for the space of symmetric tensors out of \\(\\alpha \\in P\\) like \\[\nS^\\alpha = \\sum_{\\rho \\in G_p} E^{\\rho(\\alpha)}\n\\] where \\(E^\\alpha = \\bigotimes^p_{i=1} e_{\\alpha_i}\\). (the tensors \\(E^\\alpha\\) are a the natural way to construct a basis for the non-symmetric tensors out of the basis \\(e_i \\in \\R^d\\)) To convince ourselves that \\(\\{S^\\alpha \\; | \\; \\alpha \\in P \\}\\) forms a basis of the symmetric tensors we need to check that the set is linearly independent and that it spans all symmetirc tensors. Let’s check linear independence first. Assume that we have some coefficients \\(x_\\alpha \\in \\R\\) s.t. \\[\n\\sum_{\\alpha \\in P}  x_\\alpha S^\\alpha = 0\n\\] Then, for any \\(\\beta \\in P\\) \\[\\begin{align}\n\\left[\\sum_{\\alpha \\in P}  x_\\alpha S^\\alpha\\right]_\\beta\n&= \\sum_{\\alpha \\in P}  x_\\alpha S^\\alpha_\\beta\n= \\sum_{\\alpha \\in P} x_\\alpha \\sum_{\\rho \\in G_p} E^{\\rho(\\alpha)}_\\beta \\\\\n&= \\sum_{\\alpha \\in P} x_\\alpha \\sum_{\\rho \\in G_p} \\delta(\\rho(\\alpha) = \\beta) \\\\\n\\end{align}\\] Since \\(\\alpha, \\beta \\in P\\) the only way there can exist a \\(\\rho \\in G_p\\) such that \\(\\rho(\\alpha) = \\beta\\) is when \\(\\alpha = \\beta\\). So \\[\\begin{align}\n0 &= \\left[\\sum_{\\alpha \\in P}  x_\\alpha S^\\alpha\\right]_\\beta \\\\\n&= x_\\beta \\sum_{\\rho \\in G_p} \\delta(\\rho(\\beta) = \\beta) \\\\\n&= x_\\beta \\; | \\stab (\\alpha) | \\\\\n\\end{align}\\] And, since \\(| \\stab (\\alpha) | \\ge 1\\) that implies that \\(x_\\alpha = 0\\) and we have that the set of \\(S^\\alpha\\) is linearly independent. To show \\(S^\\alpha\\) span all symmetric tensors it we can just show that, for any symmetric tensor \\(T\\), if we define \\[\nQ = \\sum_{\\alpha \\in P} \\frac {T_\\alpha} {\\stab(\\alpha)} S^\\alpha\n\\] Then \\(T = Q\\). That can be easily seen by noticing that \\(Q\\) is a symmetric tensor and that, evaluating \\(Q\\) at \\(\\beta \\in P\\) \\[\\begin{align}\nQ_\\beta &= \\left[\\sum_{\\alpha \\in P} \\frac {T_\\alpha} {\\stab(\\alpha)} S^\\alpha \\right]_\\beta\n= \\sum_{\\alpha \\in P} \\frac {T_\\alpha} {\\stab(\\alpha)} \\sum_{\\rho \\in G_p} E^{\\rho(\\alpha)}_\\beta \\\\\n&= \\sum_{\\alpha \\in P} \\frac {T_\\alpha} {\\stab(\\alpha)} \\sum_{\\rho \\in G_p} \\delta(\\rho(\\alpha) = \\beta) \\\\\n&= \\frac {T_\\beta} {\\stab(\\beta)} \\sum_{\\rho \\in G_p} \\delta(\\rho(\\beta) = \\beta) \\\\\n&= T_\\beta\n\\end{align}\\]\nDimension of symmetric tensors. Since we’ve created a basis for the space of symmetric tensors out of non-decreasing sequences we can establish the dimension of the space by counting all such sequences. This is a standard combinatorial problem solved via the method of stars and bars. Which tells us that the dimension is \\[\\binom{d+p-1}{p}\\]\nThe only thing we must note to apply the standard combinatorial results is that there is a 1-1 correspondance between non-decreasing sequences and multisets of \\(\\N_d\\) with cardinality \\(p\\). This is because there is a unique way to lay out a multi-set into a non-decreasing sequence. However many \\(1\\)s there are in the multi-set, they will all come first, then all the \\(2\\) etc…\nSymmetric powers of vectors span the symmetric tensors Showing that all tensors of the form \\(v^{\\otimes p}\\) are symmetric was tivial, but there is a harder question we might ask ourselves. Do we actually need all the \\(\\binom {d + p - 1} p\\) dimensions of the symmetric tensors? The way to formalize this question is to ask whether the space of all symmetric tensors is spanned by rank-1 symmetric tensors \\(v^{\\otimes p}\\). We will prove that the answer is yes by building every basis vector \\(S^\\alpha\\) that way. Concretely, if we define \\[\nZ^\\alpha\n= \\sum_{b_1, \\cdots, b_p = 0}^1\n\\prod_{i=1}^p (-1)^{b_i -1}\n\\left ( \\sum_{j=1}^p b_j e_{\\alpha_j}  \\right)^{\\otimes p}\n\\] Turns out that \\(Z^\\alpha = S^\\alpha\\). It is evident that both, the \\(Z^\\alpha\\) and \\(S^\\alpha\\) are symmetric tensors so, to convince ourselves that they are equivalent, we just need to index them at a non-decreasing multi-index \\(\\beta \\in P\\). First, see that \\[\\begin{align}\nS^\\alpha_\\beta\n&= \\sum_{\\rho \\in G_p} \\left [ \\bigotimes^p_{i=1} e_{\\rho(\\alpha)_i} \\right]_\\beta \\\\\n&= \\sum_{\\rho \\in G_p} \\prod^p_{i=1} \\delta(\\rho(\\alpha)_i = \\beta_i)  \\\\\n&= \\sum_{\\rho \\in G_p} \\delta(\\rho(\\alpha) = \\beta)  \\\\\n&= |\\stab(\\alpha) | \\; \\delta(\\alpha = \\beta)  \\\\\n\\end{align}\\] And, on the other hand, \\[\\begin{align}\nZ^\\alpha_\\beta &= \\sum_{b_1, \\cdots, b_p = 0}^1\n\\prod_{i=1}^p (-1)^{b_i -1}\n\\left [ \\left ( \\sum_{j=1}^p b_j e_{\\alpha_j} \\right)^{\\otimes p} \\; \\right ]_\\beta \\\\\n&= \\sum_{b_1, \\cdots, b_p = 0}^1\n\\prod_{i=1}^p (-1)^{b_i -1}\n\\prod_{i=1}^p \\left ( \\sum_{j=1}^p b_j \\delta(\\alpha_j = \\beta_i)  \\right)  \\\\\n&= \\sum_{b_1, \\cdots, b_p = 0}^1\n\\prod_{i=1}^p (-1)^{b_i -1}\n\\sum_{j_1, \\cdots, j_p = 1}^d  \\prod_{i=1}^p b_{j_i} \\delta(\\alpha_{j_i} = \\beta_i)  \\\\\n&= \\sum_{j_1, \\cdots, j_p = 1}^d  \\sum_{b_1, \\cdots, b_p = 0}^1\n\\prod_{i=1}^p (-1)^{b_i -1}\n\\prod_{i=1}^p b_{j_i} \\delta(\\alpha_{j_i} = \\beta_i)  \\\\\n\\end{align}\\] During the sum over all combinations of \\(j_1, \\cdots, j_p\\), if there is any \\(l \\in \\N_p\\) that does not appear in the set \\(j_1, \\cdots, j_p\\), then that term of the sum will drop out. This is because the only place where \\(b_l\\) will appear is in the term \\((-1)^{b_l-1}\\), so since we are summing over \\(b_l \\in \\{0, 1\\}\\), the terms will cancel out. Thus, we can restrict ourselves to summing over \\(j_1, \\cdots, j_p\\) that contain every element in \\(\\N_p\\). In other words, the \\(j\\) terms must be a permutation \\([j_1, \\cdots, j_p] = \\rho([1, \\cdots, p])\\) for some \\(\\rho \\in G_p\\). So we can continue \\[\\begin{align}\nZ^\\alpha_\\beta &=  \\sum_{\\rho \\in G_p} \\sum_{b_1, \\cdots, b_p = 0}^1\n\\prod_{i=1}^p (-1)^{b_i -1}\n\\prod_{i=1}^p b_{\\rho(i)} \\delta(\\alpha_{\\rho(i)} = \\beta_i)  \\\\\n&= \\sum_{\\rho \\in G_p}  \\prod_{i=1}^p  \\delta(\\alpha_{\\rho(i)} = \\beta_i)  \\\\\n\\end{align}\\] Where we used the fact that, since the inner term is multiplied by every \\(b_i\\), the only way the term can be non \\(0\\) is if every single \\(b_i =1\\). Finally, we can wrap up the proof, \\[\\begin{align}\nZ^\\alpha_\\beta\n&= \\sum_{\\rho \\in G_p} \\prod_{i=1}^p \\delta(\\alpha_{\\rho(i)} = \\beta_i)  \\\\\n&= \\sum_{\\rho \\in G_p} \\delta(\\rho(\\alpha) = \\beta)  \\\\\n&= |\\stab(\\alpha) | \\; \\delta(\\alpha = \\beta)  \\\\\n&= S^\\alpha_\\beta\n\\end{align}\\]\n\n\n\n4.2 Implementation\nThe symmetric power embedding \\(\\phi^p_\\text{SYM}(v)\\) will give a list of \\(\\binom{d+p-1}{p}\\) numbers, each corresponding to \\([v^{\\otimes p}]_\\alpha\\) for a non-decreasing \\(\\alpha\\). Just as we did in the example of \\(\\phi^2_\\text{SYM}\\), we also need to apply a correction that is the square root of the duplicate count of that particular \\(\\alpha\\). The inner product of two vectors embedded in this way is identical to the tensor power embedding, \\(\\phi^2_\\text{SYM}(v)^T \\phi^2_\\text{SYM}(w) = \\phi^2_\\text {TP}(v)^T \\phi^2_\\text {TP}(w)\\). The following is an example implementation of this embedding:\n\nEmbeddingTest\n\n\ndef symmetric_power_embedding(k, p):\n    d = len(k)\n    x = []\n    for midx in non_decreasing_multiindices(p, d):\n        c = count(midx, d)\n        xi = np.sqrt(multinomial(c))\n        for j in range(p):\n            xi *= k[midx[j]]\n        x.append(xi)\n    return np.array(x)\n\n# -- helper functions --\n# generates list of non-decreasing multiindices\ndef non_decreasing_multiindices(n, max_idx, starting_from=0):\n    if n == 1:\n        return [[i] for i in range(starting_from, max_idx)]\n    seqs = []\n    for i in range(starting_from, max_idx):\n        seqs += [[i, *remainder] for remainder in\n                    non_decreasing_multiindices(n-1, max_idx, starting_from=i)]\n    return seqs\n\n# computes multinomial coefficient\ndef multinomial(lst):\n    res, i = 1, 1\n    for a in lst:\n        for j in range(1, a + 1):\n            res *= i\n            res //= j\n            i += 1\n    return res\n\n# given a multiindex, counts how many times each index appears\ndef count(midx, d):\n    c = [0] * d\n    for i in midx:\n      c[i] += 1\n    return c\n\n\ndef even_power(q, k, p):\n  return np.inner(q, k) ** p\n\ndef symmetric_power_inner_product(q, k, p):\n  embedded_q = symmetric_power_embedding(q, p)\n  expanded_k = symmetric_power_embedding(k, p)\n  return (embedded_q * expanded_k).sum()\n\nd = 8\nfor p in [2, 4, 6, 8]:\n  q = np.random.random(d)\n  k = np.random.random(d)\n  assert np.allclose(\n    even_power(q, k, p), \n    symmetric_power_inner_product(q, k, p)\n  )\n\n\n\nUsing this embedding produces a massive dimensionality reduction compared to the dimensionality of \\(\\phi_\\text{TP} ^p\\). The table below compares the size of the state between repeated tensor products and symmetric powers, as measured in bytes (assuming half-precision), for a 124M-parameter GPT-2 transformer at various \\(p\\).\n\n\n\n\np\nTensor Power\nSymmetric Power\nSavings\n\n\n\n\n2\n77 MB\n39 MB\n49%\n\n\n4\n314 GB\n14 GB\n96%\n\n\n6\n1.3 PB\n2.2 TB\n99.8%\n\n\n8\n5.3 EB\n199 TB\n99.996%\n\n\n\n\nWe can evaluate each symmetric power architecture against our two metrics, state size (under 80 GB) and performance (loss below baseline).\n\n\n\n\np\n\n\nState Size\n\n\nMemory ≤ 80 GB?\n\n\nRelative Loss at 100K Steps\n\n\nLoss ≤ baseline?\n\n\n\n\n\n\n2\n\n\n39 MB\n\n\n✓\n\n\n1.03x\n\n\n✗\n\n\n\n\n4\n\n\n14 GB\n\n\n✓\n\n\n0.98x\n\n\n✓\n\n\n\n\n6\n\n\n2.2 TB\n\n\n✗\n\n\n0.97x\n\n\n✓\n\n\n\n\nThe symmetric power transformer with \\(p=4\\) passes our bar."
  },
  {
    "objectID": "articles/symmetric-power-transformers/index.html#conclusion",
    "href": "articles/symmetric-power-transformers/index.html#conclusion",
    "title": "Symmetric Power Transformers",
    "section": "5. Conclusion",
    "text": "5. Conclusion\nIn this article, we have introduced the symmetric power transformer, a linear transformer which closes the performance gap to classic softmax transformers using a tractably-small state. We replace the exponentiation in a traditional softmax transformer with an even power, and then show that this is equivalent to a linear transformer with the symmetric power embedding. We expect this approach will provide transformer-level performance at greatly reduced training costs when combined with the chunked algorithm. It will also enjoy cheaper inference, thanks to the constant-time inference costs common to all RNNs. In an upcoming article, we plan to release an open-source model that uses a symmetric power transformer at its core, together with an efficient CUDA kernel implementation. Stay tuned!\n\n  \n  Subscribe to be notified of new posts:"
  },
  {
    "objectID": "articles/symmetric-power-transformers/index.html#footnotes",
    "href": "articles/symmetric-power-transformers/index.html#footnotes",
    "title": "Symmetric Power Transformers",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is also known as the kernel method. It is essential when one works with infinite-dimensional embeddings, and it’s also useful in this case to avoid materializing large-but-finite embeddings.↩︎\nThis state-size threshold is admittedly somewhat arbitrary. In principle, larger states are possible with clever sharding; but for excessively large states, which must be sharded across a huge number of GPUs, the hardware cost of sharding becomes completely prohibitive. In this article, we are training models whose parameters fit on a single GPU, so it seems reasonable to use the memory of a single GPU as the threshold for tractability.↩︎\nA natural choice for the ordering \\(\\sigma\\) is row major ordering.↩︎\nYou might have previously seen this expression in the multinomial theorem. This connection is no coincidence. Symmetric powers are highly related to polynomails.↩︎"
  },
  {
    "objectID": "articles/what-is-vidrial/index.html",
    "href": "articles/what-is-vidrial/index.html",
    "title": "What Is Vidrial?",
    "section": "",
    "text": "Vidrial is a set of tools for writing clean high-performance CUDA code. Kernels written with Vidrial are blazing fast, and are more readable, more extensible, and more testable than any other framework."
  },
  {
    "objectID": "articles/what-is-vidrial/index.html#footnotes",
    "href": "articles/what-is-vidrial/index.html#footnotes",
    "title": "What Is Vidrial?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe have not yet implemented SM90 features such as TMA and WGMMA in Vidrial, so a comparison against FlashAttention2 is more fair than FlashAttention3.↩︎\nThis sweep of configurations has already been pared down from the full set of possible configurations using hand-designed heuristics. Worse configurations than those shown here exist.↩︎"
  },
  {
    "objectID": "blogposts/mission/index.html",
    "href": "blogposts/mission/index.html",
    "title": "Our Mission",
    "section": "",
    "text": "A poem is a shadow of the act of writing poetry.\nHumanity casts many shadows. All literature, letters, recipes, and tweets. Mathematical proofs and git repositories. Laws, treaties, and declarations of war. Financial statements, employee performance reports, and bankruptcy filings. Podcasts, operas, accidental voicemails, YouTube videos and Hollywood blockbusters. Restaurant reviews and love letters and times tables. Individually, each of these pieces of information is nothing but the faintest shadow of the process that produced it. But collectively, these shadows tell a rich story about the world.\nSince the dawn of humanity, the brain alone could reconstruct the world from these shadows. But the last decade of deep learning has convinced us that this won’t be the case for much longer. Though significant challenges remain, we stand poised to solve them. If successful, it will become possible to synthesize every documentable aspect of humanity into the weights of a neural network.\nOur mission is to train a neural network to model all human output.\nThere are two primary challenges in our pursuit of this goal.\n\nCurrently, it’s not technically feasible to train a model that can ingest all the data that we can collect. Limitations around context length, modality, and throughput force us to use only a small subset of the data available to us.\nMuch of the data we will need has never been collected, curated, and organized into datasets that we can use for training.\n\nWe are building a world-class team of engineers and researchers to tackle these challenges, united around shared principles and a specific research agenda. We value clear thinking, sharing knowledge, and an extreme commitment to scientific honesty. Our research is guided by mathematical beauty and grounded in rigorous empiricism. We are committed to letting the quality of our work speak for itself. No hype, no fluff. All meat.\nIf this vision resonates, please reach out:          777b7a607577605479757a7d72716760757d3a777b79\n\n\n  \n  Or, subscribe to be notified of new posts:"
  }
]