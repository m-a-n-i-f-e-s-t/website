[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJan 5, 2024\n\n\nLinear Transformers Are Faster After All\n\n\nJacob Buckman, Carles Gelada\n\n\n\n\nJan 4, 2024\n\n\nOur Mission\n\n\nJacob Buckman, Carles Gelada\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "articles/linear-transformers-are-faster/index.html",
    "href": "articles/linear-transformers-are-faster/index.html",
    "title": "Linear Transformers Are Faster",
    "section": "",
    "text": "\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\Z}{\\mathbb{Z}}\n\\newcommand{\\N}{\\mathbb{N}}\n\\newcommand{\\sft}{\\text{softmax}}\n\\newcommand{\\List}{\\text{List}}\n\\newcommand{\\Seq}{\\text{Seq}}\n\\newcommand{\\SeqT}{\\text{SeqT}}\n\\newcommand{\\CSeqT}{\\text{CSeqT}}\n\\newcommand{\\Dist}{\\text{Dist}}\n\\newcommand{\\SM}{\\text{SM}}\n\\newcommand{\\Fn}{\\text{Fn}}\n\\newcommand{\\Tok}{\\text{Tok}}\n\\newcommand{\\Aij}{ A_{[i,j]}}\n\\]\nIt is well-known that removing the exponential from the attention layer of a transformer allows for a recurrent reformulation, with computational cost that is linear instead of quadratic on context length [1]. One would expect that such an architecture would be far faster to train, especially when the context size is large. However, when training deep neural networks on hardware accelerators (e.g. GPUs), reductions in FLOPs do not always straightforwardly translate into practical speedups. Initial empirical experiments showed that large language models based on linear transformers train more slowly than classic transformers, and led many to dismiss the approach as nice-in-theory but unhelpful-in-practice [2].\nAt the moment, the conventional wisdom in the field is that a quadratic-cost algorithm with highly optimized hardware utilization (e.g. FlashAttention) gives the best training throughput [3]. But this is mistaken. In this post, we explain several different ways of implementing linear transformers and we show that, in fact, they can produce massive speed-ups.\nThe experiment below showcases the central takeaway. We compare the speed of an optimized transformer, a straightforward recurrent implementation of the linear transformer (as described in [1]), and an optimized linear transformer (described in Section 3). We see that, despite exhibiting better growth with respect to context size, the linear transformer trains much more slowly than the transformer in wall-clock time. In contrast, our algorithm is strictly faster than even the highly-optimized FlashAttention baseline, at all context and model sizes. At moderately large context sizes, this speedup has an larger impact than FlashAttention itself.\nBut speed is only one part of the picture. We also need to know whether linear transformers actually minimize the loss as well as standard transformers do. Unfortunately, as the next plot shows, directly converting GPT2 to use linear transformer layers hurts learning significantly.\nThese results are discussed in detail in Section 5, but in short: linear transformers seem to become increasingly unstable as the sequence length grows, negatively affecting learning. This means that our linear transformers are somewhat useless,1 as the positive impact from the speedup seen in long contexts is undermined by the negative impact of degraded learning.\nOther variants of linear transformers have been proposed that claim resolve these learning issues [5]–[11], but we do not survey them today. Since our main motivation in this post is to share our insights on how to implement efficient linear transformers we stick with the most natural variant, which is most closely analogous to standard transformers. In a future post, we will explain how to improve the learning of linear transformers, allowing us to efficiently train unprecedentedly-long-context language models with super-fast sampling."
  },
  {
    "objectID": "articles/linear-transformers-are-faster/index.html#linear-transformers",
    "href": "articles/linear-transformers-are-faster/index.html#linear-transformers",
    "title": "Linear Transformers Are Faster",
    "section": "1. Linear Transformers",
    "text": "1. Linear Transformers\nThe inputs to a transformer layer are sequences of \\(Q_i, K_i, V_i \\in \\R^d\\) of query, key and values, where \\(i\\) ranges from \\(1\\) to the sequence length \\(t\\). The outputs are a sequence \\(Y_i\\in \\R^d\\). The well-known formula for the transformer layer, first popularized by Vaswani et al [12], is: \\[\nY_i^\\text{Transformer} = \\sum_{j=1}^i e^{Q^T_i K_j} V_j\n\\] Here we are excluding the denominator of the softmax for simplicity of notation. Although the normalization provided by the denominator is important for good learning performance, it doesn’t have a major impact on the computational cost or implementation speed, which are our main focus here.\n\n\nEven though we omit the normalization for the mathematical exposition, it is included in our implementation for all experiments.\nThe formula for the linear transformer (LT) layer is quite similar: just change the term \\(e^{Q^T_i K_j} \\to  Q^T_i K_j\\) yielding \\[\nY_i^\\text{LinearTransformer} = \\sum_{j=1}^i Q^T_i K_j V_j\n\\]\n\n\nAll our experiments with linear transformers also include a normalization factor, which we empirically found to be important for learning performance. Drawing on [1], we divide each \\(Y_i\\) by \\(\\sum_{j=1}^i Q^T_i K_j\\) after eunsuring the sum is positive by making keys and queries live in the positive quadrant using softplus.\nThis layer is “linear” in that the outputs \\(Y\\) are linearly related to all of \\(Q\\), \\(K\\), and \\(V\\).2 From now on, we will omit the superscript of \\(Y_i^\\text{LinearTransformer}\\) and just write \\(Y_i\\). To begin our exploration of the computational cost of linear transformers, consider the following implementation.\ndef LT_attention(Q, K, V):\n    \"\"\"\n    Shapes of inputs are\n     Q: [t, d]  K: [t, d]  V: [t, d]\n    Shapes of outputs are\n     Y: [t, d]\n    \"\"\"\n    t, d = Q.shape\n    Y_list = []\n    for i in range(t):           # loop cost: O(t^2 d)\n        Y_i = zeros(d)\n        Q_i = Q[i]\n        for j in range(i+1):     # loop cost: O(id)\n            A_ij = inner(K[j], Q_i)  # cost: O(d)\n            Y_i += A_ij * V[j]   # cost: O(d)\n        Y_list.append(Y_i)\n    return stack(Y_list)\nAnyone who has worked with self-attention will recognize that this is a terrible implementation, since nested for-loops are not GPU-friendly. Don’t worry: in the next section, we will discuss implementations that parallelize computation, and thus run much faster on modern hardware. For now, the main point of this pseudocode is to highlight that first mode of computation, which we call attention formulation, has a FLOP cost of \\(O(t^2 d)\\).\nThe key to the massive speedups we saw in the introduction comes from leveraging linearity to restructure the computation. Consider the following factorization: \\[\nY_i = \\sum_{j=1}^i Q^T_i K_j V_j = \\underbrace{ \\left (  \\sum_{j=1}^i V_j  K_j^T\\right )}_{S_i} \\; \\; Q_i\n\\] Written in this form, we notice that the term labeled \\(S_i \\in \\R^{d\\times d}\\) can be thought of as a state summarizing all the relevant information up to time \\(i\\). It’s easy to rewrite into the following recurrent equations \\[\nY_{i} = S_i Q_i\n\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\nS_i = S_{i-1} + V_i K_i^T\n\\] where we assume \\(S_{0} = 0\\in \\R^{d\\times d}\\). Written in this form, we realize that a linear transformer is an RNN. We can also write pseudocode for this approach, which we call the state formulation, and analyze the cost:\ndef LT_state(Q, K, V):\n    \"\"\"\n    Shapes of inputs are\n     Q: [t, d]  K: [t, d]  V: [t, d]\n    Shapes of outputs are\n     Y: [t, d]\n    \"\"\"\n    t, d = Q.shape\n    S_i = zeros(d, d) # shape [d,d]\n    Y_list = []\n    for i in range(t):        # loop cost: O(t d^2)\n        S_i += outer(K[i], V[i]) # cost: O(d^2)\n        Y_i = S_i @ Q[i]      # cost: O(d^2)\n        Y_list.append(Y_i)\n    return stack(Y_list)\nWe see that the cost here is \\(O(t d^2)\\).\nSo, while a standard transformer layer always has cost \\(O(t^2 d)\\), linear transformers have two formulations with different costs. By switching from the attention formulation to the state formulation, we can change the cost from \\(O(t^2 d)\\) to \\(O(t d^2)\\), trading a \\(t\\) term for a \\(d\\) term."
  },
  {
    "objectID": "articles/linear-transformers-are-faster/index.html#parallel-implementations",
    "href": "articles/linear-transformers-are-faster/index.html#parallel-implementations",
    "title": "Linear Transformers Are Faster",
    "section": "2. Parallel Implementations",
    "text": "2. Parallel Implementations\nIn general, for-loops are a terrible way to implement anything that will run on a GPU. When using high-level frameworks like PyTorch or JAX, the easiest way to get high GPU utilization is to only use primitives that are already highly optimized for GPU: matrix multiplication, elementwise operations, etc. We can rewrite our attention and state algorithms in this style to make them efficient.\nFirst, let’s do this for attention. Our main technique is to compute the attention matrix \\(A\\), which contains all the terms outer(Q[i], K[j]) that appeared inside the for-loops of LT_attention, using a single heavyweight matrix multiply.\ndef LT_attention_parallel_no_flash(Q, K, V):\n    \"\"\"\n    Shapes of inputs are\n     Q: [t, d]  K: [t, d]  V: [t, d]\n    Shapes of outputs are\n     Y: [t, d]\n    \"\"\"\n    t = Q.shape[0]\n    M = causal_mask(t)\n    A_raw = Q @ K.T  # cost O(t^2 d)\n    A = A_raw * M    # cost O(t^2)\n    Y = A @ V        # cost O(t^2 d)\n    return Y\nThis implementation lies at the core of nearly every transformer of the past five years, powering all of the AI models that have recently exploded in popularity. Since it is composed of such a small number of ultra-parallelizable ops, it obtains high GPU utilization. Recently, specialized flash attention kernels [3] have been used to get even further speedups by avoiding explicitly storing the attention matrix \\(A\\), and thereby saving both memory and time spent on memory-transfers. Algorithmically, though, flash attention is a variant of parallel attention, and has the same computational cost (as measured in FLOPs). We use LT_attention_parallel to refer to the flash attention implementation.\nNext, let’s parallelize the recurrent formulation. This optimization is less well-known. The key is to compute all the terms \\(V_i K^T_i\\) in parallel, and then use a cumulative-sum, which can be parallelized, to combine them.\ndef LT_state_parallel(Q, K, V):\n    \"\"\"\n    Shapes of inputs are\n     Q: [t, d]  K: [t, d]  V: [t, d]\n    Shapes of outputs are\n     Y: [t, d]\n    \"\"\"\n    P = V[:,:,None] @ K[:,None,:]  # cost: O(t d^2)\n    S = cumsum(P, axis=0)          # cost: O(t d^2)\n    Y = S @ Q[:,:,None]            # cost: O(t d^2)\n    return Y[:,:0]\nThe cost in FLOPs of this algorithm is \\(O(t d^2)\\).\nNow that we have four mathematically-equivalent implementations of a linear transformer, let’s time the training step of our GPT2 models and see how big of a difference it makes. For our LT_attention_parallel implementation, we use a custom linear self-attention flash kernel we implemented in Triton [13] based on OpenAI’s FlashAttention2 implementation.\n\n\n\n\nHere are some takeaways:\n\nAs expected, the attention variants all have a quadratic asymptotic cost (slope of 2 on a log-log plot3). The state variants all have linear asymptotic cost (slope 1). 4\nLT_state_parallel is an order-of-magnitude faster than LT_state.\nLT_attention_parallel_no_flash is two orders-of-magnitude faster than LT_attention.\nLT_attention_parallel seems to asymptotically stabilize into being an order-of-magnitude faster than LT_attention_parallel_no_flash.\nFor the majority of settings, LT_attention_parallel is the fastest. (This is the linear version of the algorithm used by the standard transformer.)\nParallel attention is the fastest algorithm for small context sizes. However, LT_state_parallel overcomes LT_attention_parallel_no_flash at around 13k context size, and overcomes LT_attention_parallel at around 100k.\n\nOverall, these results paint a clear picture: use the attention algorithm for small contexts and the state algorithm for large contexts. But do we really face a binary choice? Can we combine the state and attention ideas and get the best of both words?"
  },
  {
    "objectID": "articles/linear-transformers-are-faster/index.html#chunked-formulation",
    "href": "articles/linear-transformers-are-faster/index.html#chunked-formulation",
    "title": "Linear Transformers Are Faster",
    "section": "3. Chunked Formulation",
    "text": "3. Chunked Formulation\nIt’s evident that, for small context sizes, computing the \\(t\\) by \\(t\\) attention matrix is much more efficient than computing many \\(d\\) by \\(d\\) state matrices. But as \\(t\\) grows, there is a point where the quadratic cost of the attention matrix ends up dominating. Noticing that attention is extremely efficient for small \\(t\\) and that states are necessary for large \\(t\\) motivates doing one last reworking of the LT equation.\nLet \\(c \\in \\N\\) be a positive integer that we’ll call the chunk size. For any \\(i\\in \\N\\) find the unique \\(n\\in \\Z\\) s.t. \\(cn &lt; i \\le c(n+1)\\). We can easily see that the following equations are equivalent to the previous ones. \\[\nY_{i} = S_{cn}Q_i + \\sum_{j=cn+1}^i Q_i^T K_j V_j\n\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\nS_{c(n+1)} = S_{cn} + \\sum_{j=cn+1}^{c(n+1)} V_j K_j^T\n\\] The key idea is that we are only going to compute a subset of all states: \\(S_0, S_c, S_{2c}, \\cdots\\). Then, to compute each output \\(Y_i\\), we need only to take into account the contribution via the most recent state \\(S_{cn}\\), as well as the contribution (computed via attention) of all moments in time \\(j\\) in the range \\(cn &lt; j \\le i\\).\nAs pseudocode, this looks like:\ndef LT_attention_with_initial_state(S, Q, K, V):\n    \"\"\"\n    Shapes of inputs are\n     S: [d, d]  Q: [c, d]  K: [c, d]  V: [c, d]\n    Shapes of outputs are\n     Y: [c, d]\n    \"\"\"\n    Y_state = Q @ S                               # cost O(c d^2)\n    Y_attention = LT_attention_parallel(Q, K, V)  # cost O(c^2 d)\n    Y = Y_state + Y_attention                     # cost O(cd)\n    return Y\n\ndef LT_chunked(Q, K, V, c):\n    \"\"\"\n    Shapes of inputs are\n     Q: [t, d]  K: [t, d]  V: [t, d], c: int\n    Shapes of outputs are\n     Y: [t, d]\n    \"\"\"\n    t, d = Q.shape\n    assert t % c == 0\n    Q_, K_, V_ = [arr.reshape(t//c, c, d)\n    `               for arr in [Q,K,V]]\n    P_ = K_.transpose([0,2,1]) @ V_  # cost O(t d^2)\n    S_ = cumsum(P_, axis=0) - P_     # cost O((t/c)d^2)\n    Y_ = vmap(LT_attention_with_initial_state, axis=0)(\n                S_, Q_, K_, V_)      # cost O(td^2 + tcd)\n    return Y_.reshape(t, d)\nThe cost is \\(O\\left(td^2 + tcd\\right)\\), once again avoiding a quadratic dependency on \\(t\\). Also, note that this algorithm makes an inner call to LT_attention_parallel, so we can use a flash-attention kernel to do that part of the computation.\nThis algorithm has a hyperparameter, the chunk size, which must be set correctly for optimal performance. Fortunately, it is inexpensive to identify the best chunk size empirically, since measuring just a few steps of training at each chunk size is sufficient to identify which is the fastest. In the plot below, we plot the speed of the optimally-chunked algorithm in each setting.\n\n\n\n\nWe see LT_chunked gives the desired best-of-both-worlds behavior, as it is equal-or-better than all other approaches in all settings. And we now see the massive (& rapidly growing) speedup relative to standard self-attention, finally unlocking the true power of linear transformers."
  },
  {
    "objectID": "articles/linear-transformers-are-faster/index.html#sampling",
    "href": "articles/linear-transformers-are-faster/index.html#sampling",
    "title": "Linear Transformers Are Faster",
    "section": "4. Sampling",
    "text": "4. Sampling\nWhen working with language models, efficient training is not the only performance that deserves consideration. Once the model is trained, how expensive is it to utilize? When we are sampling we have a sequence of tokens, \\(z_1 \\cdots z_t\\), and we want to sample the next token, \\(z_{t+1}\\).\nThe most efficient algorithm to sample from traditional transformers is called the KV-cache algorithm [14]. This algorithm assumes that when we generate token \\(z_{t+1}\\), we will have already computed and cached all the \\(K_i, V_i\\) for all \\(0 \\le i \\le t\\). In order to compute the output of the attention layer at time \\(t+1\\) given this cached information, we can use \\[\nY_{t+1}^\\text{Transformer} = \\sum_{j=1}^{t+1} e^{Q^T_i K_j} V_j\n\\] It is easy to see that this is an \\(O(td)\\) operation. In other words, as the sequence length grows, sampling each subsequent token becomes more computationally expensive.5 This is one of the major limitations of the classic transformer architecture.\nWith linear transformers, however, we have access to the recurrent formulation. A linear transformer is an RNN where the state has size \\(O(d^2)\\). \\[\nY_{i} = S_i Q_i\n\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\nS_i = S_{i-1} + V_i K_i^T\n\\] We can compare the time it takes to generate any particular token when sampling a sequence:\n\n\n\n\nAs expected, we see that the time to sample of the linear transformer is independent of context length, whereas that of the KV-cache transformer grows linearly. This leads to a large gap in inference speed at large contexts.6"
  },
  {
    "objectID": "articles/linear-transformers-are-faster/index.html#learning-performance",
    "href": "articles/linear-transformers-are-faster/index.html#learning-performance",
    "title": "Linear Transformers Are Faster",
    "section": "5. Learning Performance",
    "text": "5. Learning Performance\nUntil now, our focus has been on how to implement linear transformers efficiently. But an architecture that runs really fast is useless unless it also learns well. We will now compare the learning curves of the GPT2 baselines with their linear transformer counterparts, at various context sizes.\nIn order to control for the fact that longer contexts help learning by introducing more tokens per update, we hold the number of tokens-per-update constant across context sizes by decreasing batch size. At all context-lengths, each update sees \\(2^{19}\\) tokens.7 Importantly, for this set of experiments, we have used the dataset c4 [4], which does not have much long-term structure: it consists of a shuffled collection of short articles, of average length ~500 tokens.\nFirst, let’s look at the parameter scaling law of GPT2 and our modified Linear-GPT2. The following plot shows the final performance after 24h of training on our 8xH100 server for each scale and each context size. Click the second tab if you want to see the full learning curves.\n\n\n\n\nBoth architectures scale similarly with respect to model size, but there is a gap in performance. This gap seems to grow as context size increases, together with more loss spikes and general instability. It’s possible that the gap can be explained by the linear transformer not effectively utilizing its context. To explore whether or not this is the case, we can use these same experiments, but visualize all context-lengths together.\n\n\n\n\nWe see a dramatically different trend between the two architectures. For GPT2, each increase in context length slows down the initial pace of learning, but ultimately all context-lengths (beyond at least 1024) converge to a similar final loss. Whereas for Linear-GPT2, not only does increasing the context slow down learning in the beginning, but it also causes convergence to a worse final performance and nasty-looking loss spikes.\nThe results for GPT2 are what we would expect from a healthy model, given the setting. Note that since our dataset consists of short articles, of average length ~500 tokens, we can assume that even a context window of 1024 would include nearly everything relevant. Thus, we wouldn’t expect that increasing the context length beyond this point would decrease the loss the model ultimately converges to. Longer-context models do however need to learn to ignore many more irrelevant tokens, explaining the slowed initial learning.8\nIn contrast, the results for Linear-GPT2 seem to indicate some underlying instability of the linear transformer architecture. It doesn’t even seem capable of ignoring irrelevant information, let alone exploiting useful long-term structure.\nRemedying these learning issues will be the main focus of our future work in linear transformers. Our current architecture is essentially a one-to-one clone of GPT2, sticking as architecturally close to the original as possible; aspects such as initializations, normalizations and hyperparameters were directly copied. It is well-known that these decisions can have a large impact on scaling and stability and often need to be tuned in an architecture-specific way. In the literature, various other linear variants of self-attention have been proposed, that incorporate techniques such as gating mechanisms in order to improve stability and learning [5]–[11]. A future post will include a thorough study of the impact of all of these choices.\nUltimately, it may be impossible for any linear transformer to perfectly match the context scaling laws of the classic transformer baseline. Although removing the exponential seems like a minor change, it represents a meaningful decrease in expressivity. But as we’ve shown, linear transformers can be trained orders-of-magnitude more efficiently. On equivalent compute, linear transformers can be trained for many more steps; or, keeping steps constant as well, we can train a much larger model. If this additional scaling is sufficient to compensate for the reduced expressivity, linear transformers will be the more efficient architecture overall.\nIn follow-up work, we show that a linear transformer variant that we call the symmetric power transformer is able to match the softmax transformer in performance.\n\n  \n  Subscribe to be notified of new posts:"
  },
  {
    "objectID": "articles/linear-transformers-are-faster/index.html#footnotes",
    "href": "articles/linear-transformers-are-faster/index.html#footnotes",
    "title": "Linear Transformers Are Faster",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs discussed in Section 4, a second benefit of linear transformers is that the cost to sample a token does not grow with context size. Perhaps one could argue that this improvement in sampling speed could, on its own, justify using linear transformers for applications where the inference costs vastly exceed training costs. But it is evident to us that, for linear transformers to become actually useful, we need to address these instability issues.↩︎\nIt is not named for the fact that the computational cost is linear with respect to \\(t\\)! That is just a coincidence. (And is not even always true, as we will see.)↩︎\nIf \\(y=x^2\\), a log-log plot where \\(y'=\\log_a(y)\\) and \\(x'=\\log_a(x)\\) for any base \\(a\\), then \\(y'=\\log_a(y) = \\log_a(x^2) = 2 \\log_a(x) = 2 x'\\). So the graph will be a line with slope 2.↩︎\nThe reason we see the expected slopes asymptotically is that we are timing a full GPT2 architecture which has many other components besideds the attention layer. If we were only timing the attention layer, the plots would all be straight lines.↩︎\nAn interesting connection is that the KV-cache can be understood as the state of an RNN with non-constant state size; namely, one whose state-size is \\(O(td)\\).↩︎\nThis comparison may not be completely fair. In these experiments, our implementation of neither sampling algorithm makes use of specialized kernels. A lot of the ideas of flash attention can be used to write a much faster KV cache sampling algorithm; on the other hand, it’s unclear if much improvement is possible on the recurrent sampling. Thus, it’s possible that with engineering effort the gap between the two algorithms could become smaller. However, the overall pattern will certainly remain the same.↩︎\ne.g. runs with context-size 1024 would have batch-size of \\(2^{19} / 2^{10} = 2^{9} = 512\\).↩︎\nPut another way: doubling the size of the input vastly increases the size of the function space over which gradient descent must search, and it’s intuitive that in a larger space it takes somewhat longer to find a good solution.↩︎"
  },
  {
    "objectID": "articles/gd-minimizes-loss/index.html",
    "href": "articles/gd-minimizes-loss/index.html",
    "title": "Why Gradient Descent Minimizes Training Loss",
    "section": "",
    "text": "\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\der}{\\partial}\n\\newcommand{\\dldt}{\\frac{\\der l}{\\der t}}\n\\newcommand{\\len}{\\text{length}}\n\\newcommand{\\en}{\\text{energy}}\n\\newcommand{\\relu}{ {\\small\\text{ReLU}} }\n\\newcommand{\\dim}{\\text{dim}}\n\\newcommand{\\tr}{\\text{trace}}\n\\newcommand{\\lin}{\\text{Lin}}\n\\newcommand{\\rnk}{\\text{rank}}\n\\newcommand{\\ht}{\\widehat}\n\\newcommand{\\dwdt}{\\frac{\\der w}{\\der t}}\n\\newcommand{\\l}{\\mathscr{l}}\n\\newcommand{\\E}{\\mathbb E}\n\\]\nEvery student of deep learning has wondered at some point in their journey: “How can gradient descent on a neural network possibly succeed? Surely, on such a large and non-convex loss landscape, we will get stuck in some local minimum, far from the optimal solution.” And yet, deep learning practitioners quickly learn to trust that a large enough neural network with properly tuned hyper-parameters will successfully fit any training data we throw at it [1]. In this article, we explore a mathematical theory to explain this surprising phenomenon.\nOne common approach to showing that gradient descent will converge to a global optimum is to show that the loss landscape is convex, which means that a line drawn between any pair of points will never intersect its surface. This property can be used to prove that no local minima exist [2]. But unfortunately, this technique cannot be applied to neural networks, as the loss function of a neural network is not a convex function of its parameters. This can be demonstrated by a simple counterexample.\nHowever, while convexity is sufficient to prove convergence, it is not required for convergence to occur. For example, look at the plot below. On the left, we see the loss landscape for a quadratic loss on a two-dimensional parameter space. On the right, we see a loss landscape that is similar, except that it has a bulge that clearly makes it non-convex. And yet it is visually evident that neither plot has local minima. Gradient descent would optimize successfully in both settings.\nIn this article, we investigate an alternative property to convexity which we call learning health. Informally, it says that if the loss is large, the gradient must also be large. (In the plot above, both loss landscapes are healthy.)\nThe central result of this article is to use learning health to prove that gradient descent on neural networks will decrease the loss to a small value. The argument goes like this:\nThese three results allow us to conclude that the loss decays exponentially for a meaningful amount of time, and therefore is guaranteed to reach some small value. In general, we observe that the bounds we derive are improved by the size of the neural network, so when using a large enough network, the loss can be guaranteed to fall arbitrarily close to zero.\nIn this article, we apply this theory only to a very simple neural network: a 2-layer MLP trained with the L2 loss on a dataset consisting of a single datapoint. But the general theory we develop here applies more broadly. In follow-up posts, we will show that the results can also be applied to MLPs with multiple layers, trained on multiple data points, and using the cross-entropy loss instead of the L2. There is also existing literature exploring similar ideas (see e.g. [1], [3], [4]).\nThe plot below visualizes the bound we derive, juxtaposed with the actual training curve of a neural network. Every time you click the train button a new set of initial parameters, inputs, and targets are randomly initialized, and 200 steps of gradient descent training are performed. You can see for yourself that the loss always approaches 0, and that our bound is non-vacuous."
  },
  {
    "objectID": "articles/gd-minimizes-loss/index.html#notation-and-definitions",
    "href": "articles/gd-minimizes-loss/index.html#notation-and-definitions",
    "title": "Why Gradient Descent Minimizes Training Loss",
    "section": "0. Notation and Definitions",
    "text": "0. Notation and Definitions\nMuch of the theory in this article treats the space of parameters as a vector space equipped with an inner product. The parameters of a neural network tend to either be elements of \\(\\R^{n}\\) or matrices in \\(\\R^{n\\times m}\\), so we need to define suitable inner products for these two types of vectors.\nDefinition 0.1: Inner Product. For two vectors \\(x,y \\in \\R^n\\) we take the inner product to be \\(&lt;x, y&gt; = x^T y\\). For two matrices \\(M,N\\in \\R^{n\\times m}\\) we use \\(&lt;M, N&gt; = \\tr(M^T N)\\). Any inner product induces a norm \\(|\\cdot|\\) via \\(|v| = \\sqrt{&lt;v,v&gt;}\\), and you can verify that, in the \\(\\R^n\\) and \\(\\R^{n\\times m}\\) cases, this induced norm takes the following forms: \\[\\begin{align}\n|x| = \\sqrt{\\sum_i x_i^2} \\quad\\quad |M| = \\sqrt{\\sum_{ij} M_{ij}^2}\n\\end{align}\\] which are also known as the L2 norms of the respective vector spaces. \\(\\blacksquare\\)\nDefinition 0.2: Derivative. Let \\(f: V \\to W\\) be a function between two vector spaces. The derivative at a point \\(v\\in V\\) is a linear function \\(\\der f(v): V\\to W\\). It tells us how changing the input \\(v \\to v + u\\) will affect the output, if the change \\(u\\in V\\) is small. Concretely, \\[\n\\der f(v)(u) \\simeq f(v + u) - f(v)\n\\] Some people would call \\(\\der f(v)(u)\\) the directional derivative of \\(f\\) along \\(u\\) at a point \\(v\\). \\(\\blacksquare\\)\nDefinition 0.3: Path Derivative. An especially important case is when we are taking the derivative of a function \\(h:\\R \\to V\\), also known as a path through \\(V\\). Here, using the notation \\(\\der h(t): \\R \\to V\\) is a little cumbersome. Instead, we can use Newton’s notation, which defines \\(h'(t) = \\der h(t)(1)\\in V\\). This allows us to think of the derivative of \\(h\\) as a vector in \\(V\\), as opposed to a map \\(\\R \\to V\\). This is possible because any linear map of the form \\(M: \\R \\to V\\) can be turned into scalar-vector multiplication \\(M(r) = r \\: v\\) by defining \\(v = M(1)\\). \\(\\blacksquare\\)\nOne important use of the derivative of a path is to define the length and energy of the path.\nDefinition 0.4: length and energy of a path. Given a normed vector space \\(V\\) and a differentiable path \\(h: [0, t] \\to V\\), the length and energy of the path are defined as \\[\\begin{align}\n\\len(h) &= \\int_0^t |h'(s)| ds \\\\\n\\en(h) &= \\int_0^t |h'(s)|^2 ds\n\\end{align}\\] \\(\\blacksquare\\)\nDefinition 0.5: Gradient. Given a vector space \\(V\\) equipped with an inner product, the gradient of a differentiable function \\(f: V \\to \\R\\), denoted \\(\\nabla f\\), is a map \\(\\nabla f: V\\to V\\) defined so that \\(\\nabla f(v)\\in V\\) is the unique vector satisfying \\[\n\\der f(v)(u) = &lt;\\nabla f(v), u&gt; \\quad \\forall v\\in V\n\\] Whenever it is completely clear from the context which function \\(f\\) we are taking the gradient of we can use the shorthand \\(\\hat v = \\nabla f(v)\\). \\(\\blacksquare\\)\n\n\nYou might be used to a different definition of the gradient. The one you know turns out to be equivalent to the one we introduce here. Take a look at this great article for an in-depth explanation.\nDefinition 0.6: Gradient Flow. Let \\(V\\) be a vector space with an inner product. Given a differentiable and lower-bounded function \\(f: V\\to \\R\\), the gradient flow starting at \\(v_0\\in V\\) is defined as the path \\(\\gamma: [0, \\infty) \\to V\\) satisfying \\[\n\\gamma(0) = v_0  \\quad\\text{and}\\quad \\gamma'(t) = - \\nabla f\\circ \\gamma(t)\n\\] The existence of \\(\\gamma\\) follows from the existence and uniqueness of partial differential equations. The fact that \\(f\\) is lower-bounded guarantees that the solution never diverges, and so \\(\\gamma\\) is a map with domain \\([0, \\infty)\\). \\(\\blacksquare\\)"
  },
  {
    "objectID": "articles/gd-minimizes-loss/index.html#learning-with-gradient-flows",
    "href": "articles/gd-minimizes-loss/index.html#learning-with-gradient-flows",
    "title": "Why Gradient Descent Minimizes Training Loss",
    "section": "1. Learning with Gradient Flows",
    "text": "1. Learning with Gradient Flows\nDefinition 1.1: Learning Problem. The problem setup consists of a tuple \\((W, w_0, f)\\) where:\n\n\\(W\\) is the parameter space, which controls the behavior of the model. Mathematically, it is a vector space equipped with an inner product \\(&lt;\\cdot, \\cdot&gt;\\).\n\\(w_0\\in W\\) is the initial point from which the learning will proceed.\n\\(f: W \\to \\R^+\\) is the loss function, which tells us how good the parameters are (presumably at fitting some dataset). The lower the loss, the better the parameters. A loss function must be lower bounded so it’s nice to assume wlog that \\(\\inf_{w} f(w) = 0\\).\n\nOut of \\(f, W\\) and \\(w_0\\), the following objects are defined:\n\nA path \\(\\gamma: [0, \\infty) \\to W\\) is a gradient flow of \\(f\\) starting at \\(w_0\\). It describes the evolution of the parameters through time.\nA loss curve \\(\\l:\\R \\to \\R\\), defined as \\(\\l(t) = f\\circ \\gamma(t)\\), tells us the amount of loss at any moment in time.\n\n\\(\\blacksquare\\)\nThis setting is deeply connected to the learning algorithms used in practice to train neural networks. But it is worth pointing out some important differences:\n\nLearning algorithms used in practice have discrete aspects. For example, gradient descent starts with the initial parameters \\(w_0\\in W\\) and repeatedly applies the update \\(w_{t+1} = w_t - \\delta \\nabla f(w_t)\\) for some learning rate hyperparameter \\(\\delta \\in \\R^+\\). Clearly, in the limit \\(\\delta \\to 0\\) this discrete path converges to the gradient flow \\(\\gamma\\). But it is less clear whether the learning rates used in practice are small enough for the gradient flow to be a good approximation.\nModern deep learning always uses stochastic gradient descent (SGD). Instead of computing the gradient on the entire dataset, we estimate it by computing the average gradient on a small random subset of the data. These stochastic gradients result in slightly worse updates, but they are much cheaper to compute. This tradeoff is worth making because, for the same amount of compute, it allows us apply many more updates. Again, as the learning rate approaches \\(0\\), SGD converges to the gradient flow. But it is worth asking whether or not this approximation applies in practice.\nIt’s common for deep learning optimizers to include details like a momentum term, a correction for the variance, etc… each of these details is yet another reason why practical algorithms may look quite different from gradient flow.\n\nUltimately, we will want to prove guarantees for the algorithms we run in practice. But studying the behavior of gradient flows is a useful intermediate step. The rest of this section will lay out some of the key mathematical properties of gradient flows that make them so powerful when deriving learning guarantees.\nResult 1.2. The derivative of the loss curve satisfies: \\[\\l'(t) = - | \\nabla f\\circ \\gamma(t)| ^2\\]\n\n\nProof\n\nFor simplicity let \\(w=\\gamma(t)\\). Then, \\[\\begin{align}\n\\l'(t) &= \\der f(w)( \\gamma'(t))\n&\\quad &\\text{(chain rule)} \\\\\n&= &lt; \\nabla f(w), \\gamma'(t)&gt;\n&\\quad &\\text{(gradient def.)} \\\\\n&= -&lt; \\nabla f(w), \\nabla f(w)&gt;\n&\\quad &\\text{(gradient flow def.)} \\\\\n&= - | \\nabla f (w)| ^2\n\\end{align}\\]\n\nSo, under gradient flow, the magnitude of the gradient tells us how fast the loss is decreasing at any moment in time. A large gradient means fast learning! A consequence of this result is that \\(\\en(\\gamma)\\) measures how much the path \\(\\gamma\\) has managed to reduce the loss.\nResult 1.3. Let \\(f:W \\to \\R\\) be a differentiable function and \\(\\gamma:[0, t] \\to W\\) be a gradient flow of \\(f\\) as in definition 1.1. Then:\n\\[\n\\en(\\gamma) = \\l(0) - \\l(t)\n\\]\n\n\nProof\n\n\\[\\begin{align}\n\\en(\\gamma) &= \\int_0^t |\\gamma'(s)|^2 ds \\\\\n&= \\int_0^t |\\nabla f \\circ \\gamma(s)|^2 ds\n&\\quad &\\text{(gradient flow def.)} \\\\\n&= - \\int_0^t  \\l'(s) ds\n&\\quad &\\text{(result 1.2)} \\\\\n&= \\l(0) - \\l(t)\n\\end{align}\\]\n\nThus, we can reformulate questions about \\(\\l(t)\\) into questions about \\(\\en(\\gamma)\\); an interesting shift in perspective. For our purposes, the most important implication is that \\(\\en(\\gamma) \\le \\l(0)\\), i.e., the energy is bounded by the initial loss. This follows from Result 1.3 and from the assumption in Definition 1.1 that \\(f(w) \\ge 0\\).\nThe last step before we are ready to prove the main result of this section is to establish an inequality between the length and energy of a generic path (not necessarily a gradient flow).\nResult 1.4. If \\(h: [0, t] \\to V\\) is a differentiable path, then\n\\[\n\\len(h)^2 \\le t\\;  \\en(h)\n\\]\n\n\nProof\n\nJust note that \\[\\begin{align}\n\\len(h) &=  \\int_0^t |h'(s)| ds  \\\\\n&\\le  \\sqrt{\\int_0^t  1^2 ds } \\sqrt{\\int_0^t | h'(s)|^2 ds } \\;\\;\\;\\;\\;\\; \\text{(by Cauchy Schwarz)} \\\\\n&\\le\\sqrt{ t \\; \\en(h)} \\\\\n\\end{align}\\] From which the result follows.\n\nWe are finally ready to show that, in a short amount of time \\(t\\), the parameters \\(\\gamma(t)\\) cannot move very far away from the initialization.\nResult 1.5. Let \\(\\gamma\\) be the gradient flow of \\(f\\) starting at \\(w_0\\) as in Definition 1.1. Then: \\[\n|\\gamma(t) - w_0| \\le \\sqrt{t\\;\\l(0)}\n\\]\n\n\nProof\n\n\\[\\begin{align}\n|\\gamma(t) - \\gamma(0)|\n&= \\left |\\int_0^t  \\gamma'(s) ds \\right| \\\\\n&\\le \\int_0^t  \\left | \\gamma'(s) \\right| ds\n&\\quad &\\text{(triangle inequality for integrals)} \\\\\n&= \\len(\\gamma) \\\\\n&\\le \\sqrt{t \\; \\en(\\gamma)}\n&\\quad &\\text{(result 1.4)}\n\\end{align}\\]"
  },
  {
    "objectID": "articles/gd-minimizes-loss/index.html#a-replacement-for-convexity",
    "href": "articles/gd-minimizes-loss/index.html#a-replacement-for-convexity",
    "title": "Why Gradient Descent Minimizes Training Loss",
    "section": "2. A Replacement for Convexity",
    "text": "2. A Replacement for Convexity\nAs described in the introduction, our proof utilizes the concept of learning health instead of convexity, which we define the following way: a loss function \\(f : W \\to \\mathbb{R}^+\\) has learning health if there exists \\(\\alpha \\in \\R^+\\) such that for all \\(w \\in W\\), \\[\n\\frac{|\\nabla f(w)|^2}{f(w)}  \\ge \\alpha\n\\] One implication of this definition is that no local minima can exist in a healthy loss landscape. After all, the definition of a local minimum is a point \\(w\\) with no gradient but high loss, the existence of which is ruled out by the condition.\nBut the following result tells us that learning health also implies something much stronger. On a healthy loss landscape, the loss of a gradient flow is guaranteed to decay to \\(0\\) exponentially quickly.\nResult 2.1. If \\(f: W\\to \\R^+\\) satisfies \\(\\frac{|\\nabla f(w)|^2}{f(w)}  \\ge \\alpha\\) for some \\(\\alpha \\in \\R^+\\), then \\[\n\\l(t) \\le \\l(0) \\; e^{-\\alpha t}\n\\]\n\n\nProof\n\n\\[\\begin{align}\n\\frac{\\der \\ln \\l(t)}{\\der t} &= \\frac{\\l'(t)}{\\l(t)}\n= -\\frac{|\\nabla f \\circ \\gamma(t)|^2}{f\\circ \\gamma(t)}\n\\le  -\\alpha \\\\\n\\end{align}\\] so \\[\n\\ln \\l(t) - \\ln \\l(0) = \\int_0^t \\frac{\\der \\ln \\l(s)}{\\der s}  ds \\le - \\int_0^t \\alpha ds = - \\alpha t\n\\] And then \\(\\ln \\l(t)  \\le  \\ln \\l(0) - \\alpha t\\). To conclude, use the fact that exponential is a monotone function and \\[\n\\l(t) = e^{\\ln \\l(t)} \\le  e^{\\ln \\l(0) - \\alpha t} = \\l(0) e^{ - \\alpha t}\n\\]\n\nUnfortunately, neural networks don’t satisfy the learning health property for all \\(w\\in W\\). (To see that, just consider an MLP with “degenerate” parameters, where all the weight matrices are \\(0\\). That MLP will have 0 gradient, even when it has high loss.) But that does not prevent us from using learning health to derive guarantees. In Section 3 we will see that a properly-initialized 2-layer MLP does indeed satisfy a relaxed version of this property: there exists \\(\\alpha,\\beta \\in \\R^+\\) such that for all \\(w \\in W\\), \\[\n\\frac{|\\nabla f(w)|^2}{f(w)}  \\ge \\alpha - \\beta \\; |w-w_0|\n\\]\nThe main result of this section is a guarantee that can be applied to any learning problem satisfying this relaxed property.\nResult 2.2. If \\(f\\) and \\(w_0\\) satisfy \\(\\frac{|\\nabla f(w)|^2}{f(w)}  \\ge \\alpha - \\beta \\; |w-w_0|\\) for all \\(w\\in W\\), then \\[\nl(\\infty) \\le \\l(0) \\; \\exp({-\\frac {\\alpha^3}{3 \\beta^2 \\l(0)} })\n\\]\n\n\nProof\n\nThe proof follows a very similar argument to 2.1,\n\\[\\begin{align}\n\\frac{\\der \\ln \\l(t)}{\\der t} &= -\\frac{|\\nabla f(w)|^2}{f(w)} \\quad &\\text{(result 1.3)} \\\\\n&\\le - \\alpha + \\beta \\;  |w - w_0| \\\\\n&\\le - \\alpha + \\beta \\sqrt {\\l(0) t} \\quad &\\text{(result 1.5)} \\\\\n\\end{align}\\]\nIntegrating both sides from \\(0\\) to \\(t\\) we get \\[\n\\ln \\l(t) - \\ln \\l(0) \\le - \\alpha t + \\frac{2}{3}  \\beta \\sqrt {\\l(0)} \\; t^{3/2}\n\\]\nwhich implies \\[\n\\l(t) \\le \\l(0) \\exp(- \\alpha t + \\frac{2}{3}  \\beta \\sqrt \\l(0) \\; t^{3/2})\n\\]\nNow, we want to find the value of \\(t\\) that minimizes the term in the exponential. We set the derivative to \\(0\\) by solving \\(- \\alpha t + \\frac{2}{3}  \\beta \\sqrt {\\l(0)} \\; t^{3/2}=0\\) which you can easily see is achieved at \\(t^* = \\frac {\\alpha^2} {\\beta^2 \\l(0)}\\). This implies that, \\[\n\\l(t^*) \\le \\l(0) \\exp({-\\frac{\\alpha^3}{3\\beta^2 \\l(0)}})\n\\] And, since \\(\\l'(t) = -|\\nabla f \\circ \\gamma(t)|^2 \\le 0\\), the loss function is monotonically decreasing, which implies that \\(\\l(\\infty) \\le \\l(t^*) \\le \\l(0)\\exp({-\\frac{\\alpha^3}{3\\beta^2 \\l(0)}})\\), proving the result."
  },
  {
    "objectID": "articles/gd-minimizes-loss/index.html#the-simplest-neural-network",
    "href": "articles/gd-minimizes-loss/index.html#the-simplest-neural-network",
    "title": "Why Gradient Descent Minimizes Training Loss",
    "section": "3. The Simplest Neural Network",
    "text": "3. The Simplest Neural Network\nThe objective of this section is straightforward. To take the absolute simplest neural network and show that the conditions of Result 2.2 are satisfied. We’ll look at a 2-layer MLP with ReLU nonlinearity. It has two parameter matrices \\(M\\in \\R^{k\\times m}\\) and \\(N\\in \\R^{n\\times k}\\). The ReLU is denoted by \\(\\sigma\\). Given an input \\(x \\in \\R^m\\), the MLP produces the output \\[\ny = N \\sigma(M x)\n\\] It is convenient to define intermediate variables by breaking down the computation into steps. \\[\na = Mx, \\quad b=\\sigma(a) \\quad y=Nb\n\\] Parameter space. The weight vectors are pairs of matrices \\(w = (N, M)\\) and so the parameter space is \\[W= \\R^{n\\times k} \\oplus \\R^{k\\times m}\\] The loss function. In keeping with the philosophy of studying the simplest example, we’ll take the loss function to be the L2 error on a single input-target pair \\((x, q)\\in \\R^n \\oplus \\R^m\\). \\[\nf(w) = \\frac 1 2 |y - q|^2\n\\] We will need \\(x\\) to be normalized, so we just assume that \\(|x|^2 = m\\). It’s also useful to define the loss as a variable \\(L = f(w)\\).\nInitialization. The standard way to initialize a neural network is to independently sample all the entries in the weight matrices from some distribution. In our case, we use \\[\nM_{ij} \\sim \\mathcal N (0, {\\small \\frac 2 m} ) \\quad\\quad N_{ij} \\sim \\mathcal N (0, {\\small \\frac 2 k})\n\\] This is the commonly-used He initialization, which works well for networks with ReLU nonlinearities. This initialization provides us with the guarantee that, with high probability, \\[\n|b|^2 \\simeq k\n\\]\n\n\nSee a derivation of this property\n\nRemember that we are sampling the initial weight matrices via \\[\nM_{ij} \\sim \\mathcal N (0, {\\small \\frac 2 m} ) \\quad\\quad N_{ij} \\sim \\mathcal N (0, {\\small \\frac 2 k})\n\\] Since \\(M,N\\) are random variables, so are \\(a\\) and \\(b\\). He initialization uses carefully-chosen variances to ensure that the entries of the initial activations \\(a = M x\\) and \\(b = \\sigma(a)\\) are within some reasonable range. In particular, we want the entries of \\(b_i\\) to have \\(0\\) mean and variance \\(1\\). And since all the entries are independent, that will mean that \\(|b|^2 \\simeq k\\) and so \\(b\\) will be close to the sphere of radius \\(\\sqrt k\\) with high probability. (But this is only the case if the input vector is also normalized; that is why we needed the assumption that \\(|x|^2 = m\\).) We will now prove these statements.\nFirst we want to understand \\(\\E[a_i^2]\\) \\[\\begin{align}\n\\E[a_i^2]\n&= \\E[ ({\\small \\sum_j M_{ij} x_j})^2] \\\\\n&= \\E[ {\\small \\sum_j M_{ij}^2 x_j^2 + \\sum_{k\\neq j} M_{ij} x_j M_{ik} x_k } ] \\\\\n&= \\sum_j \\E[M_{ij}^2 x_j^2] \\\\\n&= \\frac 2 m |x|^2= 2\n\\end{align}\\] where we used the independence of different entries of \\(M\\) and the fact that \\(\\E[M_{ij}]=0\\). Then \\[\n\\E[|a|^2] = \\sum_i \\E[a_i^2] = 2k\n\\] Let \\(p_M\\) denote the probability distribution functions (PDFs) of the entries of \\(M_{ij}\\) (all entries have the same PDF because they are independent and identically distributed). The PDF of \\(a_i\\) will be the nested integral of \\(\\delta(a_i - M_i^T x)\\) as \\(M_{i1},\\cdots,M_{im}\\) range from \\(-\\infty \\to \\infty\\), where \\(\\delta\\) denote the Dirac delta function. \\[\np_{a}(z) = \\int_{-\\infty}^\\infty \\cdots \\int_{-\\infty}^\\infty p_M(M_{i1}) \\cdots p_M(M_{im}) \\; \\delta (z - M_i^T x)\\; d M_{i1} \\cdots d M_{im}\n\\] Recall that a distribution is symmetric if \\(p(z) = p(-z)\\). Since \\(p_M\\) is a Gaussian, it is symmetric. The next thing we need to prove is that \\(p_a\\) is symmetric too. \\[\\begin{align}\np_{a}(z) &= \\int_{-\\infty}^\\infty \\cdots \\int_{-\\infty}^\\infty p_M(M_{i1}) \\cdots p_M(M_{im}) \\; \\delta (z - M_i^T x)\\; d M_{i1} \\cdots d M_{im} \\\\\n&=  \\int_{-\\infty}^\\infty \\cdots \\int_{-\\infty}^\\infty p_M(-M_{i1}) \\cdots p_M(-M_{im}) \\; \\delta (z + M_i^T x)\\; d M_{i1} \\cdots d M_{im} \\\\\n&=  \\int_{-\\infty}^\\infty \\cdots \\int_{-\\infty}^\\infty p_M(M_{i1}) \\cdots p_M(M_{im}) \\; \\delta (-z - M_i^T x)\\; d M_{i1} \\cdots d M_{im} \\\\\n&= p_a(-z)\n\\end{align}\\] The first step uses the change of variable \\(M_{ij}\\to -M_{ij}\\), and the second one exploits the symmetry of \\(p_M\\) and \\(\\delta\\). Finally, we compute the term we care about: \\[\\begin{align}\n\\E[b_i^2] &= \\int_{-\\infty}^\\infty p_a(a_i) \\sigma(a_i)^2 da_i \\\\\n&= \\int_{-\\infty}^0 p_a(a_i) \\:0\\: da_i + \\int_0^\\infty p_a(a_i) a_i^2 da_i \\\\\n&= \\frac 1 2 \\int_{-\\infty}^\\infty p_a(a_i) a_i^2 da_i \\quad \\text{(using symmetry)}\\\\\n&= \\frac 1 2 \\E[a_i^2] \\\\\n&= k\n\\end{align}\\] We have only been working out \\(\\E[|b|^2] = k\\), but we wanted to make claims about the particular samples themselves being approximately \\(|b|^2\\simeq k\\). The rigorous way to go about doing so would be to prove statements like \\(\\Pr( k - \\epsilon \\le |b|^2 \\le k+\\epsilon) \\le \\alpha\\). But this type of argument is very tedious and adds little insight about the ideas this document is exploring, so instead, in the rest of this article we just assume that \\(|b|^2 = k\\). When the dimension \\(k\\) is large, this approximation will be very accurate.\n\nGradients. The gradients of the loss \\(\\l\\) wrt all the intermediate variables and weights are: \\[\n\\ht y = y -q, \\quad\n\\ht b = N^T \\ht y, \\quad\n\\ht a =\\der \\sigma(a)^T \\ht b, \\quad\n\\ht M = \\ht a x^T, \\quad\n\\ht N = \\ht y b^T, \\quad\n\\]\n\n\nIn this notation, a hat on top of a variable denotes the gradient of the loss function wrt that variable.\n\n\nExpand for full derivation of the gradients.\n\nNote that \\(|\\hat y |^2 = 2 \\l\\) so, when the loss is large, the gradient of \\(\\l\\) wrt \\(y\\) is large too. Ultimately we are trying to show something similar, but about gradients of \\(\\l\\) wrt \\(M\\) (in order to then apply Result 2.2.)\nBelow, we’ve written a short derivation of all these gradient formulas, but using some unconventional notation and techniques. If you find them confusing, just work out the gradients in your own way and confirm you get the same answers.\nStarting with \\(\\ht y\\), the gradient of \\(\\l\\) wrt \\(y\\). Let \\(\\dot y \\in \\R^n\\) denote an arbitrary change to \\(y\\). Then \\[\\begin{align}\n&lt;\\ht y, \\dot y&gt; =\\frac {\\der \\l} {\\der y} (\\dot y)\n&\\simeq \\frac 1 2 |y +\\dot y - q|^2 - \\frac 1 2 |y - q|^2 \\\\\n&= \\frac 1 2 ( &lt;\\dot y, y-q&gt; + &lt;\\dot y, \\dot y&gt; + &lt;y-q, \\dot y&gt; ) \\\\\n&\\simeq &lt;y-q, \\dot y&gt; \\quad \\text{(dropping the lower order term)} \\\\\n\\end{align}\\] Now, let’s see how a change in \\(b\\) affects \\(y\\). Like before, let \\(\\dot b\\) denote a change to \\(b\\). The derivative \\(\\frac {\\der y}{\\der b}(\\dot b) \\simeq N(b+ \\dot b) - M b = \\dot M b\\). So, the gradient of \\(\\l\\) wrt \\(b\\) satisfies \\[\n&lt;\\ht b, \\dot b&gt;\n= {\\frac {\\der \\l}{\\der b}(\\dot b)}\n= {\\frac {\\der \\l}{\\der y}} \\left(  {\\frac {\\der y}{\\der b}}(\\dot b) \\right)\n=  &lt;\\ht y,  {\\frac {\\der y}{\\der b}} (\\dot b)&gt;\n= &lt;\\hat y, M \\dot b&gt; = &lt;M^T \\ht y, \\dot b&gt;\n\\] Now, let’s look at \\(N\\). The derivative \\(\\frac {\\der y}{\\der N}(\\dot N) \\simeq (N+\\dot N) b - N b = \\dot N b\\). And the gradient \\[\n&lt;\\ht N, \\dot N&gt;  = &lt;\\ht y, \\dot N b&gt; = &lt;\\ht y b^T, \\dot N&gt;\n\\] Recall that, since we are taking the inner product of two matrices, we are using the trace under the hood. To see why the last step is true, just use the cyclic property of the trace. Finally, gradients of \\(a\\) and \\(M\\): \\[\\begin{gather}\n&lt;\\hat a, \\dot a&gt;\n= &lt;\\hat b, \\der \\sigma(a)(\\dot a)&gt;\n= &lt;\\der \\sigma(a)^T \\ht b, \\dot a&gt;  \\\\\n&lt;\\ht M, \\dot M&gt;\n= &lt;\\ht a, \\dot M x&gt; = &lt;\\ht a x^T, \\dot M&gt;\n\\end{gather}\\]\n\nNote that \\(|\\hat y|^2 = 2 f(w) = L\\), the gradient of the loss wrt the output, is proportional to the loss itself.\nGradient health. The gradient of the loss wrt the matrix \\(N\\) satisfies \\[\n|\\ht{N} |^2\n\\ge 2 L ( k - 2 \\sqrt {k} |w_0 - w| ) \\\\\n\\]\n\n\nExpand for activation and gradient bounds.\n\nFirst, let’s review a basic fact. Given a matrix \\(A\\in \\R^{n\\times m}\\) and \\(x\\in \\R^m\\) we have that \\(|Ax| \\le |A| \\; |x|\\). This follows form the SVD decomposition \\(A = R \\Lambda S^T\\), where \\(R,S\\) are orthogonal matrices and \\(\\Lambda\\) is a diagonal matrix with the singular values \\(\\lambda_1,\\cdots, \\lambda_n \\ge 0\\) in it’s diagonal. First, note that \\[\n|A^T A |^2 = | S \\Lambda^2 S^T|^2 = | \\Lambda^2 |^2 = \\sum_i \\lambda_i^4 \\le \\left( \\sum_i \\lambda_i^2 \\right)^2 = | \\Lambda |^4 = | R \\Lambda S^T|^4 = | A |^4\n\\] So \\[\\begin{align}\n|A x|^2 &= \\tr(x^T A^T A x) = \\tr(A^T A x x^T) = &lt;A^T A, x x^T&gt; \\\\\n&\\le  |A^T A| |x x^T| = |A^T A| |x|^2 \\\\\n&\\le |A|^2 |x|^2\n\\end{align}\\]\nNow, recall that \\(w_0 = (M_0, N_0)\\) and let \\(a_0=M_0 x, b_0=\\sigma(a_0)\\) and \\(y_0=N_0 b_0\\) be the activations at initialization. We want to derive upper bounds on \\(|a - a_0|\\) and \\(|b - b_0|\\) based on \\(|w-w_0|\\). First, \\[\\begin{align*}\n|a_0 - a| &\\le |M_0 x - Mx| \\\\\n          & = |M_0 - M| \\; |x| \\quad \\text{(using the fact we just proved)} \\\\\n          & \\le |w-w_0| \\; |x| \\\\\n          & = \\sqrt m \\; |w-w_0|\n\\end{align*}\\] where the last step used the assumption that \\(|x| = \\sqrt m\\) (the inputs are normalized). To bound \\(|b_0 - b |\\) we need to use the fact that the ReLU \\(\\sigma\\) is 1-Lipschitz. So \\[\n|b_0 - b| = |\\sigma(a) - \\sigma(a_0) | \\le |a_0 - a | \\le \\sqrt m \\; |w-w_0|\n\\]\nRecall that our weight initialization guarantees that \\(|b_0| \\simeq \\sqrt k\\) (for simplicity we assume exact equality). Now we can conclude with: \\[\\begin{align}\n| \\ht{N} |^2 &= |\\ht y b^T|^2 = \\text{trace}(b y^T y b^T)\n              = |\\ht{y}|^2 |b|^2\n              = 2 L |b|^2 \\\\\n              &\\ge L (|b_0|  - |b_0 - b|)^2\n              = 2 L ( \\sqrt k - |b_0 - b| )^2 \\\\\n              &= 2 L ( k - 2 \\sqrt k |b_0 - b| +  |b_0 - b| ^2 ) \\\\\n              &\\ge 2 L ( k - 2 \\sqrt k |b_0 - b| ) \\\\\n              &\\ge 2 L ( k - 2 \\sqrt {km} |w_0 - w| ) \\\\\n\\end{align}\\]\n\nWe could also attempt to derive a lower bound for \\(|\\ht M|^2\\), but it is not really necessary to do so. We already have enough to apply 2.2.\nLearning Guarantees. Since \\(|\\nabla f(w)|^2 = |\\ht N|^2 + |\\ht N|^2\\), the previous result implies that \\[\n\\frac{|\\nabla f(w)|^2}{f(w)} \\ge 2k - 4 \\sqrt {km} |w_0 - w|\\\\\n\\] So by setting \\(\\alpha = 2k\\) and \\(\\beta = 4 \\sqrt {km}\\), the application of Result 2.2 gives \\[\n\\l(\\infty) \\le \\l(0) \\; \\text{exp}({-\\frac {k^2}{6 m \\l(0)} })\n\\] From looking at the above equation, it is apparent that the scale of the MLP helps it learn. By growing \\(k\\) we can very quickly guarantee that the loss is decreased to any desired value.\nBelow is an interactive visualization that can be played with to see how this simple 2-layer MLP learns. The red line shows the bound we’ve just derived."
  },
  {
    "objectID": "articles/rnn-state-scaling/index.html",
    "href": "articles/rnn-state-scaling/index.html",
    "title": "RNN State Scaling",
    "section": "",
    "text": "TODO write intro\nIn this post we go over how to think about RNN state size. We find an architecture, symmetric power, that looks very promising."
  },
  {
    "objectID": "articles/rnn-state-scaling/index.html#background",
    "href": "articles/rnn-state-scaling/index.html#background",
    "title": "RNN State Scaling",
    "section": "1. Background",
    "text": "1. Background\nWe begin by introducing some conceptual scaffolding to connect important ideas around transformers and RNNs.\nAll auto-regressive sequence models can be thought of as state machines (SMs). A state machine is a generic abstract model for any system that takes in an input and a state, and gives an output and updated state. In the case of neural language modeling, the inputs are typically tokens, the outputs are typically next-token distribution predictions, and the states are typically arrays of floating-point numbers.\nLet \\(\\mathcal{X}, \\mathcal{Y}, \\mathcal{S}\\) denote the input space, output space, and state space, respectively. A state machine is a triplet \\((S_0, \\delta, \\rho)\\), where \\(S_0 \\in \\mathcal{S}\\) denotes the initial state, \\(\\delta \\in (\\mathcal{S} \\times \\mathcal{X}) \\to \\mathcal{S}\\) denotes the state transition function, \\(\\rho \\in (\\mathcal{S} \\times \\mathcal{X}) \\to \\mathcal{Y}\\) denotes the output function, and \\[\n\\begin{aligned}\nS_t &:= \\delta(S_{t-1}, X_t)\\\\\nY_t &:= \\rho(S_{t}, X_t)\\\\\n\\end{aligned}\n\\] where \\(X_t\\) and \\(Y_t\\) denote the input and output at time \\(t\\), respectively.1 This simple but powerful abstraction encapsulates every modern language model, including models based on the standard softmax-based transformer, RNNs (e.g. LSTMs, GRUs, linear transformers), and state-space models like Mamba. At first glance, it may be surprising that transformers are included in this list; transformer language models are not typically presented as state machines. See Appendix A.1. for a constructive proof.\nWe are further interested in a more specific concept: a finite state machine (FSM). We use \\(\\operatorname{size}\\) to denote the function which returns the dimensionality of an object. In the context of this article, we define a finite state machine as a state machine with the property that there exists \\(\\varsigma \\in \\mathbb{N}\\) such that \\(\\operatorname{size}(S) \\leq \\varsigma\\) for all \\(S \\in \\mathcal{S}\\). We call \\(\\varsigma\\) the state size.2\nIt is easy to see that the prototypical RNN is a finite state machine: each state \\(S \\in \\mathcal{S}\\) is a fixed-and-finite-dimensional array.3 Furthermore, it can be shown that transformers are not finite state machines (see Appendix A.2). Intuitively, this is because the state size of a transformer grows with the length of its input history, so no fixed \\(\\varsigma\\) is larger than every possible states.\nThis paints a clear picture of the relationship between transformers and RNNs. The two approaches can be unified under the umbrella of state machines, but the crucial distinction between them is that transformers do not have a finite-sized state. From this perspective, it becomes clear that increasing the state size of an RNN will make it more similar to a transformer (and in fact, in Sections 2.2 and 2.5 we give families of RNNs that converge exactly to the transformer in the limit of increasing state size). Noting the performance gap between transformers and RNNs motivates a research question: can the performance of an RNN be improved by increasing its state size?\nThe remaining sections of article will answer that question in the affirmative, showing that the performance of RNNs with large states matches, and sometimes surpasses, that of transformers."
  },
  {
    "objectID": "articles/rnn-state-scaling/index.html#state-scaling-of-rnns",
    "href": "articles/rnn-state-scaling/index.html#state-scaling-of-rnns",
    "title": "RNN State Scaling",
    "section": "2. State Scaling Of RNNs",
    "text": "2. State Scaling Of RNNs\nLet’s take a look at some sequence modeling architectures through the lens of state size scaling. The table below gives an overview of what we will discover.\n\n\n\n\n\n\n\n\n\n\nRNN Architecture\nAdjustable State-Weight Ratio\nUnlimited Effective Context Window\nConverges To Transformer\nState-Weight Ratio\n\n\n\n\nFeedforward RNN\n✗\n✓\n✗\n\\(\\frac{1}{2d}\\)\n\n\nWindowed Transformer\n✓\n✗\n✓\n\\(\\frac{w}{6d + 1}\\)\n\n\nLinear Transformer\n✗\n✓\n✗\n\\(\\frac{d/head}{12d + 2}\\)\n\n\nSplit-Product Transformer\n✓\n✓\n✗\n\\(\\frac{(d/(head\\cdot s))^s}{12d + 2}\\)\n\n\nTaylor Transformer\n✓\n✓\n✓\n\\(\\frac{\\sum_{i=0}^{D}\\binom{\\frac{d}{head} + i - 1}{i}}{12d + 2}\\)\n\n\nSymmetric Power Transformer\n✓\n✓\n✗\n\\(\\frac{\\binom{\\frac{d}{head} + D - 1}{D}}{12d + 2}\\)\n\n\n\nAll architectures we discuss in this article have the same high-level structure. Letting \\(d \\in \\mathbb{N}\\) denote the width of a network and \\(\\ell \\in \\mathbb{N}\\) to denote the depth, each architecture has an embedding layer mapping tokens to \\(\\mathbb{R}^d\\), followed by \\(\\ell\\) copies of an RNN with \\(\\mathcal{X} = \\mathcal{Y} = \\mathbb{R}^d\\), and finally an outbedding layer mapping \\(\\mathbb{R}^d\\) to next-token distributions. Since this structure is shared, the section studying each article focuses only on the inner RNN, where we use \\(s\\) to denote the number of splits in Split-Product Transformer, and use \\(D\\) to denote the degrees of power in Taylor Transformer and Symmetric Power Transformer.\n\n2.1. Feedforward RNN\nA natural place to begin is with the classic feedforward RNN layer, defined as:\n\\[\n\\begin{aligned}\nS_0 &:= \\textbf{0} \\in \\mathbb{R}^d\\\\\n\\delta(S_{t-1}, X_t) &:= \\sigma(W_{\\delta}(S_{t-1} + X_t))\\\\\n\\rho(S_{t}, X_{t}) &:= W_{\\rho}S_t\n\\end{aligned}\n\\]\nwhere weights \\(W_{\\delta}, W_{\\rho} \\in \\mathbb{R}^{d \\times d}\\) and \\(\\sigma\\) denotes a nonlinearity like ReLU.\nSince \\(S \\in \\mathbb{R}^d\\) for all \\(S \\in \\mathcal{S}\\), the state size of each layer of this network is \\(d\\). The overall state size is therefore \\(d\\ell\\). If we want to increase the state size, we need to increase the width \\(d\\) or the depth \\(\\ell\\). But notice that, since the parameter count is \\(2d^2\\ell\\), increasing the width or depth also increases the number of parameters. This is problematic for two reasons, one scientific and one practical.\nScientifially, we were hoping to answer the question of does increasing the state size improve performance? This motivates an experimental design where we increase the state size and measure the change in performance. But if, when we increase state size, we also increase parameter count, our experiment becomes unable to disentagle the impact of increasing the size of the state from the familiar performance boost that comes with increasing the model scale.\nOn the practical side, the concern is best illustrated by highlighting a quantity we call the state-weight ratio. For the classic RNN, this is \\(\\frac{d\\ell}{2d^2\\ell} = \\frac{1}{2d}\\). This ratio is small and we have no way of increasing it. As we increase the state size by increasing \\(d\\), this architecture will actually spend a smaller fraction of its memory on its state. In order to design an RNN to have a similar parameter scaling law to that of a transformer, we need it to have not only large states, but a large state-weight ratio.\n\n\n2.2. Windowed Transformer\nIn the this section, we study an RNN architecture that allows us to freely adjust the state-weight ratio. The formula for a transformer layer is:\n\\[\nY^\\text{Transformer}_t = \\sum_{i=1}^t e^{Q^T_t K_i} V_i\n\\]\nIn Appendix A.1, we construct an equivalent state machine with \\(S_t = (K_{i=1}^t, V_{i=1}^t)\\). This means that \\(\\operatorname{size}(S_t) = 2td\\ell\\), which, being dependent on \\(t\\), has no upper bound.\nHowever, simply truncating the state beyond a certain size straightforwardly leads to a transformer-like RNN, which we call a windowed transformer. The truncation is given by a window size hyperparameter, which we denote \\(w \\in \\mathbb{N}\\). Its formula is:\n\\[\nY^\\text{WindowedTransformer}_t = \\sum_{i=t-w}^t e^{Q^T_t K_i} V_i\n\\]\nIt is easy to see that this is a state machine with \\(S_t = (K_{i=t-w}^t, V_{i=t-w}^t)\\), and is therefore a finite state machine with state size \\(2wd\\ell\\). The implementation is almost exactly the same as for a regular transformer, but in addition to a causal mask, it also includes an “ancient history mask”.\n[Visualization]\n\n\nIt is interesting to note that a naive implementation of windowed transformer attention (one which materializes the entire attention matrix) has cost \\(O(T^2)\\) to process a sequence of length \\(T\\) – just like a regular transformer. Running this architecture in \\(O(T)\\) requires an efficient fused implementation which avoids materializing (or even computing) segments of the attention matrix that are entirely masked out.\nBy adjusting \\(w\\), we can control the state size without affecting the parameter count. Using a GPT-2-style architecture, the (non-embedding) parameter count for a transformer is \\((2d + 12d^2)\\ell\\) (see Appendix A.3 for a breakdown). The state-weight ratio is therefore \\(\\frac{2wd\\ell}{(2d + 12d^2)\\ell} = \\frac{w}{6d + 1}\\). Any desired state-weight ratio can be achieved.\nThis gives us the ability to empirically observe the state scaling behavior of the windowed transformer. All experiments in this article were performed using a 124M parameter GPT-2 architecture, on LongCrawl64 with a context length of 4096.\n[PLOT]\n[JAX code]\nAs expected, we see that state scaling improves performance.\nSince the documents in our training set have context length of 4096 tokens, setting \\(w = 4096\\) causes a windowed transformer to exactly match the performance of the baseline transformer. Also, there is no value in increasing the state size beyond \\(w &gt; 4096\\). (On a longer-context dataset, there would be.)\nOne important perspective on windowed transformers is that they are a family of RNNs that converge to the regular transformer, in the limit as \\(w \\to \\infty\\). In Section 2.5, we will see another such family, which approaches the regular transformer but in a very different way.\n\n\n2.3. Split-Product Transformer\nIt is intuitively clear that a limitation of the windowed transformer architecture discussed in the previous section is that it has a limited effective context window, meaning that there are some tokens in the context which do not in any way impact the model’s prediction. A windowed transformer with \\(\\ell\\) layers and a window size \\(w\\) takes into account only the most recent \\(\\ell(w - 1)\\) tokens. We will show empirically in Section 3.2 how this limitation can impact performance. This motivates us to continue exploring the space of architectures, to find one that has both an unlimited effective context window (as the feedforward RNN has) and an adjustable weight-state ratio (as the windowed transformer has).\nThe linear transformer from our previous article is not that architecture, but it gives us a good starting point. The formula for the linear transformer is:\n\\[\nY_t^\\text{LT} = \\sum_{i=1}^t Q^T_t K_i V_i\n\\]\nThis can be generalized slightly by introducing \\(\\phi : \\mathbb{R}^d \\to \\mathcal{Z}\\), an embedding function which maps keys/queries into an inner product space \\(\\mathcal{Z}\\). The nice properties of linear transformers that we demonstrated in our previous article (specifically, the existence of an \\(O(t)\\) state algorithm) remain universally present for architectures in this more general family (i.e. for any choice of \\(\\phi\\)).\n\\[\nY_t^\\text{GenericLT} = \\sum_{i=1}^t \\langle \\phi(Q_t),  \\phi(K_i) \\rangle V_i \\\\\nY_{t}^\\text{GenericLT} = S_t \\phi(Q_t) \\qquad S_t = S_{t-1} + V_t  \\phi(K_t)^T\n\\]\nOf course, this reformulation is superficial.4 If we consider \\(\\phi(K_i)\\) to be part of the broader layer rather than the attention calculation, this reduces back to the original equations of the linear transformer. The advantage of thinking of \\(\\phi\\) as part of the transformer itself is that it allows us to study variants of \\(\\phi\\) while keeping the rest of the network exactly the same. In particular, we will be looking at choices of \\(\\phi\\) where \\(z \\gg d\\), which we call state expansions. State expansions allow us to freely adjust the state-weight ratio, and we will see that larger state expansions generally improve performance.\nWhat state expansion should we pick? It’s wide open, and many things have been tried [TODO cite stuff]. In this article, we focus on state expansions that leverage the tensor product, an approach that we consider to be particularly mathematically elegant. Intuitively, the tensor product can be seen as a generalization of the outer product to higher-order objects. (See Appendex A.4 for a more complete introduction to the topic.)\nHere is a straightforward way to use the tensor product to define a state expansion. Given a vector-valued key (or, wlog, query), first split the key into \\(p \\in \\mathbb{N}\\) evenly-sized pieces, and then take the tensor product of those pieces. Formally, we define the split-product \\(\\phi\\) as:\n\\[\n\\chi_i(K_t) = \\text{softplus}(K_t)_{\\left[\\frac{(i-1)d}{p} : \\frac{id}{p}\\right]} \\qquad \\phi^\\text{SplitProduct}(K_t) = \\bigotimes_{i=1}^p \\chi_i(K_t)\n\\]\nWith this embedding function, \\(\\phi^\\text{SplitProduct}(K_t) \\in \\mathbb{R}^{\\left(\\frac{d}{p}\\right)^p}\\). There is no way to represent this besides representing each component of \\(\\phi^\\text{SplitProduct}(K_t)\\) so the state size is \\(\\left(\\frac{d}{p}\\right)^p\\). (PROOF!?!!) As we increase \\(p\\), we get exponential growth in state size.\nTo implement the attention algorithm for \\(\\phi^\\text{SplitProduct}\\), there is no need to materialize the expanded state. Simply split both the key and the query, do a dot product on each piece, and then multiplying the results together. (PROOOF?)\n\\[\nY_t^\\text{SplitProduct} = \\sum_{i=1}^t \\left( \\prod_{i=1}^p \\chi_i(Q_t)^T \\chi_i(K_t) \\right) V_t\n\\]\nLet’s run some experiments to investigate the effect of the split-product state expansion on performance.\n[PLOT]\n[JAX code]\nAs expected, we see that state scaling improves performance.\n\n\n2.4. Symmetric Power Transformer\nThe split-product approach is a straightforward enough application of the tensor power, but in some ways it is a bit awkward: splitting up the keys means that \\(p\\) is required to divide \\(d\\) evenly, and the possibility of negative dot-products means we still need to use the softplus to push everything into the positive quadrant in order to normalize later.\nThere is a somewhat nicer variation, which we call this the symmetric power expansion. In this approach, we simply tensor the key with itself an even number of times. This obviates the need for awkward splitting, and guarantees that each dot-product term will be positive even when keys are not projected into the positive quadrant. The symmetric power formula is:\n\\[\n\\phi^\\text{SymmetricPower}(K_t) = \\bigotimes_{i=1}^p K_t\n\\]\nNaively, we might assume that the size of this state is \\(d^p\\). But actually, we can exploit symmetry: many terms are repeated. The real state size is \\(\\binom{d + p - 1}{p}\\). (PROOF!!!!!)\nThe attention algorithm for the symmetric power is pleasingly straightforward. Simply replace exponentiation operation with raising-to-\\(p\\):\n\\[\nY_t^\\text{SymmetricPower} = \\sum_{i=1}^t \\left( \\chi_i(Q_t)^T \\chi_i(K_t)^p \\right) V_t\n\\]\nLet’s run some experiments to investigate the effect of the symmetric power state expansion on performance.\n[PLOT]\n[JAX code]\nAs expected, we see that state scaling improves performance. Impressively, it matches the performance of the softmax baseline by \\(p=4\\), and surpasses it slightly at \\(p=6\\).\n\n\n2.5. Transformer (Taylor’s Version)\nThe previous two approaches each have an adjustable state-weight ratio and no maximum effective context size. But both are still missing one cool aspect of the windowed transformer: the guarantee that, in the limit of large state, it approaches the transformer. This third property is not needed for good performance, so a purely-practical-minded person might not care. But it is deeply insightful to think about what such an architecture would look like.\nStart off with a linear transformer, and let \\(\\phi\\) equal the first \\(n\\) terms of the Taylor series approximation of the exponential. This is the the sum of \\(n\\) different symmetric powers, so the state size is \\(\\sum_{p=1}^{n} \\binom{d + p - 1}{p}\\). (PROOF!!!!)\nAs we increase \\(n\\), the state size of the model grows, and we approach the original transformer.\nThis gives a second perspective on the idea that a transformer is a state machine, but not a finite state machine. Rather than thinking about the transformers as having a growing state (as in the KV cache perspective), we can also think about transformers as having an infinite-dimensional state."
  },
  {
    "objectID": "articles/rnn-state-scaling/index.html#comparing-state-scaling-curves",
    "href": "articles/rnn-state-scaling/index.html#comparing-state-scaling-curves",
    "title": "RNN State Scaling",
    "section": "3. Comparing State Scaling Curves",
    "text": "3. Comparing State Scaling Curves\nWe’ve gone on a wonderful journey though architecture land, looking at lots of qualitative differences between them. But now it is almost time to answer the question on everyone’s mind: which architecture is the best?\nBefore diving in, let’s take one more moment to reflect on what we are looking for in an architecture.\n\n3.1. Why State Scaling?\nUltimately, the goal of any training run is to reach the best performance given training budget. The most relevant metric would therefore be to directly measure the impact of this increased cost by comparing models on the basis of their performance-per-GPU-hour. But this sort of comparison is difficult to get right. Firstly, performance-per-GPU-hour is highly implementation-dependent. Without an efficient fused kernel for the chunked algorithm, any RNN will inevitably fall far short of its potential; but it is challenging and time-consuming to implement these kernels. Secondly, compute-optimal performance relies on jointly choosing the state size with the model size, context length, etc., a process which requires an expensive sweep over various combinations.\nIn light of these difficulties, it is convenient to have a simpler experimental design. As we have seen, increasing the state size improves performance per step of training. It also increases the cost, since a larger state means more computation per step. The computation per step is roughly proportional to the size of the state for most relevant architecutures, since each dimension of the state needs to be read, processed, and written at least once.5 State size is therefore a reasonable heuristic to use in place of wall-clock time, and so it makes sense to focus on architectures which have high growth in performance per dimension of state.\nSince the size of the state for any architecture can be computed in closed form, we do not even need to implement the state or chunked algorithms in order to assess quality on the basis of state size scaling laws. A simple attention implementation is all that is needed. This makes it easy to quickly iterate on architectural chocies, and only implement the difficult chunked algorithm for architectures that seem promising.\nTo summarize: our goal in this section is to identify which architectures (if any) we want to spend our time implementing efficiently, by comparing their state scaliing laws.\n\n\n3.2. Empirical Results\nBelow on the left you can see the state size scaling curves for all the architectures we described today. On the right, click the names of individual architectures to see their training curves.\n[PLOT]\nThere are two clear winners here: the windowed transformer and the symmetric power transformer.\n\n\n3.3. Data Dependence\nAn important consideration for these experiments is the choice of dataset. In theory this is always the case: for example, when Chinchilla computes its optimal model sizes, it’s doing so on the basis of scaling laws extrapolated for one particular dataset, and these may or may not hold on other datasets. Fortunately, for the vast majority of deep learning work, it really does not seem to matter. Evidence seems to indicate that the ordering of architectures is pretty consistent accross datasets (TODO CITE THIS), and it’s difficult to see how one might intentionally construct datasets that are more advantageous to one architecture vs another.\nWhen it comes to RNNs, this is not the case. There are some strong and easily visible data dependence effects. An obvious one in this case is the fact that the windowed transformer has a limited effective context length, whereas other architectures do not. When most of the information needed to make a prediction is local, the windowed transformer is great. But when the information is further away in the sequence, it does worse.\nIt seems natural language is, for the most part, pretty local, and so the windowed transformer is mostly able to crush other architectures on LongCrawl64. But we can construct an aritificial dataset where this is not the case. Simply take LongCrawl64 and insert dummy characters between each pair of consecutive tokens (increasing the overall context length by the commensurate amount). Intuitively, this will preserve the information in the sequence, so no prediction is made more challenging for models with no limitation on maximum effective context size. But it will make the information less dense, meaning less information will be accessible within the effective context window of the windowed transformer.\nWe can run this experiment to validate empirically. We train a windowed transformer and symmetric power transformer on a sequence of datasets transformed in this way.\n[PLOT]\nAs expected, we see the windowed transformer get worse and worse, but the symmetric power transformer stays strong.\nThe takeaway here is that you should be careful before generalizing a ranking of architectures across all datasets. Different RNNs can be more or less suitable for different datasets."
  },
  {
    "objectID": "articles/rnn-state-scaling/index.html#conclusion",
    "href": "articles/rnn-state-scaling/index.html#conclusion",
    "title": "RNN State Scaling",
    "section": "4. Conclusion",
    "text": "4. Conclusion\nTODO write conclusion\nThanks for reading idk we might not need this. Plz subscribe\n\n  \n  Subscribe to be notified of new posts:"
  },
  {
    "objectID": "articles/rnn-state-scaling/index.html#footnotes",
    "href": "articles/rnn-state-scaling/index.html#footnotes",
    "title": "RNN State Scaling",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAll of our architectures grow in FLOPs linearly with state size. One could imagine RNN architectures that process each element of their state \\(n^2\\) or more, in which case it might make more sense to modify the state scaling setup accordingly.↩︎\nThe more typical definition, \\(|\\mathcal{S}| &lt; \\infty\\), follows directly from ours if it is assumed that all elements of each \\(S \\in \\mathcal{S}\\) are themselves finite. Since we are only really interested in algorithms that we can implement on computers, and floating point values have finite precision, we are happy to make this simplifying assumption.↩︎\nRNN is a term that is widely used but rarely rigorously defined, and so there is not complete agreement on its definition. In some obscure parts of the literature people may have used the term “RNN” to describe an architecture that is not a finite state machine (as defined here). But we think that the definition we use here is the most widely understood. In any case, we aim only to describe a internally-consistent set of concepts, it is OK if we are not consistent with the whole of the external literature.↩︎\nIn fact, a careful read of our previous article will reveal that we already used this more general form without explicitly commenting on it, by setting \\(\\phi(Q_i) = \\text{softplus}(Q_i)\\).↩︎\nAll of our architectures grow in FLOPs linearly with state size. One could imagine RNN architectures that process each element of their state \\(n^2\\) or more, in which case it might make more sense to modify the state scaling setup accordingly.↩︎"
  },
  {
    "objectID": "articles/symmetric-power-transformers/index.html",
    "href": "articles/symmetric-power-transformers/index.html",
    "title": "Symmetric Power Transformers",
    "section": "",
    "text": "\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\Z}{\\mathbb{Z}}\n\\newcommand{\\N}{\\mathbb{N}}\n\\newcommand{\\sft}{\\text{softmax}}\n\\newcommand{\\List}{\\text{List}}\n\\newcommand{\\Seq}{\\text{Seq}}\n\\newcommand{\\SeqT}{\\text{SeqT}}\n\\newcommand{\\CSeqT}{\\text{CSeqT}}\n\\newcommand{\\Dist}{\\text{Dist}}\n\\newcommand{\\SM}{\\text{SM}}\n\\newcommand{\\Fn}{\\text{Fn}}\n\\newcommand{\\Tok}{\\text{Tok}}\n\\newcommand{\\Aij}{ A_{[i,j]}}\n\\newcommand{\\ten}{\\small\\text{tensor}}\n\\newcommand{\\sym}{\\small\\text{symmetric}}\n\\newcommand{\\flat}[1]{\\text{flat}\\left(#1\\right)}\n\\newcommand{\\stab}{\\text{stab}}\n\\newcommand{\\orbit}{\\text{orbit}}\n\\]\nLinear transformers [1] can be formulated as linear-cost RNNs, which have better theoretical context scaling than ordinary transformers. In our previous article [2], we presented an efficient chunked algorithm that turns this theoretical advantage into practical speedups when the context is long: 10x faster training for a 64k-token context. Unfortunately, we also found that vanilla linear transformers suffer from degraded performance, especially at long contexts, rendering any benefits from the speedup useless. This article advances our previous discussion by introducing a linear transformer variant that solves the degraded performance issue while still enabling an efficient linear-cost implementation.\nBehind all the algorithms explored in this post there is a central idea: for RNNs, thinking about the size of the model purely in terms of the number of parameters misses something important. An RNN encodes all the information from the past inputs \\(X_1,...,X_{t}\\) into a finite-dimensional vector \\(S_t\\) called the state. If the states are too small, the model will struggle to store all the information it will later require. Could this be the cause of the poor performance of the GPT-2-style linear transformers we evaluated in our previous article? If we look at the state sizes, we notice they are many orders of magnitude smaller than the weights of the architecture:\nFortunately, since the architecture is a linear transformer, this imbalance has a straightforward remedy. The size of the state of a linear transformer can be controlled by simply embedding the keys and queries in a higher-dimensional space. (The larger the space in which we embed the key, the larger the state becomes). Previous work [3]–[10] has already observed that this improves the performance of linear transformers, but the resulting architectures are still not competitive with standard transformers.\nIn this article we introduce symmetric power transformers, which are a variant of linear transformers with an embedding function based on the theory of symmetric tensors. They have a hyperparameter \\(p\\) that controls the state size. For \\(p=4\\) and above, they outperform the transformer baseline, and at \\(p=4\\) and below, they have a state size small enough to fit on a modern GPU. A second advantage is that, unlike other variants of linear transformers, one can combine symmetric power transformers with the commonly used rotary embeddings [11]. Below, we see a performance comparison (full experimental details are given in Section 2).\nIn this article, our experiments used the attention formulation of linear transformers (with \\(O(t^2)\\) cost) rather than the more efficient chunked formulation (with \\(O(t)\\) cost). This allowed us to validate the learning ability of the architecture without writing custom CUDA kernels (which an efficient implementation of chunked symmetric power transformers requires). We will release the efficient chunked implementation in an upcoming article."
  },
  {
    "objectID": "articles/symmetric-power-transformers/index.html#linear-transformers-with-embeddings",
    "href": "articles/symmetric-power-transformers/index.html#linear-transformers-with-embeddings",
    "title": "Symmetric Power Transformers",
    "section": "1. Linear Transformers with Embeddings",
    "text": "1. Linear Transformers with Embeddings\nWe begin with a review of linear transformers. The inputs to a transformer layer are sequences of \\(Q_i, K_i, V_i \\in \\R^d\\) of queries, keys, and values, where \\(i\\) ranges from \\(1\\) to the sequence length \\(t\\). The outputs are a sequence \\(Y_i\\in \\R^d\\). The formula for the output vectors is: \\[\nY_i = \\sum_{j=1}^i A_{ij} V_j \\qquad A_{ij}  = \\frac{ \\phi(Q_i)^T \\phi(K_j)}{\\sum_{k=1}^i \\phi(Q_i)^T \\phi(K_k) }\n\\] where \\(\\phi : \\R^d \\to \\R^D\\) is an embedding function that maps keys or queries into vectors of dimension \\(D\\). This formulation is what we call the attention formulation of a linear transformer, because it involves explicitly computing the attention scores used to weight the values \\(V_j\\).\n\n\nThis linear transformer is the same architecture as in our previous article, but here we present the complete formula in its full generality, including both the normalizing term and embedding function (previously suppressed for clarity).\nThe exact same outputs can be computed via a recurrent formulation: \\[\nY_{i} = \\frac{S_i \\phi(Q_i)}{Z_i \\phi(Q_i)} \\qquad Z_i = Z_{i-1} + \\phi(K_i)^T \\qquad S_i = S_{i-1} + V_i \\phi(K_i)^T\n\\] where \\(Z_0\\) and \\(S_0\\) are \\(\\mathcal 0\\) vectors in their respective spaces. Since \\(S_i \\in \\R^{d \\times D}\\) and \\(Z_i \\in \\R^{D}\\), the size of the state is \\(D(d+1)\\).\n\n\n\nNote that \\(D(d+1)\\) gives the size of the state for any one linear transformer head. To compute the state size for an entire multi-head architecture, one must multiply by the number of layers and number of heads per layer.\n\n\n\nExpand for a derivation of the recurrent formulation.\n\nIf we expand the recurrent the definitions of \\(S_i\\) and \\(Z_i\\) we get that \\[\nZ_i = \\sum_{j=1}^i \\phi(K_j)^T \\qquad S_i = \\sum_{j=1}^i V_j \\phi(K_j)^T\n\\] Then, starting with the attention formulation \\[\n\\begin{aligned}\nY_i &= \\sum_{j=1}^i \\frac{ \\phi(Q_i)^T \\phi(K_j)}{\\sum_{k=1}^i \\phi(Q_i)^T \\phi(K_k) } V_j \\\\\n    &= \\sum_{j=1}^i V_j \\frac{ \\phi(K_j)^T \\phi(Q_i)}{\\sum_{k=1}^i \\phi(K_k)^T \\phi(Q_i) } \\\\\n    &= \\frac{ \\left( \\sum_{j=1}^i V_j  \\phi(K_j)^T \\right) \\phi(Q_i)}{ \\left(\\sum_{m=1}^i \\phi(K_m)^T \\right) \\phi(Q_i) } \\\\\n    &= \\frac{S_i\\phi(Q_i)}{Z_i\\phi(Q_i)} \\\\\n\\end{aligned}\n\\]\n\nThese two forms give rise to a variety of algorithms for training linear transformers, with differing computational properties. Read our earlier article on linear transformers for a detailed explanation. In particular, there are two algorithms that are relevant to our present discussion: parallel attention and chunked. The parallel attention algorithm is the standard algorithm used to train transformers. It does not require using the state, but its cost is \\(O(t^2d)\\), where \\(t\\) is the sequence length. The chunked algorithm can be used to train only linear transformers. It has a cost of \\(O(tdD)\\), and it requires materializing the state. The chunked algorithm is primarily what makes linear transformers interesting, because it is what allows them to be trained much more efficiently than softmax transformers when \\(t\\) is large.\n\n\nMaterializing an object refers to storing it in the GPU main memory. Sometimes, a mathematical object is necessary to perform a computation, and yet one can avoid having to store the whole thing in RAM. The most prominent example of this is Flash Attention [12], which avoids having to materialize the [t,t] attention matrix.\nWith these computational considerations in mind, how should we choose \\(\\phi\\)? Here are some attributes that we want:\n\nAdjustable dimensionality. To balance the size of the state with the size of the weights, there should be some hyperparameter controlling the dimension \\(D\\).\nEfficient dot product. In certain parts of the algorithm, \\(\\phi(Q_i)\\) and \\(\\phi(K_j)\\) appear as intermediate steps in the computation of \\(\\phi(Q_i)^T\\phi(K_j)\\). For some choices of \\(\\phi\\), there is a more efficient formula for \\(\\phi(Q_i)^T\\phi(K_j)\\) that does not require computing these intermediate objects.1\nPositive dot product. We want \\(\\phi(Q_i)^T \\phi(K_j)\\) to always be positive. This ensures that each normalized output \\(Y_i\\) is a convex combination of all preceding values \\(V_1, \\cdots, V_i\\). We found this to be essential for stable and performant learning.\nCompatible with rotary positional encoding. Rotary encodings take \\(Q_i \\to R Q_i\\) and \\(K_j \\to R K_j\\). To preserve their translational symmetry, we want \\(\\phi(R Q_i)^T \\phi (R K_j) = \\phi(Q_i)^T \\phi (K_j)\\).\n\nSubject to these four constraints, we are searching for the embedding function with the best empirical performance at each state size. Since our ultimate goal is to replace transformers (trained with attention) with linear transformers (trained with the chunked algorithm), the bar for success is simple: an embedding whose performance matches that of a transformer baseline at a state size small enough to be tractable. We’ll use 80 GB as our limit for the state size because that is the entire memory capacity of A100 and H100 GPUs.2\nMany possible embedding functions have already been investigated in the literature [3]–[6], but we are not aware of one that satisfies all of our requirements. In the next section, we’ll describe a variant of attention whose empirical performance is competitive with softmax attention. In the sections that follow, we will show that this can be implemented via a linear transformer that satisfies our desiderata."
  },
  {
    "objectID": "articles/symmetric-power-transformers/index.html#experiments",
    "href": "articles/symmetric-power-transformers/index.html#experiments",
    "title": "Symmetric Power Transformers",
    "section": "2. Experiments",
    "text": "2. Experiments\nAt first, it might not be evident that the following attention layer corresponds to a linear transformer, but it should be clear that it is a reasonable modification to the standard softmax transformer. Let \\(p\\in\\N\\) be even, \\[\nY_i = \\sum_{j=1}^i A_{ij} V_j \\qquad A_{ij}  = \\frac{ (Q_i^T K_j)^p}{\\sum_{k=1}^i (Q_i^T K_k)^p }\n\\] A classic softmax transformer would guarantee that all the attention \\(A_{i1}, \\cdots, A_{ii}\\) are positive and sum to one by applying the exponential \\(e^{Q_i^T K_j}\\). Instead, this variant does \\((Q_i^T K_j)^p\\) to achieve the same result. Raising each inner product to an even power makes the term positive, and dividing by the sum ensures each row of the attention is a distribution. We call this variant even-power attention, it has been studied before in the literature [5].\nLike softmax attention, even-power attention is compatible with rotary embeddings. A key motivation for using rotary embeddings is that they are relative, meaning that only the difference in time \\(i-j\\) influences the attention score \\(A_{ij}\\). See [11] for the full details, but in short, the relative property of rotary embeddings is guaranteed when the attention scores \\(A_{ij}\\) are unchanged by the rotation of \\(Q_i\\) and \\(K_j\\) by the same rotation matrix \\(R \\in \\R^{d \\times d}\\). This holds for even-power attention: \\[\n\\left( (R Q_i)^T R K_j \\right)^p =  \\left( Q_i^T R^T R K_j \\right)^p = \\left( Q_i^T K_j \\right)^p\n\\] Many other variants of linear transformers do not have this property [1].\nHere is a simple JAX implementation of even-power attention:\ndef even_power_attention(Q, K, V, p):\n    # even only\n    assert p % 2 == 0\n    # compute inner products\n    C = Q @ K.T\n    # raise to power\n    B = D**p\n    # apply causal mask\n    B = where(tril(ones(B.shape)), B, 0)\n    # project to simplex\n    A = B / B.sum(-1, keepdims=True)\n    # compute output\n    Y = A @ V\n    return Y\nThis implementation turns out to be more pedagogical than practical, because numerical stability is an important empirical consideration. Expand below for an implementation that addresses these issues.\n\n\nNumerically-stable implementation of power attention.\n\nNumerical instabilities come from numbers underflowing (too small) or overflowing (too large). Solutions typically fall into a few main categories:\n\nMake sure a number is not too small or too large, e.g. turn log(x) into log(x + ε). This prevents overflow.\nAccumulate in fp32. When accumulating a long list of small values in half-precision, it is sometimes the case that each addition will underflow and no accumulation will occur at all.\nManipulate an equation to cancel out some common factors algebraically, rather than letting them cancel out computationally. For example, to calculate \\(\\frac{x}{y}\\) where \\(x = Am\\) and \\(y = An\\) for some large \\(A\\), compute m / n instead of x / y.\nSeparate magnitude and sign, and work with magnitude in log-space, i.e. manipulate sign(x) and log(abs(x)) instead of working with x directly. Convert back to linear-space with sign(x) * exp(f(log(abs(x)))), where f has internally completed the relevant cancellations, so f(log(abs(x))) is small enough to avoid overflow.\n\nWith these techniques in mind, we can implement a numerically-stable version of the attention algorithm. This is the code used to generate the experimental results in this article.\ndef even_power_attn(Q, K, V, p, ε):\n     # even only\n     assert p % 2 == 0\n\n    # compute inner products\n    D = Q @ K.T\n\n    # raise to power, in log space for numerical stability\n    log_C = p * log(abs(D) + ε)\n    # apply causal mask\n    log_C = where(tril(ones(log_C.shape)), log_C, -inf)\n    # subtract rowmax for numerical stability\n    log_C -= log_C.max(axis=-1, keepdims=True)\n    \n    # Return to linear space\n    B = exp(log_C)\n    # Compute the normalizing term, accumulating in float32 for numerical stability\n    denom = B.sum(-1, keepdims=True, dtype=float32).astype(B.dtype)\n    \n    # project to simplex, adding ε for numerical stability\n    A = B / (denom + ε)\n    # compute output\n    Y = A @ V\n    return Y\n\nNow we are ready to train some models. For this experiment, we used the LongCrawl64 dataset [13], a context length of 4096, a batch size of 524288 tokens. The architecture was similar to the 124M-parameter GPT-2 architecture, but with rotational positional encoding and an additional layernorm after input embeddings. The optimization was conducted in bf16 mixed-precision using Adam with learning rate .0006 and no scheduling. Each model was trained on a node of 8 H100s.\n\n\n\n\nPerformance seems to improve consistently as we increase \\(p\\). For large enough \\(p\\), the performance of even-power transformers matches or even surpasses that of the softmax transformer baseline. This architecture is looking promising!\nIn Section 3, we will show how to use the tensor product to implement even-power attention as a linear transformer, albeit one with a state size so large as to be impractical. In Section 4, we will see that the tensor product embedding is highly symmetric and contains a lot of redundant information. We will exploit this structure to construct an embedding function for even-power attention with tractable state size."
  },
  {
    "objectID": "articles/symmetric-power-transformers/index.html#tensor-product",
    "href": "articles/symmetric-power-transformers/index.html#tensor-product",
    "title": "Symmetric Power Transformers",
    "section": "3. Tensor Product",
    "text": "3. Tensor Product\nIn this section, we show how the tensor product, a deep and ubiquitous mathematical idea, can be used to construct embeddings for the even-power transformer.\n\n3.1. Mathematical Background\nThe tensor product of vectors generalizes the outer product and formalizes the concepts of multi-dimensional arrays. Given two vectors \\(v\\in \\R^{d_1}\\) and \\(w\\in \\R^{d_2}\\) one can think of their tensor product \\(v\\otimes w\\) as the matrix \\(v w^T \\in \\R^{d_1\\times d_2}\\), \\[\nv w^T = \\left[\n\\begin{array}{cccc}\nv_1w_1 & v_1w_2 & \\cdots & v_1w_m \\\\\nv_2w_1 & v_2w_2 & \\cdots & v_2w_m \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nv_nw_1 & v_nw_2 & \\cdots & v_nw_m \\\\\n\\end{array}\n\\right]\n\\] Intuitively, \\(v \\otimes w \\otimes u\\) would be a 3-dimensional table containing all possible entries of the sort \\(v_i\nw_j u_k\\). But let’s make the intuition of multi-dimensional tables more rigorous.\nMulti-indices. A multi-index \\(\\alpha\\) specifies a location in a \\(p\\) dimensional table with dimension sizes \\(d_1,\n\\cdots, d_p \\in \\N\\). Let \\(\\N_d\\) denote the set \\(\\{1,2,\\cdots,d\\}\\). Then, the space of multi-indices is \\(\\N_{d_1} \\times \\cdots \\times \\N_{d_p}\\) and we refer to a generic multi-index as \\(\\alpha = [\\alpha_1, \\cdots, \\alpha_p] \\in \\N_{d_1} \\times \\cdots \\times \\N_{d_p}\\).\nTensors. A tensor \\(T\\in \\R^{d_1 \\times \\cdots d_p}\\) corresponds to a high-dimensional table where every location has a numerical value assigned to it. In other words, it is a map from multi-indices to the reals. \\[\nT: \\N_{d_1} \\times \\cdots \\times \\N_{d_p} \\to \\R\n\\] By convention, we index tensors with subscript notation \\(T_\\alpha\\) instead of functional notation \\(T(\\alpha)\\), but the meaning is the same.\nTensor product of vectors. Given a list of \\(p\\) vectors \\(v_i \\in \\R^{n_i}\\), we denote by \\(v_1 \\otimes \\cdots\n\\otimes v_p\\) (or alternatively \\(\\bigotimes_{i=1}^p v_i\\)) as the tensor in \\(\\R^{n_1 \\times \\cdots \\times n_p}\\) with entries given by the following formula: \\[\n\\left[\\bigotimes_{i=1}^p v_i\\right]_\\alpha = \\prod_{i=1}^p v_{i, \\alpha_i}\n\\] Where \\(v_{i,j}\\in \\R\\) denotes the \\(j\\)th entry of the \\(i\\)th vector.\nFlattening. To build embedding functions \\(\\phi\\), we are going to use the tensor product to embed lists of vectors into \\(\\R^{d_1\\times \\cdots d_p}\\) tensors. But once we’ve done that, we will no longer care about the tensor structure and we will prefer to think of them as vectors in \\(\\R^D\\), where \\(D=\\prod_{i=1}^p d_i\\). The map \\(\\text{flat}: \\R^{d_1\\times \\cdots d_p} \\to \\R^D\\) implements this transformation by writing every entry of the array into a flat vector. This can be done with any bijective function \\(\\sigma: D \\to \\N_{d_1} \\times \\cdots \\times \\N_{d_p}\\) which effectively imposes an (arbitrary) ordering on the multi-indices.3 The flattening is defined as: \\[\n\\flat{T}_i = T_{\\sigma(i)}\n\\] The dot product of flattened tensors satisfies the following property: \\[\n\\flat{\\bigotimes_{i=1}^p v_i}^T \\flat{\\bigotimes_{i=1}^p w_i} = \\prod_{i=1}^p v_i^T w_i \\qquad \\text{(Result 1)}\n\\]\n\n\nExpand to see a proof.\n\nWe can just check the both sides of the equation match. First, \\[\\begin{align}\n\\flat{\\bigotimes_{i=1}^p v_i}^T \\flat{\\bigotimes_{i=1}^p w_i}\n&= \\sum_{l=1}^D \\left[ \\bigotimes_{i=1}^p v_i \\right]_{\\sigma(l)} \\left [ \\bigotimes_{i=1}^p w_i \\right]_{\\sigma(l)}  \\\\\n&= \\sum_{l=1}^D \\prod_{i=1}^p v_{i, \\sigma(l)_i} w_{i, \\sigma(l)_i} \\\\\n&= \\sum_{j_1=1}^{d_1} \\cdots \\sum_{j_p=1}^{d_p} \\prod_{i=1}^p v_{i, j_i} w_{i, j_i} \\\\\n\\end{align}\\] Where we used the assumption that \\(\\sigma(l)\\) ranges over every possible combination of \\([j_1, \\cdots, j_p]\\). On the other hand, \\[\\begin{align}\n\\prod_{i=1}^p v_i^T w_i\n&= \\prod_{i=1}^p \\sum_{j=1}^{d_i}  v_{i, j} w_{i, j}  \\\\\n&= \\sum_{j_1=1}^{d_1} \\cdots \\sum_{j_p=1}^{d_p} \\prod_{i=1}^p v_{i, j_i} w_{i, j_i}\n\\end{align}\\] Where the last step used a generalization of the distributive property: \\(\\prod_{i=1}^p \\sum_{j=1}^{d_i}  v_{i, j} = \\sum_{j_1=1}^{d_1} \\cdots \\sum_{j_p=1}^{d_p} \\prod_{i=1}^p v_{i, j_i}\\)\n\n\n\n3.2. Implementation\nArmed with the tensor product, we are ready to define an embedding \\(\\phi^p_{\\text{TP}}\\), and in doing so define a linear transformer architecture. The definition is simple: embed a key by taking its tensor product with itself, \\(p\\) times.\n\n\nIn this section, we focus on the effect of \\(\\phi\\) on keys \\(k\\), but wlog all discussion applies equally to queries.\n\\[\n\\phi^p_{\\text{TP}}(k) =\n\\text{flat}\\left(\\bigotimes_{i=1}^p k\\right) \\in \\mathbb{R}^{d^p}\n\\]\nIf this embedding is used with even \\(p\\), the resulting architecture is an even-power transformer:\n\\[\n\\phi^p_{\\text{TP}}(q)^T \\phi^p_{\\text{TP}}(k) =\n\\text{flat}\\left(\\bigotimes_{i=1}^p q\\right)^T \\text{flat}\\left(\\bigotimes_{i=1}^p k\\right) =\n\\prod_{i=1}^p q^T k = (q^T k)^p\n\\]\nwhere in the second step we used Result 1.\nThe implementation is straightforward.\n\nEmbeddingTest\n\n\ndef tensor_power_embedding(k, p):\n    expanded_k = k\n    for _ in range(p-1):\n        expanded_k = expanded_k[...,None] @ k[None,:]\n    return expanded_k.flatten()\n\n\ndef even_power(q, k, p):\n  return np.inner(q, k) ** p\n\ndef tensor_power_inner_product(q, k, p):\n  embedded_q = tensor_power_embedding(q, p)\n  expanded_k = tensor_power_embedding(k, p)\n  return (embedded_q * expanded_k).sum()\n\nd = 8\nfor p in [2, 4, 6, 8]:\n  q = np.random.random(d)\n  k = np.random.random(d)\n  assert np.allclose(\n    even_power(q, k, p), \n    tensor_power_inner_product(q, k, p)\n  )\n\n\n\nEmbedding in hand, we can return to our main objective. Have we found an linear transformer whose performance is competitive with that of a strong transformer baseline, while, at the same time, having a state size small enough to fit on a GPU?\nThe table below shows the size of a single state, as measured in bytes (assuming fp16/bf16 precision), for a 124M-parameter GPT-2 tensor power transformer at various \\(p\\).\n\n\nThe formula for the state size of a linear transformer with the tensor power embedding is layer_n * head_count *  key_size**p * value_size.\n\n\n\n\n\n\n\n\n\n\n\np\nState Size\nMemory ≤ 80 GB?\nRelative Loss at 100K Steps\nLoss ≤ baseline?\n\n\n\n\n2\n77 MB\n✔\n1.03x\n✘\n\n\n4\n314 GB\n✘\n.98x\n✔\n\n\n6\n1.3 PB\n✘\n.97x\n✔\n\n\n\n\nThe settings of \\(p\\) that improve upon the baseline have states that are far too large. So the embedding \\(\\phi^p_{\\text{TP}}\\) still does not satisfy all the properties we are after. But we are close."
  },
  {
    "objectID": "articles/symmetric-power-transformers/index.html#symmetric-power-transformers",
    "href": "articles/symmetric-power-transformers/index.html#symmetric-power-transformers",
    "title": "Symmetric Power Transformers",
    "section": "4. Symmetric Power Transformers",
    "text": "4. Symmetric Power Transformers\nThe missing piece is to realize the huge embeddings we’ve been working with are highly symmetric. The theory of symmetric powers will help us compress the same information into much smaller objects. We will begin with an introduction to the relevant mathematical ideas. Then, we will put them to use by proposing symmetric power transformers, whose embedding is \\(\\phi^p_{\\text{SYM}}\\).\nTo build intuition, observe that the embedding \\(\\phi^2_{\\text{TP}}(v)= \\flat {v v^T}\\) is somewhat wasteful. The matrix \\(v v^T\\) is symmetric, so all the information we need can be found in the upper triangular part of the matrix. \\[\nv v^T = \\left[\n\\begin{array}{cccc}\nv_1v_1 & v_1v_2 & \\cdots & v_1v_m \\\\\nv_2v_1 & v_2v_2 & \\cdots & v_2v_m \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nv_nv_1 & v_nv_2 & \\cdots & v_nv_m \\\\\n\\end{array}\n\\right]\n\\] Entries at indices \\((i,i)\\) appear a single time, but due to the commutativity of scalar multiplication (i.e. \\(v_i v_j =\nv_j v_i\\)), the entries at indices \\((i,j)\\) each appear twice (if \\(i\\neq j\\)).\nNoticing this symmetry in the matrix \\(v v^T\\) allows us to create an alternative embedding, \\(\\phi^2_\\text{SYM}: \\R^d \\to \\R^{\\frac{d^2 +d} 2}\\), which can be implemented as:\ndef sym_2_embedding(v):\n  x, d = [], v.size\n  for i in range(d):\n    for j in range(i, d):\n      count = 1 if i==j else 2\n      x.append(sqrt(count) * v[i] * v[j])\n  return x\nThis construction of \\(\\phi^2_\\text{SYM}\\) guarantees that \\(\\phi^2_\\text{SYM}(v)^T \\phi^2_\\text{SYM}(w) = \\phi^2_\\text {TP}\n(v)^T \\phi^2_\\text{TP}(w)= (v^T w)^2\\). Recall that in the attention formulation of the linear transformer the embedding \\(\\phi\\) only influences the outputs via the attention scores, which were defined as \\[\nA_{ij}  = \\frac{ \\phi(Q_i)^T \\phi(K_j)}{\\sum_{k=1}^i \\phi(Q_i)^T \\phi(K_k) }\n\\] Then two linear transformers with embeddings \\(\\phi^2_\\text{TP}(v)\\) and \\(\\phi^2_\\text{SYM}(v)\\) will have exactly the same outputs, since they have the same inner products \\(\\phi(Q_i)^T \\phi(K_j)\\) (namely, \\((Q_i^T K_j)^p\\)). We’ve been able to exploit the symmetry of \\(v v^T\\) to construct an equivalent embedding function with approximately half the dimensionality!\nIn this section, we will generalize this idea to arbitrary powers.\n\n4.1. Mathematical Background\nWe begin by introducing some key tools.\nPermutation group. The first thing we need is the permutation group of \\(p\\) elements, which is defined as the set of all functions \\(\\rho: \\N_p \\to \\N_p\\) that are invertible and denoted by \\(G_p\\). We also overload the notation slightly. For a multi-index \\(\\alpha = [\\alpha_1, \\cdots, \\alpha_p]\\) define the permutation of the multi-index as \\(\\rho(\\alpha) = [\\alpha_{\\rho(1)}, \\cdots, \\alpha_{\\rho(p)}]\\). This is useful to define symmetric tensors.\nSymmetric tensors. A tensor \\(T \\in \\R^{\\underbrace{d\\times \\cdots \\times d}_p}\\) is symmetric if for all multi-indices \\(\\alpha \\in \\N_d \\times \\cdots \\times \\N_d\\) and permutations \\(\\rho \\in G_p\\) we have that: \\[\nT_\\alpha = T_{\\rho(\\alpha)}\n\\]\nSymmetric power of vectors. We use the notation \\(v^{\\otimes p}\\) to refer to \\(\\otimes^p_{i=1} v\\) (the tensor product of \\(p\\) copies of \\(v\\)), and call it the \\(p\\)th symmetric power of \\(v\\). Due to the commutativity of multiplication, all symmetric powers of vectors are symmetric tensors. For example, for a multi-index \\([1, 2, 3]\\), the entrie \\([v^{\\otimes 3}]_{[1, 2, 3]} = v_1 v_2 v_3\\) will equal \\([v^{\\otimes 3}]_{[3, 2, 1]} = v_3 v_2 v_1\\). Showing that a general tensor \\(v^{\\otimes p}\\) is symmetric is simple: \\[\n\\left[ v^{\\otimes p} \\right ]_{\\rho(\\alpha)} = \\prod_{i=1}^p v_{\\alpha_{\\rho(i)}} = \\prod_{i=1}^p v_{\\alpha_i} = \\left[ v^{\\otimes p} \\right ]_{\\alpha}\n\\]\nTo construct embeddings that exploit the symmetries of \\(T=v^{\\otimes p}\\) we will need some key properties about symmetric tensors:\n\nDuplication counts: If \\(T\\) is symmetric, the entry \\(T_\\alpha\\) might have duplicate entries. To know how many duplicates a multi-index \\(\\alpha\\) has, we first need to count how many times each number \\(i\\in \\{1, \\cdots, d\\}\\) occurs in \\(\\alpha\\). Define the counts \\(c_i = \\sum_{j=1}^p \\delta(\\alpha_j, i)\\). Then, the number of multi-indices containing the same data as \\(\\alpha\\) is given by the formula \\(\\frac{d!}{c_1 ! \\; \\cdots \\; c_p!}\\).\nUnique multi-indices: No data is lost if we restrict ourselves to only looking at entries \\(T_\\alpha\\) for multi-indices \\(\\alpha\\) that are non-decreasing (i.e. \\(\\alpha_i \\le \\alpha_{i+1}\\)). The intuition is that an arbitrary multi-index \\(\\beta\\) can always be transformed into a non-decreasing multi-index \\(\\alpha\\) by applying some permutation \\(\\rho\\). Using the defining property of symmetric tensors, \\(T_\\beta = T_{\\rho(\\beta)} = T_\\alpha\\). Thus, we lose no information by excluding every multi-index that isn’t non-decreasing.\nDimension: The space of symmetric tensors has dimension \\(\\binom{d+p-1}{p}\\). This can be derived via a classic combinatorial argument counting the number of non-decreasing sequences.\n\n\n\nExpand to see a complete derivation of these properties and a few other relevant facts about symmetric tensors.\n\nDuplicate counts. By definition, the only constraint a symmetric tensor has, is that all the entries \\(T_{\\rho(\\alpha)}\\) must be the same for all permutations \\(\\rho \\in G_p\\). Now we want to understand the amount of duplication that that any specific \\(\\alpha\\) has. Since the number of permutations of the multi-indices is \\(|G_p| = p!\\), a naive estimate would be that every entrie \\(T_\\alpha\\) appears \\(p!\\) times in the tensor. And indeed, that is the case for some multi-indices. For example, every permutation \\(\\rho \\in G_3\\) sends the multi-index \\([1,4,6]\\) to a different multi-index, so there are \\(3!\\) entries with the same value. But, on the other hand, for the multi-index \\([1,1,1]\\) it doesn’t matter what permutation \\(\\rho\\) we apply, we always have that \\(\\rho \\alpha = \\alpha\\). So the entrie at \\([1,1,1]\\) has no duplicates.\nTo count the number of duplicates for a generic mulit-index \\(\\alpha\\) we are going to use the orbit stabilizer theorem. This theorem tells us that the number of elements in the set \\(\\orbit(\\alpha) = \\{ \\rho(\\alpha) \\; | \\; \\rho \\in G_p \\}\\) is given by the formula: \\[\n|\\orbit(\\alpha) | =\\frac {|G_p|} {|\\stab(\\alpha)|}\n\\] where the stabilizer \\(\\stab(\\alpha) = \\{ \\rho \\in G_p \\; | \\; \\rho(\\alpha) = \\alpha \\}\\) is the set of permutations that fix \\(\\alpha\\). Working out the size of the stabilizer is not hard. For a permutation \\(\\rho \\in G_p\\) to leave \\(\\alpha\\) fixed it must satisfy that \\(\\alpha_{\\rho(i)} = \\alpha_i\\) for all \\(i\\in \\N_p\\). In other words, \\(\\rho\\) must only interchange entreis of \\(\\alpha\\) that hold the same index. Say \\(\\alpha = [1,1,2]\\), then we can only exchange the fist and second element. Generically, if index \\(i\\in\\N_d\\) appears \\(c_i\\) times in \\(\\alpha\\), then there are \\(c_i!\\) permutations that move around entries of \\(\\alpha\\) with value \\(i\\) while keeping the rest fixed. From this, it is clear that: \\[\n|\\stab(\\alpha)| = \\prod_{i=1}^d c_i !\n\\] For the example, the counts for \\([1,1,2]\\) are \\(c_1=2, \\; c_2 = 1\\) so \\(|\\stab([1,1,2])| = 2! \\: 1! =2\\). With this, we can get the formula for the number of replicated entries of \\(\\alpha\\) by applying the orbit stabilizer theorem: 4 \\[\n|\\orbit(\\alpha) | = \\frac {p!} {\\prod_{i=1}^d c_i!}\n\\]\nBasis of symmetric tensors. We know that a lot of multi-indices of a symmetric tensor are redundant. To understand the true structure (like the dimensionality) of symmetric tensors we need to find a way to select an instance of each, non redundant, multi-indices. One way to do that is to restrict oursleves to non decreasing multi-indices. Denote them by \\(P = \\{\\alpha \\in \\N_d^{\\times p} \\; | \\; \\alpha_i \\le \\alpha_{i+1} \\}\\). Then we can construct a basis for the space of symmetric tensors out of \\(\\alpha \\in P\\) like \\[\nS^\\alpha = \\sum_{\\rho \\in G_p} E^{\\rho(\\alpha)}\n\\] where \\(E^\\alpha = \\bigotimes^p_{i=1} e_{\\alpha_i}\\). (the tensors \\(E^\\alpha\\) are a the natural way to construct a basis for the non-symmetric tensors out of the basis \\(e_i \\in \\R^d\\)) To convince ourselves that \\(\\{S^\\alpha \\; | \\; \\alpha \\in P \\}\\) forms a basis of the symmetric tensors we need to check that the set is linearly independent and that it spans all symmetirc tensors. Let’s check linear independence first. Assume that we have some coefficients \\(x_\\alpha \\in \\R\\) s.t. \\[\n\\sum_{\\alpha \\in P}  x_\\alpha S^\\alpha = 0\n\\] Then, for any \\(\\beta \\in P\\) \\[\\begin{align}\n\\left[\\sum_{\\alpha \\in P}  x_\\alpha S^\\alpha\\right]_\\beta\n&= \\sum_{\\alpha \\in P}  x_\\alpha S^\\alpha_\\beta\n= \\sum_{\\alpha \\in P} x_\\alpha \\sum_{\\rho \\in G_p} E^{\\rho(\\alpha)}_\\beta \\\\\n&= \\sum_{\\alpha \\in P} x_\\alpha \\sum_{\\rho \\in G_p} \\delta(\\rho(\\alpha) = \\beta) \\\\\n\\end{align}\\] Since \\(\\alpha, \\beta \\in P\\) the only way there can exist a \\(\\rho \\in G_p\\) such that \\(\\rho(\\alpha) = \\beta\\) is when \\(\\alpha = \\beta\\). So \\[\\begin{align}\n0 &= \\left[\\sum_{\\alpha \\in P}  x_\\alpha S^\\alpha\\right]_\\beta \\\\\n&= x_\\beta \\sum_{\\rho \\in G_p} \\delta(\\rho(\\beta) = \\beta) \\\\\n&= x_\\beta \\; | \\stab (\\alpha) | \\\\\n\\end{align}\\] And, since \\(| \\stab (\\alpha) | \\ge 1\\) that implies that \\(x_\\alpha = 0\\) and we have that the set of \\(S^\\alpha\\) is linearly independent. To show \\(S^\\alpha\\) span all symmetric tensors it we can just show that, for any symmetric tensor \\(T\\), if we define \\[\nQ = \\sum_{\\alpha \\in P} \\frac {T_\\alpha} {\\stab(\\alpha)} S^\\alpha\n\\] Then \\(T = Q\\). That can be easily seen by noticing that \\(Q\\) is a symmetric tensor and that, evaluating \\(Q\\) at \\(\\beta \\in P\\) \\[\\begin{align}\nQ_\\beta &= \\left[\\sum_{\\alpha \\in P} \\frac {T_\\alpha} {\\stab(\\alpha)} S^\\alpha \\right]_\\beta\n= \\sum_{\\alpha \\in P} \\frac {T_\\alpha} {\\stab(\\alpha)} \\sum_{\\rho \\in G_p} E^{\\rho(\\alpha)}_\\beta \\\\\n&= \\sum_{\\alpha \\in P} \\frac {T_\\alpha} {\\stab(\\alpha)} \\sum_{\\rho \\in G_p} \\delta(\\rho(\\alpha) = \\beta) \\\\\n&= \\frac {T_\\beta} {\\stab(\\beta)} \\sum_{\\rho \\in G_p} \\delta(\\rho(\\beta) = \\beta) \\\\\n&= T_\\beta\n\\end{align}\\]\nDimension of symmetric tensors. Since we’ve created a basis for the space of symmetric tensors out of non-decreasing sequences we can establish the dimension of the space by counting all such sequences. This is a standard combinatorial problem solved via the method of stars and bars. Which tells us that the dimension is \\[\\binom{d+p-1}{p}\\]\nThe only thing we must note to apply the standard combinatorial results is that there is a 1-1 correspondance between non-decreasing sequences and multisets of \\(\\N_d\\) with cardinality \\(p\\). This is because there is a unique way to lay out a multi-set into a non-decreasing sequence. However many \\(1\\)s there are in the multi-set, they will all come first, then all the \\(2\\) etc…\nSymmetric powers of vectors span the symmetric tensors Showing that all tensors of the form \\(v^{\\otimes p}\\) are symmetric was tivial, but there is a harder question we might ask ourselves. Do we actually need all the \\(\\binom {d + p - 1} p\\) dimensions of the symmetric tensors? The way to formalize this question is to ask whether the space of all symmetric tensors is spanned by rank-1 symmetric tensors \\(v^{\\otimes p}\\). We will prove that the answer is yes by building every basis vector \\(S^\\alpha\\) that way. Concretely, if we define \\[\nZ^\\alpha\n= \\sum_{b_1, \\cdots, b_p = 0}^1\n\\prod_{i=1}^p (-1)^{b_i -1}\n\\left ( \\sum_{j=1}^p b_j e_{\\alpha_j}  \\right)^{\\otimes p}\n\\] Turns out that \\(Z^\\alpha = S^\\alpha\\). It is evident that both, the \\(Z^\\alpha\\) and \\(S^\\alpha\\) are symmetric tensors so, to convince ourselves that they are equivalent, we just need to index them at a non-decreasing multi-index \\(\\beta \\in P\\). First, see that \\[\\begin{align}\nS^\\alpha_\\beta\n&= \\sum_{\\rho \\in G_p} \\left [ \\bigotimes^p_{i=1} e_{\\rho(\\alpha)_i} \\right]_\\beta \\\\\n&= \\sum_{\\rho \\in G_p} \\prod^p_{i=1} \\delta(\\rho(\\alpha)_i = \\beta_i)  \\\\\n&= \\sum_{\\rho \\in G_p} \\delta(\\rho(\\alpha) = \\beta)  \\\\\n&= |\\stab(\\alpha) | \\; \\delta(\\alpha = \\beta)  \\\\\n\\end{align}\\] And, on the other hand, \\[\\begin{align}\nZ^\\alpha_\\beta &= \\sum_{b_1, \\cdots, b_p = 0}^1\n\\prod_{i=1}^p (-1)^{b_i -1}\n\\left [ \\left ( \\sum_{j=1}^p b_j e_{\\alpha_j} \\right)^{\\otimes p} \\; \\right ]_\\beta \\\\\n&= \\sum_{b_1, \\cdots, b_p = 0}^1\n\\prod_{i=1}^p (-1)^{b_i -1}\n\\prod_{i=1}^p \\left ( \\sum_{j=1}^p b_j \\delta(\\alpha_j = \\beta_i)  \\right)  \\\\\n&= \\sum_{b_1, \\cdots, b_p = 0}^1\n\\prod_{i=1}^p (-1)^{b_i -1}\n\\sum_{j_1, \\cdots, j_p = 1}^d  \\prod_{i=1}^p b_{j_i} \\delta(\\alpha_{j_i} = \\beta_i)  \\\\\n&= \\sum_{j_1, \\cdots, j_p = 1}^d  \\sum_{b_1, \\cdots, b_p = 0}^1\n\\prod_{i=1}^p (-1)^{b_i -1}\n\\prod_{i=1}^p b_{j_i} \\delta(\\alpha_{j_i} = \\beta_i)  \\\\\n\\end{align}\\] During the sum over all combinations of \\(j_1, \\cdots, j_p\\), if there is any \\(l \\in \\N_p\\) that does not appear in the set \\(j_1, \\cdots, j_p\\), then that term of the sum will drop out. This is because the only place where \\(b_l\\) will appear is in the term \\((-1)^{b_l-1}\\), so since we are summing over \\(b_l \\in \\{0, 1\\}\\), the terms will cancel out. Thus, we can restrict ourselves to summing over \\(j_1, \\cdots, j_p\\) that contain every element in \\(\\N_p\\). In other words, the \\(j\\) terms must be a permutation \\([j_1, \\cdots, j_p] = \\rho([1, \\cdots, p])\\) for some \\(\\rho \\in G_p\\). So we can continue \\[\\begin{align}\nZ^\\alpha_\\beta &=  \\sum_{\\rho \\in G_p} \\sum_{b_1, \\cdots, b_p = 0}^1\n\\prod_{i=1}^p (-1)^{b_i -1}\n\\prod_{i=1}^p b_{\\rho(i)} \\delta(\\alpha_{\\rho(i)} = \\beta_i)  \\\\\n&= \\sum_{\\rho \\in G_p}  \\prod_{i=1}^p  \\delta(\\alpha_{\\rho(i)} = \\beta_i)  \\\\\n\\end{align}\\] Where we used the fact that, since the inner term is multiplied by every \\(b_i\\), the only way the term can be non \\(0\\) is if every single \\(b_i =1\\). Finally, we can wrap up the proof, \\[\\begin{align}\nZ^\\alpha_\\beta\n&= \\sum_{\\rho \\in G_p} \\prod_{i=1}^p \\delta(\\alpha_{\\rho(i)} = \\beta_i)  \\\\\n&= \\sum_{\\rho \\in G_p} \\delta(\\rho(\\alpha) = \\beta)  \\\\\n&= |\\stab(\\alpha) | \\; \\delta(\\alpha = \\beta)  \\\\\n&= S^\\alpha_\\beta\n\\end{align}\\]\n\n\n\n4.2 Implementation\nThe symmetric power embedding \\(\\phi^p_\\text{SYM}(v)\\) will give a list of \\(\\binom{d+p-1}{p}\\) numbers, each corresponding to \\([v^{\\otimes p}]_\\alpha\\) for a non-decreasing \\(\\alpha\\). Just as we did in the example of \\(\\phi^2_\\text{SYM}\\), we also need to apply a correction that is the square root of the duplicate count of that particular \\(\\alpha\\). The inner product of two vectors embedded in this way is identical to the tensor power embedding, \\(\\phi^2_\\text{SYM}(v)^T \\phi^2_\\text{SYM}(w) = \\phi^2_\\text {TP}(v)^T \\phi^2_\\text {TP}(w)\\). The following is an example implementation of this embedding:\n\nEmbeddingTest\n\n\ndef symmetric_power_embedding(k, p):\n    d = len(k)\n    x = []\n    for midx in non_decreasing_multiindices(p, d):\n        c = count(midx, d)\n        xi = np.sqrt(multinomial(c))\n        for j in range(p):\n            xi *= k[midx[j]]\n        x.append(xi)\n    return np.array(x)\n\n# -- helper functions --\n# generates list of non-decreasing multiindices\ndef non_decreasing_multiindices(n, max_idx, starting_from=0):\n    if n == 1:\n        return [[i] for i in range(starting_from, max_idx)]\n    seqs = []\n    for i in range(starting_from, max_idx):\n        seqs += [[i, *remainder] for remainder in\n                    non_decreasing_multiindices(n-1, max_idx, starting_from=i)]\n    return seqs\n\n# computes multinomial coefficient\ndef multinomial(lst):\n    res, i = 1, 1\n    for a in lst:\n        for j in range(1, a + 1):\n            res *= i\n            res //= j\n            i += 1\n    return res\n\n# given a multiindex, counts how many times each index appears\ndef count(midx, d):\n    c = [0] * d\n    for i in midx:\n      c[i] += 1\n    return c\n\n\ndef even_power(q, k, p):\n  return np.inner(q, k) ** p\n\ndef symmetric_power_inner_product(q, k, p):\n  embedded_q = symmetric_power_embedding(q, p)\n  expanded_k = symmetric_power_embedding(k, p)\n  return (embedded_q * expanded_k).sum()\n\nd = 8\nfor p in [2, 4, 6, 8]:\n  q = np.random.random(d)\n  k = np.random.random(d)\n  assert np.allclose(\n    even_power(q, k, p), \n    symmetric_power_inner_product(q, k, p)\n  )\n\n\n\nUsing this embedding produces a massive dimensionality reduction compared to the dimensionality of \\(\\phi_\\text{TP} ^p\\). The table below compares the size of the state between repeated tensor products and symmetric powers, as measured in bytes (assuming half-precision), for a 124M-parameter GPT-2 transformer at various \\(p\\).\n\n\n\n\np\nTensor Power\nSymmetric Power\nSavings\n\n\n\n\n2\n77 MB\n39 MB\n49%\n\n\n4\n314 GB\n14 GB\n96%\n\n\n6\n1.3 PB\n2.2 TB\n99.8%\n\n\n8\n5.3 EB\n199 TB\n99.996%\n\n\n\n\nWe can evaluate each symmetric power architecture against our two metrics, state size (under 80 GB) and performance (loss below baseline).\n\n\n\n\np\n\n\nState Size\n\n\nMemory ≤ 80 GB?\n\n\nRelative Loss at 100K Steps\n\n\nLoss ≤ baseline?\n\n\n\n\n\n\n2\n\n\n39 MB\n\n\n✓\n\n\n1.03x\n\n\n✗\n\n\n\n\n4\n\n\n14 GB\n\n\n✓\n\n\n0.98x\n\n\n✓\n\n\n\n\n6\n\n\n2.2 TB\n\n\n✗\n\n\n0.97x\n\n\n✓\n\n\n\n\nThe symmetric power transformer with \\(p=4\\) passes our bar."
  },
  {
    "objectID": "articles/symmetric-power-transformers/index.html#conclusion",
    "href": "articles/symmetric-power-transformers/index.html#conclusion",
    "title": "Symmetric Power Transformers",
    "section": "5. Conclusion",
    "text": "5. Conclusion\nIn this article, we have introduced the symmetric power transformer, a linear transformer which closes the performance gap to classic softmax transformers using a tractably-small state. We replace the exponentiation in a traditional softmax transformer with an even power, and then show that this is equivalent to a linear transformer with the symmetric power embedding. We expect this approach will provide transformer-level performance at greatly reduced training costs when combined with the chunked algorithm. It will also enjoy cheaper inference, thanks to the constant-time inference costs common to all RNNs. In an upcoming article, we plan to release an open-source model that uses a symmetric power transformer at its core, together with an efficient CUDA kernel implementation. Stay tuned!\n\n  \n  Subscribe to be notified of new posts:"
  },
  {
    "objectID": "articles/symmetric-power-transformers/index.html#footnotes",
    "href": "articles/symmetric-power-transformers/index.html#footnotes",
    "title": "Symmetric Power Transformers",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is also known as the kernel method. It is essential when one works with infinite-dimensional embeddings, and it’s also useful in this case to avoid materializing large-but-finite embeddings.↩︎\nThis state-size threshold is admittedly somewhat arbitrary. In principle, larger states are possible with clever sharding; but for excessively large states, which must be sharded across a huge number of GPUs, the hardware cost of sharding becomes completely prohibitive. In this article, we are training models whose parameters fit on a single GPU, so it seems reasonable to use the memory of a single GPU as the threshold for tractability.↩︎\nA natural choice for the ordering \\(\\sigma\\) is row major ordering.↩︎\nYou might have previously seen this expression in the multinomial theorem. This connection is no coincidence. Symmetric powers are highly related to polynomails.↩︎"
  },
  {
    "objectID": "drafts/research.html",
    "href": "drafts/research.html",
    "title": "Research",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\n\n\n\n\nDec 27, 2023\n\n\nAn Architecture For Neural Language Modeling\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blogposts/mission/index.html",
    "href": "blogposts/mission/index.html",
    "title": "Our Mission",
    "section": "",
    "text": "A poem is a shadow of the act of writing poetry.\nHumanity casts many shadows. All literature, letters, recipes, and tweets. Mathematical proofs and git repositories. Laws, treaties, and declarations of war. Financial statements, employee performance reports, and bankruptcy filings. Podcasts, operas, accidental voicemails, YouTube videos and Hollywood blockbusters. Restaurant reviews and love letters and times tables. Individually, each of these pieces of information is nothing but the faintest shadow of the process that produced it. But collectively, these shadows tell a rich story about the world.\nSince the dawn of humanity, the brain alone could reconstruct the world from these shadows. But the last decade of deep learning has convinced us that this won’t be the case for much longer. Though significant challenges remain, we stand poised to solve them. If successful, it will become possible to synthesize every documentable aspect of humanity into the weights of a neural network.\nOur mission is to train a neural network to model all human output.\nThere are two primary challenges in our pursuit of this goal.\n\nCurrently, it’s not technically feasible to train a model that can ingest all the data that we can collect. Limitations around context length, modality, and throughput force us to use only a small subset of the data available to us.\nMuch of the data we will need has never been collected, curated, and organized into datasets that we can use for training.\n\nWe are building a world-class team of engineers and researchers to tackle these challenges, united around shared principles and a specific research agenda. We value clear thinking, sharing knowledge, and an extreme commitment to scientific honesty. Our research is guided by mathematical beauty and grounded in rigorous empiricism. We are committed to letting the quality of our work speak for itself. No hype, no fluff. All meat.\nIf this vision resonates, please reach out:          777b7a607577605479757a7d72716760757d3a777b79\n\n\n  \n  Or, subscribe to be notified of new posts:"
  },
  {
    "objectID": "coming-soon.html",
    "href": "coming-soon.html",
    "title": "Manifest AI",
    "section": "",
    "text": "Coming Soon\nThe page you requested is an upcoming release and not yet available.\nIf you believe this to be in error please reach out to us at\ncontact at manifest ai dot com"
  },
  {
    "objectID": "blogposts/faster-after-all/index.html",
    "href": "blogposts/faster-after-all/index.html",
    "title": "Linear Transformers Are Faster After All",
    "section": "",
    "text": "\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\Z}{\\mathbb{Z}}\n\\newcommand{\\N}{\\mathbb{N}}\n\\newcommand{\\sft}{\\text{softmax}}\n\\newcommand{\\List}{\\text{List}}\n\\newcommand{\\Seq}{\\text{Seq}}\n\\newcommand{\\SeqT}{\\text{SeqT}}\n\\newcommand{\\CSeqT}{\\text{CSeqT}}\n\\newcommand{\\Dist}{\\text{Dist}}\n\\newcommand{\\SM}{\\text{SM}}\n\\newcommand{\\Fn}{\\text{Fn}}\n\\newcommand{\\Tok}{\\text{Tok}}\n\\newcommand{\\Aij}{ A_{[i,j]}}\n\\]\nIt is well-known that removing the exponential from the attention layer of a transformer allows for a recurrent reformulation, with computational cost that is linear instead of quadratic on context length [1]. One would expect that such an architecture would be far faster to train, especially when the context size is large. However, when training deep neural networks on hardware accelerators (e.g. GPUs), reductions in FLOPs do not always straightforwardly translate into practical speedups. Initial empirical experiments showed that large language models based on linear transformers train more slowly than classic transformers, and led many to dismiss the approach as nice-in-theory but unhelpful-in-practice [2].\nAt the moment, the conventional wisdom in the field is that a quadratic-cost algorithm with highly optimized hardware utilization (e.g. FlashAttention) gives the best training throughput [3]. But this is mistaken. In this post, we explain several different ways of implementing linear transformers and we show that, in fact, they can produce massive speed-ups.\nThe experiment below showcases the central takeaway. We compare the speed of an optimized transformer, a straightforward recurrent implementation of the linear transformer (as described in [1]), and an optimized linear transformer (described in Section 3). We see that, despite exhibiting better growth with respect to context size, the linear transformer trains much more slowly than the transformer in wall-clock time. In contrast, our algorithm is strictly faster than even the highly-optimized FlashAttention baseline, at all context and model sizes. At moderately large context sizes, this speedup has an larger impact than FlashAttention itself.\nBut speed is only one part of the picture. We also need to know whether linear transformers actually minimize the loss as well as standard transformers do. Unfortunately, as the next plot shows, directly converting GPT2 to use linear transformer layers hurts learning significantly.\nThese results are discussed in detail in Section 5, but in short: linear transformers seem to become increasingly unstable as the sequence length grows, negatively affecting learning. This means that our linear transformers are somewhat useless,1 as the positive impact from the speedup seen in long contexts is undermined by the negative impact of degraded learning.\nOther variants of linear transformers have been proposed that claim resolve these learning issues [5]–[11], but we do not survey them today. Since our main motivation in this post is to share our insights on how to implement efficient linear transformers we stick with the most natural variant, which is most closely analogous to standard transformers. In a future post, we will explain how to improve the learning of linear transformers, allowing us to efficiently train unprecedentedly-long-context language models with super-fast sampling."
  },
  {
    "objectID": "blogposts/faster-after-all/index.html#linear-transformers",
    "href": "blogposts/faster-after-all/index.html#linear-transformers",
    "title": "Linear Transformers Are Faster After All",
    "section": "1. Linear Transformers",
    "text": "1. Linear Transformers\nThe inputs to a transformer layer are sequences of \\(Q_i, K_i, V_i \\in \\R^d\\) of query, key and values, where \\(i\\) ranges from \\(1\\) to the sequence length \\(t\\). The outputs are a sequence \\(Y_i\\in \\R^d\\). The well-known formula for the transformer layer, first popularized by Vaswani et al [12], is: \\[\nY_i^\\text{Transformer} = \\sum_{j=1}^i e^{Q^T_i K_j} V_j\n\\] Here we are excluding the denominator of the softmax for simplicity of notation. Although the normalization provided by the denominator is important for good learning performance, it doesn’t have a major impact on the computational cost or implementation speed, which are our main focus here.\n\n\nEven though we omit the normalization for the mathematical exposition, it is included in our implementation for all experiments.\nThe formula for the linear transformer (LT) layer is quite similar: just change the term \\(e^{Q^T_i K_j} \\to  Q^T_i K_j\\) yielding \\[\nY_i^\\text{LinearTransformer} = \\sum_{j=1}^i Q^T_i K_j V_j\n\\]\n\n\nAll our experiments with linear transformers also include a normalization factor, which we empirically found to be important for learning performance. Drawing on [1], we divide each \\(Y_i\\) by \\(\\sum_{j=1}^i Q^T_i K_j\\) after eunsuring the sum is positive by making keys and queries live in the positive quadrant using softplus.\nThis layer is “linear” in that the outputs \\(Y\\) are linearly related to all of \\(Q\\), \\(K\\), and \\(V\\).2 From now on, we will omit the superscript of \\(Y_i^\\text{LinearTransformer}\\) and just write \\(Y_i\\). To begin our exploration of the computational cost of linear transformers, consider the following implementation.\ndef LT_attention(Q, K, V):\n    \"\"\"\n    Shapes of inputs are\n     Q: [t, d]  K: [t, d]  V: [t, d]\n    Shapes of outputs are\n     Y: [t, d]\n    \"\"\"\n    t, d = Q.shape\n    Y_list = []\n    for i in range(t):           # loop cost: O(t^2 d)\n        Y_i = zeros(d)\n        Q_i = Q[i]\n        for j in range(i+1):     # loop cost: O(id)\n            A_ij = inner(K[j], Q_i)  # cost: O(d)\n            Y_i += A_ij * V[j]   # cost: O(d)\n        Y_list.append(Y_i)\n    return stack(Y_list)\nAnyone who has worked with self-attention will recognize that this is a terrible implementation, since nested for-loops are not GPU-friendly. Don’t worry: in the next section, we will discuss implementations that parallelize computation, and thus run much faster on modern hardware. For now, the main point of this pseudocode is to highlight that first mode of computation, which we call attention formulation, has a FLOP cost of \\(O(t^2 d)\\).\nThe key to the massive speedups we saw in the introduction comes from leveraging linearity to restructure the computation. Consider the following factorization: \\[\nY_i = \\sum_{j=1}^i Q^T_i K_j V_j = \\underbrace{ \\left (  \\sum_{j=1}^i V_j  K_j^T\\right )}_{S_i} \\; \\; Q_i\n\\] Written in this form, we notice that the term labeled \\(S_i \\in \\R^{d\\times d}\\) can be thought of as a state summarizing all the relevant information up to time \\(i\\). It’s easy to rewrite into the following recurrent equations \\[\nY_{i} = S_i Q_i\n\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\nS_i = S_{i-1} + V_i K_i^T\n\\] where we assume \\(S_{0} = 0\\in \\R^{d\\times d}\\). Written in this form, we realize that a linear transformer is an RNN. We can also write pseudocode for this approach, which we call the state formulation, and analyze the cost:\ndef LT_state(Q, K, V):\n    \"\"\"\n    Shapes of inputs are\n     Q: [t, d]  K: [t, d]  V: [t, d]\n    Shapes of outputs are\n     Y: [t, d]\n    \"\"\"\n    t, d = Q.shape\n    S_i = zeros(d, d) # shape [d,d]\n    Y_list = []\n    for i in range(t):        # loop cost: O(t d^2)\n        S_i += outer(K[i], V[i]) # cost: O(d^2)\n        Y_i = S_i @ Q[i]      # cost: O(d^2)\n        Y_list.append(Y_i)\n    return stack(Y_list)\nWe see that the cost here is \\(O(t d^2)\\).\nSo, while a standard transformer layer always has cost \\(O(t^2 d)\\), linear transformers have two formulations with different costs. By switching from the attention formulation to the state formulation, we can change the cost from \\(O(t^2 d)\\) to \\(O(t d^2)\\), trading a \\(t\\) term for a \\(d\\) term."
  },
  {
    "objectID": "blogposts/faster-after-all/index.html#parallel-implementations",
    "href": "blogposts/faster-after-all/index.html#parallel-implementations",
    "title": "Linear Transformers Are Faster After All",
    "section": "2. Parallel Implementations",
    "text": "2. Parallel Implementations\nIn general, for-loops are a terrible way to implement anything that will run on a GPU. When using high-level frameworks like PyTorch or JAX, the easiest way to get high GPU utilization is to only use primitives that are already highly optimized for GPU: matrix multiplication, elementwise operations, etc. We can rewrite our attention and state algorithms in this style to make them efficient.\nFirst, let’s do this for attention. Our main technique is to compute the attention matrix \\(A\\), which contains all the terms outer(Q[i], K[j]) that appeared inside the for-loops of LT_attention, using a single heavyweight matrix multiply.\ndef LT_attention_parallel_no_flash(Q, K, V):\n    \"\"\"\n    Shapes of inputs are\n     Q: [t, d]  K: [t, d]  V: [t, d]\n    Shapes of outputs are\n     Y: [t, d]\n    \"\"\"\n    t = Q.shape[0]\n    M = causal_mask(t)\n    A_raw = Q @ K.T  # cost O(t^2 d)\n    A = A_raw * M    # cost O(t^2)\n    Y = A @ V        # cost O(t^2 d)\n    return Y\nThis implementation lies at the core of nearly every transformer of the past five years, powering all of the AI models that have recently exploded in popularity. Since it is composed of such a small number of ultra-parallelizable ops, it obtains high GPU utilization. Recently, specialized flash attention kernels [3] have been used to get even further speedups by avoiding explicitly storing the attention matrix \\(A\\), and thereby saving both memory and time spent on memory-transfers. Algorithmically, though, flash attention is a variant of parallel attention, and has the same computational cost (as measured in FLOPs). We use LT_attention_parallel to refer to the flash attention implementation.\nNext, let’s parallelize the recurrent formulation. This optimization is less well-known. The key is to compute all the terms \\(V_i K^T_i\\) in parallel, and then use a cumulative-sum, which can be parallelized, to combine them.\ndef LT_state_parallel(Q, K, V):\n    \"\"\"\n    Shapes of inputs are\n     Q: [t, d]  K: [t, d]  V: [t, d]\n    Shapes of outputs are\n     Y: [t, d]\n    \"\"\"\n    P = V[:,:,None] @ K[:,None,:]  # cost: O(t d^2)\n    S = cumsum(P, axis=0)          # cost: O(log_2(t) t d^2)\n    Y = S @ Q[:,:,None]            # cost: O(t d^2)\n    return Y[:,:0]\nThe cost in FLOPs of this algorithm is \\(O(\\log_2(t) t d^2)\\).3\nNow that we have four mathematically-equivalent implementations of a linear transformer, let’s time the training step of our GPT2 models and see how big of a difference it makes. For our LT_attention_parallel implementation, we use a custom linear self-attention flash kernel we implemented in Triton [13] based on OpenAI’s FlashAttention2 implementation.\n\n\n\n\nHere are some takeaways:\n\nAs expected, the attention variants all have a quadratic asymptotic cost (slope of 2 on a log-log plot4). The state variants all have linear asymptotic cost (slope 1). 5\nLT_state_parallel is an order-of-magnitude faster than LT_state.\nLT_attention_parallel_no_flash is two orders-of-magnitude faster than LT_attention.\nLT_attention_parallel seems to asymptotically stabilize into being an order-of-magnitude faster than LT_attention_parallel_no_flash.\nFor the majority of settings, LT_attention_parallel is the fastest. (This is the linear version of the algorithm used by the standard transformer.)\nParallel attention is the fastest algorithm for small context sizes. However, LT_state_parallel overcomes LT_attention_parallel_no_flash at around 13k context size, and overcomes LT_attention_parallel at around 100k.\n\nOverall, these results paint a clear picture: use the attention algorithm for small contexts and the state algorithm for large contexts. But do we really face a binary choice? Can we combine the state and attention ideas and get the best of both words?"
  },
  {
    "objectID": "blogposts/faster-after-all/index.html#chunked-formulation",
    "href": "blogposts/faster-after-all/index.html#chunked-formulation",
    "title": "Linear Transformers Are Faster After All",
    "section": "3. Chunked Formulation",
    "text": "3. Chunked Formulation\nIt’s evident that, for small context sizes, computing the \\(t\\) by \\(t\\) attention matrix is much more efficient than computing many \\(d\\) by \\(d\\) state matrices. But as \\(t\\) grows, there is a point where the quadratic cost of the attention matrix ends up dominating. Noticing that attention is extremely efficient for small \\(t\\) and that states are necessary for large \\(t\\) motivates doing one last reworking of the LT equation.\nLet \\(c \\in \\N\\) be a positive integer that we’ll call the chunk size. For any \\(i\\in \\N\\) find the unique \\(n\\in \\Z\\) s.t. \\(cn &lt; i \\le c(n+1)\\). We can easily see that the following equations are equivalent to the previous ones. \\[\nY_{i} = S_{cn}Q_i + \\sum_{j=cn+1}^i Q_i^T K_j V_j\n\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\nS_{c(n+1)} = S_{cn} + \\sum_{j=cn+1}^{c(n+1)} V_j K_j^T\n\\] The key idea is that we are only going to compute a subset of all states: \\(S_0, S_c, S_{2c}, \\cdots\\). Then, to compute each output \\(Y_i\\), we need only to take into account the contribution via the most recent state \\(S_{cn}\\), as well as the contribution (computed via attention) of all moments in time \\(j\\) in the range \\(cn &lt; j \\le i\\).\nAs pseudocode, this looks like:\ndef LT_attention_with_initial_state(S, Q, K, V):\n    \"\"\"\n    Shapes of inputs are\n     S: [d, d]  Q: [c, d]  K: [c, d]  V: [c, d]\n    Shapes of outputs are\n     Y: [c, d]\n    \"\"\"\n    Y_state = Q @ S                               # cost O(c d^2)\n    Y_attention = LT_attention_parallel(Q, K, V)  # cost O(c^2 d)\n    Y = Y_state + Y_attention                     # cost O(cd)\n    return Y\n\ndef LT_chunked(Q, K, V, c):\n    \"\"\"\n    Shapes of inputs are\n     Q: [t, d]  K: [t, d]  V: [t, d], c: int\n    Shapes of outputs are\n     Y: [t, d]\n    \"\"\"\n    t, d = Q.shape\n    assert t % c == 0\n    Q_, K_, V_ = [arr.reshape(t//c, c, d)\n    `               for arr in [Q,K,V]]\n    P_ = K_.transpose([0,2,1]) @ V_  # cost O(t d^2)\n    S_ = cumsum(P_, axis=0) - P_     # cost O(log_2(t/c)(t/c)d^2)\n    Y_ = vmap(LT_attention_with_initial_state, axis=0)(\n                S_, Q_, K_, V_)      # cost O(td^2 + tcd)\n    return Y_.reshape(t, d)\nThe cost is \\(O\\left(td^2 + tcd + \\log_2(t/c)(t/c)d^2\\right)\\), once again avoiding a quadratic dependency on \\(t\\). Also, note that this algorithm makes an inner call to LT_attention_parallel, so we can use a flash-attention kernel to do that part of the computation.\nThis algorithm has a hyperparameter, the chunk size, which must be set correctly for optimal performance. Fortunately, it is inexpensive to identify the best chunk size empirically, since measuring just a few steps of training at each chunk size is sufficient to identify which is the fastest. In the plot below, we plot the speed of the optimally-chunked algorithm in each setting.\n\n\n\n\nWe see LT_chunked gives the desired best-of-both-worlds behavior, as it is equal-or-better than all other approaches in all settings. And we now see the massive (& rapidly growing) speedup relative to standard self-attention, finally unlocking the true power of linear transformers."
  },
  {
    "objectID": "blogposts/faster-after-all/index.html#sampling",
    "href": "blogposts/faster-after-all/index.html#sampling",
    "title": "Linear Transformers Are Faster After All",
    "section": "4. Sampling",
    "text": "4. Sampling\nWhen working with language models, efficient training is not the only performance that deserves consideration. Once the model is trained, how expensive is it to utilize? When we are sampling we have a sequence of tokens, \\(z_1 \\cdots z_t\\), and we want to sample the next token, \\(z_{t+1}\\).\nThe most efficient algorithm to sample from traditional transformers is called the KV-cache algorithm [14]. This algorithm assumes that when we generate token \\(z_{t+1}\\), we will have already computed and cached all the \\(K_i, V_i\\) for all \\(0 \\le i \\le t\\). In order to compute the output of the attention layer at time \\(t+1\\) given this cached information, we can use \\[\nY_{t+1}^\\text{Transformer} = \\sum_{j=1}^{t+1} e^{Q^T_i K_j} V_j\n\\] It is easy to see that this is an \\(O(td)\\) operation. In other words, as the sequence length grows, sampling each subsequent token becomes more computationally expensive.6 This is one of the major limitations of the classic transformer architecture.\nWith linear transformers, however, we have access to the recurrent formulation. A linear transformer is an RNN where the state has size \\(O(d^2)\\). \\[\nY_{i} = S_i Q_i\n\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\nS_i = S_{i-1} + V_i K_i^T\n\\] We can compare the time it takes to generate any particular token when sampling a sequence:\n\n\n\n\nAs expected, we see that the time to sample of the linear transformer is independent of context length, whereas that of the KV-cache transformer grows linearly. This leads to a large gap in inference speed at large contexts.7"
  },
  {
    "objectID": "blogposts/faster-after-all/index.html#learning-performance",
    "href": "blogposts/faster-after-all/index.html#learning-performance",
    "title": "Linear Transformers Are Faster After All",
    "section": "5. Learning Performance",
    "text": "5. Learning Performance\nUntil now, our focus has been on how to implement linear transformers efficiently. But an architecture that runs really fast is useless unless it also learns well. We will now compare the learning curves of the GPT2 baselines with their linear transformer counterparts, at various context sizes.\nIn order to control for the fact that longer contexts help learning by introducing more tokens per update, we hold the number of tokens-per-update constant across context sizes by decreasing batch size. At all context-lengths, each update sees \\(2^{19}\\) tokens.8 Importantly, for this set of experiments, we have used the dataset c4 [4], which does not have much long-term structure: it consists of a shuffled collection of short articles, of average length ~500 tokens.\nFirst, let’s look at the parameter scaling law of GPT2 and our modified Linear-GPT2. The following plot shows the final performance after 24h of training on our 8xH100 server for each scale and each context size. Click the second tab if you want to see the full learning curves.\n\n\n\n\nBoth architectures scale similarly with respect to model size, but there is a gap in performance. This gap seems to grow as context size increases, together with more loss spikes and general instability. It’s possible that the gap can be explained by the linear transformer not effectively utilizing its context. To explore whether or not this is the case, we can use these same experiments, but visualize all context-lengths together.\n\n\n\n\nWe see a dramatically different trend between the two architectures. For GPT2, each increase in context length slows down the initial pace of learning, but ultimately all context-lengths (beyond at least 1024) converge to a similar final loss. Whereas for Linear-GPT2, not only does increasing the context slow down learning in the beginning, but it also causes convergence to a worse final performance and nasty-looking loss spikes.\nThe results for GPT2 are what we would expect from a healthy model, given the setting. Note that since our dataset consists of short articles, of average length ~500 tokens, we can assume that even a context window of 1024 would include nearly everything relevant. Thus, we wouldn’t expect that increasing the context length beyond this point would decrease the loss the model ultimately converges to. Longer-context models do however need to learn to ignore many more irrelevant tokens, explaining the slowed initial learning.9\nIn contrast, the results for Linear-GPT2 seem to indicate some underlying instability of the linear transformer architecture. It doesn’t even seem capable of ignoring irrelevant information, let alone exploiting useful long-term structure.\nRemedying these learning issues will be the main focus of our future work in linear transformers. Our current architecture is essentially a one-to-one clone of GPT2, sticking as architecturally close to the original as possible; aspects such as initializations, normalizations and hyperparameters were directly copied. It is well-known that these decisions can have a large impact on scaling and stability and often need to be tuned in an architecture-specific way. In the literature, various other linear variants of self-attention have been proposed, that incorporate techniques such as gating mechanisms in order to improve stability and learning [5]–[11]. A future post will include a thorough study of the impact of all of these choices.\nUltimately, it may be impossible for any linear transformer to perfectly match the context scaling laws of the classic transformer baseline. Although removing the exponential seems like a minor change, it represents a meaningful decrease in expressivity.10 But as we’ve shown, linear transformers can be trained orders-of-magnitude more efficiently. On equivalent compute, linear transformers can be trained for many more steps; or, keeping steps constant as well, we can train a much larger model. If this additional scaling is sufficient to compensate for the reduced expressivity, linear transformers will be the more efficient architecture overall.\n\n  \n  Subscribe to be notified of new posts:"
  },
  {
    "objectID": "blogposts/faster-after-all/index.html#footnotes",
    "href": "blogposts/faster-after-all/index.html#footnotes",
    "title": "Linear Transformers Are Faster After All",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs discussed in Section 4, a second benefit of linear transformers is that the cost to sample a token does not grow with context size. Perhaps one could argue that this improvement in sampling speed could, on its own, justify using linear transformers for applications where the inference costs vastly exceed training costs. But it is evident to us that, for linear transformers to become actually useful, we need to address these instability issues.↩︎\nIt is not named for the fact that the computational cost is linear with respect to \\(t\\)! That is just a coincidence. (And is not even always true, as we will see.)↩︎\nInterestingly, LT_state_parallel is actually more expensive than LT_state. (This is in contrast with the attention formulation, where LT_attention and LT_attention_parallel share the same \\(O(t^2 d)\\) cost.) As we will see in a moment, this extra \\(\\log_2(t)\\) factor is well worth the parallelization benefits.↩︎\nIf \\(y=x^2\\), a log-log plot where \\(y'=\\log_a(y)\\) and \\(x'=\\log_a(x)\\) for any base \\(a\\), then \\(y'=\\log_a(y) = \\log_a(x^2) = 2 \\log_a(x) = 2 x'\\). So the graph will be a line with slope 2.↩︎\nThe reason we see the expected slopes asymptotically is that we are timing a full GPT2 architecture which has many other components besideds the attention layer. If we were only timing the attention layer, the plots would all be straight lines.↩︎\nAn interesting connection is that the KV-cache can be understood as the state of an RNN with non-constant state size; namely, one whose state-size is \\(O(td)\\).↩︎\nThis comparison may not be completely fair. In these experiments, our implementation of neither sampling algorithm makes use of specialized kernels. A lot of the ideas of flash attention can be used to write a much faster KV cache sampling algorithm; on the other hand, it’s unclear if much improvement is possible on the recurrent sampling. Thus, it’s possible that with engineering effort the gap between the two algorithms could become smaller. However, the overall pattern will certainly remain the same.↩︎\ne.g. runs with context-size 1024 would have batch-size of \\(2^{19} / 2^{10} = 2^{9} = 512\\).↩︎\nPut another way: doubling the size of the input vastly increases the size of the function space over which gradient descent must search, and it’s intuitive that in a larger space it takes somewhat longer to find a good solution.↩︎\nWe plan to elaborate on this topic in a future blog post.↩︎"
  },
  {
    "objectID": "drafts/research/index.html",
    "href": "drafts/research/index.html",
    "title": "An Architecture For Neural Language Modeling",
    "section": "",
    "text": "\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\Z}{\\mathbb{Z}}\n\\newcommand{\\N}{\\mathbb{N}}\n\\newcommand{\\sft}{\\text{softmax}}\n\\newcommand{\\List}{\\text{List}}\n\\newcommand{\\Seq}{\\text{Seq}}\n\\newcommand{\\SeqT}{\\text{SeqT}}\n\\newcommand{\\CSeqT}{\\text{CSeqT}}\n\\newcommand{\\Dist}{\\text{Dist}}\n\\newcommand{\\SM}{\\text{SM}}\n\\newcommand{\\id}{\\text{id}}\n\\newcommand{\\Fn}{\\text{Fn}}\n\\newcommand{\\Tok}{\\text{Tok}}\n\\newcommand{\\Aij}{ A_{[i,j]}}\n\\]\nA crucial decision in neural language modeling is the choice of architecture, which governs the compuatational and statistical properties of the model. A well-selected architecture can improve performance-per-dollar by orders of magnitude.\nThe objective of this work is to find an architecture with the following four properties:\nAt the time of writing, the most popular architecture for neural language modeling, the Transformer, has only three of these properties (it lacks efficient sampling). Many other architectures have been been proposed, but none that meets all four criteria.\nWe believe that such an architecture exists. This document is an expository writeup of progress we have made towards the goal of discovering it. This document contains our evolving understanding of the key concepts and results needed to understand neural language modeling, including mathematical, computational, and experimental results. Unlike a traditional research paper, this is living document that will continually be updated as we make progress.\nMuch of the content of this document will already be familiar to anyone who has thought deeply about language modeling. There is nothing particularly groundbreaking here. But we found a lot of value in explicitly formalizing the concepts, e.g. how think of Transformers and RNNs as members the same family; as well as grounding all of these ideas in rigorous mathematics. Also, in order to properly understand our specific setting, we often find that it is helpful to describe more general objects of which neural language modeling is a specific case. Where applicable and insightful, we provide such results in their full generality; and as such, this document contains many ideas that may be of independent interest to researchers in related areas.\nIt’s not about minimizing floating point operations. It’s about optimizing tokens per second on our hardware. GPUs perform well with tasks that hare highly parallelizable. RNNs, the more “clasical” architectures for sequences, are not very well suited for these types of tasks.\nimportant feature of the transformer architecture is that it can compute We find that it’s important to think about sequences, and maps between sequences as the core objects."
  },
  {
    "objectID": "drafts/research/index.html#sequence-transformations",
    "href": "drafts/research/index.html#sequence-transformations",
    "title": "An Architecture For Neural Language Modeling",
    "section": "2.1) Sequence Transformations",
    "text": "2.1) Sequence Transformations\nDefinition We say \\(h: \\Seq X \\to \\Seq Y\\) is a sequence transformation if \\(h\\) preserves the length of the sequence. Meaning that if the input sequence has lenght \\(t\\), so does the output sequence.\nWe refer to \\(\\SeqT(X, Y)\\subset \\Fn(\\Seq X, \\Seq Y)\\) as the subset of all sequence transformations.\nIn deep learning we care about composing multiple layers into more complex architectures. Sequence transformations are very well behaved in this way, as the composition of two sequence transforms is a sequence transform itself.1 This is a completely obvious fact, but since it is quite an important one it’s worth stateting it as a result.\nResult: If \\(h\\in \\SeqT(X, Y)\\) and \\(f\\in \\SeqT(Y, Z)\\) then \\(f\\circ h \\in \\SeqT(X,Z)\\).\nIn the next section we’ll see how RNNs and Transformers are examples of sequence transformations. So are elementwise functions on the sequence elements.\nJust from a quick glance at the list above, it’s quite obvious that many lerning problems naturally involve sequence transformations. For tasks like protein folding and image coloring, each entrie of the output sequence is trying to predict missing information of the same entrie on the input sequence. Thus, for this type of tasks we need the output sequence to have the same length as the input one, and so we our model architectures must be a sequence transformation. But of course that isn’t always the case. In particular our central objective of autoregressive language modeling requires architectures with a different signature \\(\\Seq X \\to Y\\).\nWhat is surprizing is that even for tasks like these, sequence transformations turn out to be an extremely useful building block that is used under the hood. To see why, take a moment to think about how you might construct an architecture with the desired input and output spaces. In one side you have the space \\(\\Seq X\\), where a point might contain an unboudedly large amount of informaiton. On the other, you have a \\(Y\\), which often is a finite dimensional vector space. \\(Y\\) is a much “bounded” space than \\(\\Seq X\\). When designing your architecture you are forced to decide where to place the projection from the variable sized space to the fixed sized one. Given that neural network architectures are allways constructed as the composition of many individual layers, there are two very natural choices of where to place the projection: in the beginning, or at the end. We would call the choice early drop and late drop respectively. Early drop architectures would look something like \\[\n\\Seq X \\to \\R^d \\to \\cdots \\to \\R^d \\to Y\n\\]\n\n\n\\(d\\) represents the width of each layer of the neural network. The arrows represent the individual layers of the neural network.\nand the late drop type would be constructed out of many sequence transforms layers except for the last one \\[\n\\Seq X \\to \\Seq \\R^d \\to \\cdots \\to \\Seq \\R^d \\to Y\n\\]\nOne can definetely construct neural network architectures in the two ways, and we are about to see some examples of both. But it does seem to be the case that all the architectures we actually used in practice (e.g. transformers and RNNs) are of the drop last type. We could speculate about why:\n\nusing sequences interally is a good inductive bias for tasks that are naturally expressed as sequences.\nadaptive computation. More complex tasks have more internal state abailable to solve them\n\nBut honestly, that just leads to a lot of talk with little substance. What is certainly true is that the second type is the one with widespread use, it seems to be the deep learning way of doing things. Moreover, in section 3 we will see how thinking about architectures based on sequence transforms enable one massive optimization to train autoregressive language models.\nTODO: Even when your architecture can be thought of in a different way, sequence transform perspective is what allows you to implement them efficiently on hardware. Unleash all your cores to compute different parts of the output all at the same time. Unconstrain yourself"
  },
  {
    "objectID": "drafts/research/index.html#example-architectures",
    "href": "drafts/research/index.html#example-architectures",
    "title": "An Architecture For Neural Language Modeling",
    "section": "2.3) Example Architectures",
    "text": "2.3) Example Architectures\nTODO: make the point that sequence transforms give us a unified perspective to think about RNNs and Transformers\n\n2.3.1) Recurrent Neural Networks\nAn RNN has the signature \\[\nr(s_{t-1}, x_t) = (s_t, y_t)\n\\]\nFor example, a very basic RNN layer might be something like \\[\ns_{i+1} = \\sigma(W s_i) + x_i  \\;\\;\\;\\;\\;\\;\\;\\;\\;\\; y_i = U s_i\n\\]\nwhere \\(x_i, s_i, y_i \\in \\R^d\\) are the inputs, states and outputs respectively and \\(W, U \\in \\R^{d\\times d}\\) are weight matrices and \\(\\sigma\\) is a nonlinearlity like a sigmoid. Of course, there is an infinitude of variations. The specific step equations matter much less than the general pattern of a layer with the signature of \\(r\\).\nAs written, it’s not clear that we have a sequence transformation in our hads. But once we pick an initial state \\(s_{-1}\\in \\R^d\\), any function with such a recurrent formulation can be Given the input sequence \\(x_1 \\cdots x_t\\) we can unrolled the recurrent step and compute the sequence \\(y_1 \\cdots y_t\\). Thus we have a sequence transform in our hands.\nIn deep learning we will want to stack many such layers \\(r\\). For example let’s consider stacking \\(n\\) layers \\(r\\) to construct a deep RNN. The stack of layers will have the same signature of our basic RNN, the only difference is that the state will consist of \\(n\\) vectors in \\(\\R^d\\). The following diagram represents the computation of a deep RNN with 2 layers and \\(t=3\\). The blue boxes represent each step of unrolling the RNN through time.\n But people don’t tend to implement deep RNNs this way! Anyone who has seen an implementation of an RNN will recognize that the actual computation we implement is instead:\n\n\n\nFile (12)\n\n\nIt’s like we try to avoid the recurrent formulation of a deep RNN and instead prefer to see it as a stack of sequence transformations. But why? Ofc they are mathematicaully equivalent, but there is a difference in practical performance in hardware: cache hits EXPAND FURTHER. We start to see how thinking about sequence transforms seems to have some computational advantages.\nTo create the function we need for our autoregressive LM we could take a stack of RNN layers and on the last layer throw out all the outputs except for \\(y_t\\). That gives us a function with signature \\(\\Seq X\\to Y\\).\n\n\n2.3.2) Transformers\nA transformer layer takes in a sequence of inputs \\(X_1 \\cdots X_t \\in \\R^d\\) and uses weight matrices \\(W_Q, W_K, W_v \\in \\R^{d\\times d}\\) to compute \\(Q_i = W_Q X_i\\) etc.. The outputs of the layer are \\[\nY_i = \\sum_{j=0}^t e^{Q_i K_j^T} V_j\n\\]\nAn the causal transformer layer is \\[\nY_i = \\sum_{j=0}^t e^{Q_i K_j^T} V_j\n\\]\nThey are both clearly sequence transformations since we get \\(t\\) outputs \\(Y_i\\).\nOne massive advantage of transformers over alternative architectures like the RNNs we’ve just seen is that they are highly parallelizable. Our GPUs with many cores can split the work of computing all the \\(Y_i\\) in parallel. You don’t need to finish computing \\(Y_1\\) before you can move on to \\(Y_2\\).\nThe potential for this parallelism is something quite natural when one thinks in terms of sequence transformations \\(h: \\Seq X \\to \\Seq Y\\). There is nothing inherent that says that one must compute the output sequence one step at a time.\nTo create a practical architecture for our autoregressive language modeling we would stack a sereies of transformer layers, normally with some MLPs in between. Just like for RNNs, we would throw out all the outputs of the last layer except for \\(Y_t\\). That will gives us a function with the desired signature \\(\\Seq X\\to Y\\).\nNote how it really doesn’t matter is we use causal or non causal transformers for this task. We can get an autoregressive LM out of each. Yes, there is a very good reason to use causal transformers, but to see why we will need to think carefuly about training costs. Something that we will do in section 3.\n\n\n2.3.3) k-gram MLP\nWe’ve seen a couple of common examples of “drop last” architectures. Let’s give 1 of the “drop first” type. As stated, this is not a common thing to do, and the experienced deep learning practitioner will recognize it’s kind of funky. Let’s do it non the less for the sake of completeness\nInspired by the classic \\(k\\)-gram models. If the input sequence is \\(\\Seq \\R^d\\), we could “stack” the \\(k\\) last inputs and apply an MLP to the vector in \\(\\R^{nd}\\). The last layer of the \\(MLP\\) would map into \\(Y\\).\nIndependent of how well this arch learns. It turns out that it is very poorly behaved computationally compared to RNNs and causal transformers. In section 3 we will see how this architecture is poorly suited for the porpuses of autoregressive language modeling in the same way than non causal transformers are."
  },
  {
    "objectID": "drafts/research/index.html#cost-of-training",
    "href": "drafts/research/index.html#cost-of-training",
    "title": "An Architecture For Neural Language Modeling",
    "section": "3.1) Cost of Training",
    "text": "3.1) Cost of Training\nNow, let’s return to considering autoregressive language modeling, \\(f : \\Seq X \\to \\Dist X\\). How many times do we have to call the underlying function \\(f\\) to evaluate the loss \\(l\\)?\nWhen we evaluate the loss we have to compute \\[ l = \\sum_{x\\in \\Seq_t X} \\sum_{i=1}^t -\\ln[f(x_1 \\cdots x_{i-1})(x_i)]\\]\nFor example, suppose our dataset consists of a single sentence: \\(\\text{\"I like penguins\"}\\). To evaluate the loss we need to evaluate our model \\(f\\) on the empty sequence \\(\\text{\"\"}\\) and the first character \\(\\text{\"I\"}\\) and \\(\\text{\"I \"}\\) and \\(\\text{\"I l\"}\\), \\(\\text{\"I lik\"}\\) etc..\n\\[\nf(\\text{\"\"}) \\\\\nf(\\text{\"I\"}) \\\\\nf(\\text{\"I \"}) \\\\\nf(\\text{\"I l\"}) \\\\\nf(\\text{\"I li\"}) \\\\\n\\vdots\n\\]\n\n\n\n385505895_353915030504501_2594593902089499332_n.jpeg\n\n\nSo, for every sequence \\(x_1 \\cdots x_t \\in D\\), to compute the loss we need to call our model on \\(t\\) different inputs.\nThe motivatiion to find a speed up We have been considering the possibility of using an architecture of the type \\[\nf:\\Seq X \\to \\Seq \\R^d \\to \\cdots \\to \\Seq \\R^d \\to Y\n\\]\nbut it might be dumb that all the steps of the computation \\(f(x\\cdots x_t)\\) involve sequences \\(t\\) and at the end we project down and output a vector.\nPerhaps, we could use a different architecture that was a sequence transformation\n\\[\ng: \\Seq X \\to \\Seq \\R^d \\to \\cdots \\to \\Seq \\R^d \\to \\Seq Y\n\\]\nThe dream would be that \\(g(\\text{\"I like penguins\"}) = [f(\\text{\"I\"}), f(\\text{\"I \"}), f(\\text{\"I l\"}), f(\\text{\"I li\"}), \\cdots]\\), so the output sequence contains all the values we need in a single call to our architecture."
  },
  {
    "objectID": "drafts/research/index.html#causal-sequence-transformations",
    "href": "drafts/research/index.html#causal-sequence-transformations",
    "title": "An Architecture For Neural Language Modeling",
    "section": "3.2) Causal Sequence Transformations",
    "text": "3.2) Causal Sequence Transformations\nThe fundamental principle of causality is that the past does not depend on the future. The way to formalize this intuition is to say that evaluating a causal sequence transformation on a subsequence produces an output sequence that is also a subsequence.\nDefinition We say \\(h\\in \\SeqT(X,Y)\\) is a causal sequence transformation if, given a sequence \\(x_1 \\cdots x_t \\in \\Seq_t X\\) with output sequence \\(y_1 \\cdots y_t = h(x_1 \\cdots x_t)\\), for any \\(i\\le t\\), evaluating \\(h\\) on the subsequence \\(x_1 \\cdots x_i \\in \\Seq_i X\\), the outputs \\(y'_1 \\cdots y'_i = h(x_1 \\cdots x_i)\\) have the property that \\(y_j = y'_j\\) for all \\(j\\le i\\).\nWe refer to \\(\\CSeqT(X, Y) \\subset \\SeqT(X, Y)\\) as the subset of all causal sequence transformations.\nThe reader is encouraged to check that all functions below are causal sequence transformations * RNNs as defined in EQX * Transformers as defined in EQY * Elementwise functions \\(h(x_1 \\cdots x_t) = \\sigma(x_1) \\cdots \\sigma(x_t)\\)\nJust like sequence transformations, causal sequence transformations form a category, meaning that they are closed under composition. This means we can use layers that we know are causal sequence transforms and combine as building blocks into more complex architectures.\nResult: If \\(h\\in \\CSeqT(X, Y)\\) and \\(f\\in \\CSeqT(Y, Z)\\) then \\(f\\circ h \\in \\CSeqT(X,Z)\\). Proof Bruh, why you expanding this proof? Don’t be lazy and do it yourself. It’s real easy.\nFor example, a full transformer architecture usually would be constructed by composing many transformer blocks. Where each block consists of a transformer layer as in EQY followed by an MLP applied elementwise to all outputs of the transformer layer. Thanks to the previous result we know that the full transformer architecture will be causal just by confirming that elementwise functions and EQY are causal sequence transformations.\nAlright, now that we understand causal sequence transformations and we have a vast space of such architectures at our dispoal, let’s see why they are so useful for the problem we were trying to solve.\nDefinition: The “take last” function \\(L: \\CSeqT(X, Y) \\to \\Fn(\\Seq X, Y)\\) is defined the following way: given an \\(h\\in \\CSeqT(X,Y)\\), if \\(x_1 \\cdots x_t \\in \\Seq_t X\\) then \\(L(h)(x_1 \\cdots x_t) = [h(x_1 \\cdots x_t)]_t\\). In other words, if \\(y_1 \\cdots y_t = h(x_1 \\cdots x_t)\\) then \\(L(h)(x_1 \\cdots x_t) = y_t\\).\nResult \\(L\\) is a 1-1 mapping where the inverse “concatifies” a function \\(f \\in \\Fn(\\Seq X, Y)\\) by evaluating it on \\(x_1\\) and \\(x_1, x_2\\) etc.. Concretely: \\[\nL^{-1}(f)(x_1 \\cdots x_t) = \\left(f(x_1), f(x_1, x_2), \\cdots, f(x_1 \\cdots x_t) \\right)\n\\]\nProof First note that, given an \\(f\\in \\Fn(\\Seq X, Y)\\), if \\(h\\) is the concatification of \\(f\\), meaning that \\(h(x_1 \\cdots x_t) = \\left(f(x_1), f(x_1, x_2), \\cdots, f(x_1 \\cdots x_t) \\right)\\) then clearly, \\(L(h)(x_1 \\cdots x_t) = f(x_1 \\cdots x_t)\\).\nWe also need to check the other direction. Given an \\(h\\in \\CSeqT(X, Y)\\): \\[\\begin{align}\nh(x_1 \\cdots x_t) &= \\left([h(x_1 \\cdots x_t)]_1, [h(x_1 \\cdots x_t)]_2, \\cdots, [h(x_1 \\cdots x_t)]_t \\right)  \\\\\n&= \\left([h(x_1)]_1, [h(x_1, x_2)]_2, \\cdots, [h(x_1 \\cdots x_t)]_t \\right)   &  \\text{using the fact that $h$ is causal}  \\\\\n&=  \\left(L(h)(x_1), L(h)(x_1, x_2), \\cdots, L(h)(x_1 \\cdots x_t) \\right) &  \\text{using the definition of $L$}\n\\end{align}\\]\nso if we concatify \\(L(h)\\) we get \\(h\\) back. \\(\\blacksquare\\)\nIn summary, \nWe’ve established a 1-1 mapping between the two function spaces. But computationally there is a very big difference between applying \\(L\\) and applying \\(L^{-1}\\). One is expensive the other is cheap. The color of the arrows represent that.\nWe get another interesting diagram when we combine this result with the previous one we get the following picture (where horizontal lines indicate the function is invertible and vertical ones indicate the function is surjective)\n\n\n\nFile (6)\n\n\nWhen we are trying to construct our distribution over length \\(t\\) sequences we loose no expresivety if we start with an \\(h\\in \\CSeqT(X, \\Dist X)\\) and we apply \\(A_t\\circ L: \\CSeqT(X, \\Dist X) \\to \\Dist \\Seq_t X\\). For any \\(f\\in \\Dist \\Seq_t X\\) there will be an \\(h\\in \\CSeqT(X, \\Dist X)\\) s.t. \\(f = A_t \\circ L (h)\\). So mathematically it really doesn’t matter if our base architecture is \\(g\\in \\Fn(\\Seq X, \\Dist X)\\) or \\(h\\in \\CSeqT(X, \\Dist X)\\), but computationally it makes a massive differene!\nSuppose \\(g = L (h)\\). Then, to evaluate the loss on a single sequence \\(\\text{\"hat\"})\\) we required 3 invokations of the function \\(g\\) \\[\ng(\\text{\"h\"}), \\; g(\\text{\"ha\"}), \\; g(\\text{\"hat\"})\n\\]\nBut when we evaluate \\(h(\\text{\"hat\"})\\), since \\(h = L^{-1} \\circ L (h) = L^{-1} \\circ g\\) then \\[\nh(\\text{\"hat\"}) = L^{-1}(g)(\\text{\"hat\"})  = \\left (g(\\text{\"h\"}), \\; g(\\text{\"ha\"}), \\; g(\\text{\"hat\"})  \\right)\n\\]\nFor a sequence of length \\(t\\), just by evaluating \\(h\\) a single time we get the \\(t\\) evaluations of \\(f\\) that we need for the loss. Using a causal sequence transformation is an incredible bargain. You compute 1 and get \\(t-1\\) for free!"
  },
  {
    "objectID": "drafts/research/index.html#efficient-sampling",
    "href": "drafts/research/index.html#efficient-sampling",
    "title": "An Architecture For Neural Language Modeling",
    "section": "3.3) Efficient Sampling",
    "text": "3.3) Efficient Sampling\nCan we do even better? Let’s consider what would it would look like to follow the autoregressive sampling procedure for the RNN of EQX. First we call \\(g(x_1)\\) which requires \\[\ns_1, y_1 = r(s_0, x_1) \\;\\;\\;\\;\\; z_1 \\sim y_1 \\in \\Dist Z\n\\]\nThen we want to call \\(g(x_1, x_2)\\) which requires \\[\ns_1, y_1 = r(s_0, x_1) \\;\\;\\;\\;\\; s_2, y_2 = r(s_1, x_2) \\;\\;\\;\\;\\; z_2 \\sim y_2 \\in \\Dist Z\n\\]\nAnd we can already see the problem. The second invocation of \\(g\\) internally recomputes the term \\(s_1\\) which was already computed in the first call. This is a consequence that the way we are calling \\(g\\) is very structured. And often there is potential for cacheing values and reusing computation between the individual calls.\nIn the case of an RNN, the way you actually want to sample is to first compute \\(s_1, y_1 = r(s_0, x_1)\\) and sample \\(z_1 \\sim y_1\\). Then reuse \\(s_1\\) and compute \\(s_2, y_2 = r(s_1, x_2)\\) and sample \\(z_2 \\sim y_2\\) and so on. This is a big optimization because we are cacheing computation into the state."
  },
  {
    "objectID": "drafts/research/index.html#state-machines",
    "href": "drafts/research/index.html#state-machines",
    "title": "An Architecture For Neural Language Modeling",
    "section": "3.4) State Machines",
    "text": "3.4) State Machines\nTo optimize training we exchanged \\(t\\) calls to an expensive function \\(g\\) into \\(1\\) call to an equally expensive function \\(h\\). To optimize sampling we will echange \\(t\\) calls to an expensive function \\(g\\) into \\(t\\) calls to a function \\(r\\) which is cheaper because it reuses cached computation from the previous calls.\nDefinition: A state machine A state machine in \\(\\SM(X,Y)\\) is a tuple \\((S, s_0, r)\\) where \\(S\\) is a set called the state space, \\(s_0 \\in S\\) is a point we call the origin and \\(u: S, X \\to S, Y\\) is a step/tick function \\[\nr(s_{t-1}, x_t) = (s_t, y_t)\n\\]\nThere is no difference between these equations and the basic form of an RNN, but there are a few reasons we are choosing to call the object state machine instead of RNN. * We will apply this type of equation to things that in no way can be considered neural networks. A good example is a tape, which we will discuss shortly. Using the term NN (i.e. neural network) to refer to such objects feels silly. * Another reason is that for RNN it is usually assumed that \\(s_t \\in \\R^d\\) for some \\(d\\in \\N\\). In other words, the state has a “fixed size”. As we will now see, to be able to unify transformers and RNNs, it’s very important to think in terms of state machines with states of “growing size”.\nIt’s kind of obvious that, if the whole idea of state machines is to allow for the cacheing of information during the repreated invokations of \\(g\\), there will always be a “worst case state machine” for any \\(g\\): the state machine that caches no computations. We call this the tape state machine because the only thing it stores is the inputs.\nDefinition: The tape state machine \\(P(g)\\in \\SM(X,Y)\\) of a function \\(g\\in \\Fn(\\Seq X, Y)\\) is \\[\nP(g)=( \\underbrace{\\Seq X}_S, \\underbrace{[\\;]}_{s_0}, r_g )\n\\]\nwhere the tick funciton \\(r_g\\) is defined as \\[\nr_g \\bigg( \\underbrace{[x_1 \\cdots x_{t-1}]}_{s_{t-1}}, \\; x_t \\bigg)\n= \\bigg( \\underbrace{[x_1 \\cdots x_t]}_{s_t}, \\; \\underbrace{g(s_t)}_{y_t} \\bigg)\n\\]\nThe tape state machine might seem like a bit of a silly object,\n\nIt helps us see that all \\(g\\) can be thought of as state machiens. Once we have this unified framework to compare sampling costs of different models. We just compare the size of the sates and the cost of every tick. We are going to do that next.\nEven though you never want to run the tape state machine, it’s a useful starting point to start the analysys of imporovements. We can sample from autoregressive MLPs. It’s also useful to construct baselines.\nAnd is useful as a software engineering idea. You can see if the samples from a model are good and only then take the effort to implement a highly optimized state machine. Another aplication is testing. When you are implementing an optimized SM, it’s useful to have a (slow) reference that has the exact same interface and outputs as the funciton you are trying to implement.\n\nResult: The unrolling funciton \\(U: \\SM(X,Y) \\to \\CSeqT(X,Y)\\) is surjective. Given \\(m = (S, s_0, p) \\in \\SM(X,Y)\\) \\[\nU(m)(x_1 \\cdots x_t) = [y_1 \\cdots y_t] \\;\\;\\;\\; \\text{where} \\;\\;\\;\\;  (s_t, y_t) = p(s_{t-1}, x_t)\n\\]\nProof: Use the tape state machine… \\(\\blacksquare\\)\nThe following diagram summarizes the result\n\n\n\nFile (10)\n\n\nNote how \\(P\\circ L\\) behaves similarly to an inverse of \\(U\\) because \\(U \\circ P \\circ L = \\text{id} : \\Fn(X,Y) \\to \\Fn(X,Y)\\). But the other direction is not necessarily true. For example if we start with a classic RNN \\((\\R^d, 0, r)\\) and we \\(P \\circ L \\circ U\\) we end up with tape state machine that is different from the RNN.\nWe can also combine it with the autoregressive correspondance result to get the diagram: \n\nRNN State Machines\n\nTape SM: \\(O(t)\\) memory and \\(O(td^2)\\) compute\n\n\nEfficient SM: \\(O(d)\\) memory and \\(O(d^2)\\) compute\n\n\n\nSelf Attention State Machines\n\nTape SM: \\(O(t)\\) memory and \\(O(t^2d)\\) compute\n\n\nKV cache SM: \\(O(td)\\) memory and \\(O(td)\\) compute\nFunnily enough, this imporvement #### Efficient SM: \\(O(d^2)\\) memory and \\(O(d^2)\\) compute Does it exist for the transformer? Yes under the assumption that…"
  },
  {
    "objectID": "drafts/research/index.html#summary",
    "href": "drafts/research/index.html#summary",
    "title": "An Architecture For Neural Language Modeling",
    "section": "3.5) Summary",
    "text": "3.5) Summary\nOk, we’ve seen a lot of changes of perspectives. The following diagram sumamrizes what is the relationship between the objects we’ve been using, and what each perspective is useful for.\n\n\n\nFile (1)\n\n\nOur objective now is to construct good architectures with both, an efficient causal sequence transformation and state machine implementations. We will see how causal transformers can be seen as examples of causal sequence transformations, and then we will present our star architecture a small modification that we call linear self attention that retains all the advantages of the transformers but gives a big improvement on the causal sequence implementation\nBut it’s going to be easier to introduce the key ideas that allow for these optimizations in the simpler setting of sequence transformations.\nTransformers are amazing for two general reasons * generalize very well (better than RNNs?) * run very fast on our GPUs (way better than RNNs)\nbut suffer from some disadvantages: * Cost \\(O(t^2)\\) on train on datasets of length \\(t\\) as opposed to \\(O(t)\\) for RNNs * Sampling a sequence of length \\(t\\) has cost \\(O(t^2)\\) vs \\(O(t)\\) for RNNs\nOur empirical results will be focused on an architecture we implemented that we call Self Attention RNN (SARNN) / Self Attention State Machine / We think its a very succseful architecture at combining the best of both worlds because it has the properties: * generalize very well * run very fast on our GPUs * Cost \\(O(t)\\) to train on datasets of length \\(t\\) * Sampling a sequence of length \\(t\\) has cost \\(O(t)\\)"
  },
  {
    "objectID": "drafts/research/index.html#footnotes",
    "href": "drafts/research/index.html#footnotes",
    "title": "An Architecture For Neural Language Modeling",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSequence transformations form a category.↩︎"
  },
  {
    "objectID": "articles/longcrawl64/index.html",
    "href": "articles/longcrawl64/index.html",
    "title": "LongCrawl64: A Long-Context Natural-Language Dataset",
    "section": "",
    "text": "As part of our broader mission of training language models with ultra-long context, we are releasing a dataset for use in research on architectures and algorithms for long-context modeling. This dataset, which we call LongCrawl64, is available for download. It consists of 6,661,465 pre-tokenized documents, each of which is 65,536 tokens long, for a total token count of 435 billion."
  },
  {
    "objectID": "articles/longcrawl64/index.html#footnotes",
    "href": "articles/longcrawl64/index.html#footnotes",
    "title": "LongCrawl64: A Long-Context Natural-Language Dataset",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe use the same criteria as [1].↩︎\nThe SI prefix “kibi” means 1024, so 64 KiT = 65536 tokens.↩︎\nBy “roll”, we mean in the numpy.roll sense. For example, rolling [12, 5, 7, 4, 21] by 3 would yield [7, 4, 21, 12, 5]. This preprocessing step causes us to sometimes be predicting tokens from the start of a document, conditional on tokens from its end. This is atypical, but completely legitimate; we invite any skeptics to watch the Star Wars movies in release order, beginning with Episode IV.↩︎\nWe split this into a train set of 6,609,334 documents and a heldout set of 52,131 documents.↩︎"
  },
  {
    "objectID": "articles/mission/index.html",
    "href": "articles/mission/index.html",
    "title": "Our Mission",
    "section": "",
    "text": "A poem is a shadow of the act of writing poetry.\nHumanity casts many shadows. All literature, letters, recipes, and tweets. Mathematical proofs and git repositories. Laws, treaties, and declarations of war. Financial statements, employee performance reports, and bankruptcy filings. Podcasts, operas, accidental voicemails, YouTube videos and Hollywood blockbusters. Restaurant reviews and love letters and times tables. Individually, each of these pieces of information is nothing but the faintest shadow of the process that produced it. But collectively, these shadows tell a rich story about the world.\nSince the dawn of humanity, the brain alone could reconstruct the world from these shadows. But the last decade of deep learning has convinced us that this won’t be the case for much longer. Though significant challenges remain, we stand poised to solve them. If successful, it will become possible to synthesize every documentable aspect of humanity into the weights of a neural network.\nOur mission is to train a neural network to model all human output.\nThere are two primary challenges in our pursuit of this goal.\n\nCurrently, it’s not technically feasible to train a model that can ingest all the data that we can collect. Limitations around context length, modality, and throughput force us to use only a small subset of the data available to us.\nMuch of the data we will need has never been collected, curated, and organized into datasets that we can use for training.\n\nWe are building a world-class team of engineers and researchers to tackle these challenges, united around shared principles and a specific research agenda. We value clear thinking, sharing knowledge, and an extreme commitment to scientific honesty. Our research is guided by mathematical beauty and grounded in rigorous empiricism. We are committed to letting the quality of our work speak for itself. No hype, no fluff. All meat.\nIf this vision resonates, please reach out:          777b7a607577605479757a7d72716760757d3a777b79\n\n\n  \n  Or, subscribe to be notified of new posts:"
  },
  {
    "objectID": "articles/optimizing-symmetric-power-transformers/index.html",
    "href": "articles/optimizing-symmetric-power-transformers/index.html",
    "title": "Improving Symmetric Power Transformers with Conformal Transformations",
    "section": "",
    "text": "\\[\n\\newcommand{\\R}{\\mathbb{R}}\n\\newcommand{\\Z}{\\mathbb{Z}}\n\\newcommand{\\N}{\\mathbb{N}}\n\\newcommand{\\sft}{\\text{softmax}}\n\\newcommand{\\List}{\\text{List}}\n\\newcommand{\\Seq}{\\text{Seq}}\n\\newcommand{\\SeqT}{\\text{SeqT}}\n\\newcommand{\\CSeqT}{\\text{CSeqT}}\n\\newcommand{\\Dist}{\\text{Dist}}\n\\newcommand{\\SM}{\\text{SM}}\n\\newcommand{\\Fn}{\\text{Fn}}\n\\newcommand{\\Tok}{\\text{Tok}}\n\\newcommand{\\Aij}{ A_{[i,j]}}\n\\newcommand{\\ten}{\\small\\text{tensor}}\n\\newcommand{\\sym}{\\small\\text{symmetric}}\n\\]"
  },
  {
    "objectID": "articles/optimizing-symmetric-power-transformers/index.html#symmetric-power-transformers",
    "href": "articles/optimizing-symmetric-power-transformers/index.html#symmetric-power-transformers",
    "title": "Improving Symmetric Power Transformers with Conformal Transformations",
    "section": "1. Symmetric Power Transformers",
    "text": "1. Symmetric Power Transformers\nWe begin with a high level overview of symmetric power transformers. The inputs to the layer are sequences of \\(Q_i, K_i, V_i \\in \\R^d\\) of queries, keys, and values, where \\(i\\) ranges from \\(1\\) to the sequence length \\(t\\). The outputs are a sequence \\(Y_i\\in \\R^d\\). In the attention formulation, the formula for the output vectors is: \\[\nY_i = \\sum_{j=1}^i A_{ij} V_j \\qquad A_{ij}  = \\frac{B_{ij}}{\\sum_{k=1}^i B_{ik}} \\qquad B_{ij} = (Q_i^T K_j)^p\n\\qquad \\text{(sympow)}\n\\] We refer to \\(A_{ij}\\) as the attention scores and \\(B_{ij}\\) as the preattention scores (mirroring the preactivation/activation lanugage often use to desrive the hidden values of an MLP before and after the nonlinearity).\n\n\nIt is important that the power \\(p\\) is even because that guarantees the denominator is positive, which makes \\(A_{i1}, \\cdots, A_{ii}\\) a valid probability distribution. In turn, this makes the outputs \\(Y_i\\) a convex combinatoin of \\(V_1, \\cdots, V_i\\).\nThe exact same outputs \\(Y_i\\) can be computed via a recurrent formulation. Doing so invovles an embedding function \\(\\phi^p : \\R^d \\to \\R^D\\). The vector \\(\\phi^p(k)\\) contains the same information as \\({k\\otimes \\cdots \\otimes k}\\), repeatedly taking tensor product \\(p\\) times. But it does so much more efficiently because it removes a lot of symmetry in the tensor product. Thus \\(D &lt;&lt; d^p\\). Using this embedding function, we can write the recurrent equations: \\[\nY_{i} = \\frac{S_i \\phi^p(Q_i)}{Z_i \\phi^p(Q_i)} \\qquad Z_i = Z_{i-1} + \\phi^p(K_i)^T \\qquad S_i = S_{i-1} + V_i \\phi^p(K_i)^T\n\\] where \\(Z_0\\) and \\(S_0\\) are \\(\\mathcal 0\\) vectors in their respective spaces. Since \\(S_i \\in \\R^{d \\times D}\\) and \\(Z_i \\in \\R^{D}\\), the size of the state is \\(D(d+1)\\).\nThese two forms give rise to a variety of algorithms for training linear transformers, with differing computational properties. Read our earlier article on linear transformers for a detailed explanation.\n\n1.1 Rotary embeddings (RoPE)\nIn our previous post on symmetric power transformers, we briefly discussed that even symmetric power transformers were compatible with RoPE. In this section, we will give the issue the proper discussion it deserves by deriving the attention and recurrent implementations.\nRotary embeddings [7] encode time information by rotating the keys and queries by an amount proportional to their corresponding timestep. The rotation matrix \\(R\\in \\R^{d\\times d}\\) tells us how much we want to rotate every timestep, so that: \\[\nQ'_{i} = R^i Q_i \\qquad K'_j = R^j K_j\n\\] Then the preattention is changed to: \\[\nB_{ij} = \\left({Q'_{i}}^T K'_j \\right)^p = \\left({Q_{i}}^T (R^{i-j})^T K_j \\right)^p\n\\qquad \\text{(sympow rotary)}\n\\] It is evident that the effect of rotation of the embeddings is relative because it modulates interaction between \\(Q_i\\) and \\(K_j\\) depending only on the time difference \\(i-j\\).\nThe rotation matrix \\(R\\) is constructed in a particular way. We start with some range of rotation rates \\(\\theta_1, \\theta_2, \\cdots, \\theta_{\\frac d 2}\\) defined by the formula \\(\\theta_i = \\frac{2\\pi}{N^{\\frac{2(i-1)}{d}}}\\), where \\(N\\) is the maximum document size. The vector \\(\\theta\\) contains these rotation rates. Then, the rotation matrix is \\[\\small\nR(\\theta) = \\begin{pmatrix}\n\\cos(\\theta_1) & -\\sin(\\theta_1) & 0 & 0 & \\cdots & 0 & 0 \\\\\n\\sin(\\theta_1) & \\cos(\\theta_1)  & 0 & 0 & \\cdots & 0 & 0 \\\\\n0 & 0 & \\cos(\\theta_2) & -\\sin(\\theta_2) & \\cdots & 0 & 0 \\\\\n0 & 0 & \\sin(\\theta_2) & \\cos(\\theta_2)  & \\cdots & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & 0 & \\cdots & \\cos(\\theta_{d/2}) & -\\sin(\\theta_{d/2}) \\\\\n0 & 0 & 0 & 0 & \\cdots & \\sin(\\theta_{d/2}) & \\cos(\\theta_{d/2})\n\\end{pmatrix}\n\\]\nWhen multiplying a query or key vector by this rotation matrix, each pair of dimensions indexed by \\(2j - 1\\) and \\(2j\\) for \\(j \\in \\{1, 2, ..., \\frac{d}{2} \\}\\) is rotated by a different amount \\(\\theta_j\\). Rotating each pair by a different angle helps break symmetry and increases the expressiveness of the positional encodings.\nA computational advantage of using rotation matrices of this form is that \\(R(\\theta)^k = R(k \\theta)\\), which massively simplifies the cost of computing all the \\(Q'_i\\) and \\(K'_j\\).\n\n\nExpand to see a proof of this fact\n\nWe are given a block diagonal rotation matrix \\(R(\\theta)\\), where each \\(2 \\times 2\\) block corresponds to a rotation by some angle \\(\\theta_i\\):\n\\[\nR(\\theta) = \\begin{pmatrix}\n\\cos(\\theta_1) & -\\sin(\\theta_1) & 0 & 0 & \\cdots & 0 & 0 \\\\\n\\sin(\\theta_1) & \\cos(\\theta_1)  & 0 & 0 & \\cdots & 0 & 0 \\\\\n0 & 0 & \\cos(\\theta_2) & -\\sin(\\theta_2) & \\cdots & 0 & 0 \\\\\n0 & 0 & \\sin(\\theta_2) & \\cos(\\theta_2)  & \\cdots & 0 & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & 0 & \\cdots & \\cos(\\theta_{d/2}) & -\\sin(\\theta_{d/2}) \\\\\n0 & 0 & 0 & 0 & \\cdots & \\sin(\\theta_{d/2}) & \\cos(\\theta_{d/2})\n\\end{pmatrix}.\n\\]\nWe aim to prove that \\(R(\\theta)^k = R(k \\theta)\\), where \\(k\\) is a positive integer, and \\(k \\theta = (k \\theta_1, k \\theta_2, \\dots, k \\theta_{d/2})\\).\nWe prove this statement by induction on \\(k\\) for a single \\(2 \\times 2\\) rotation matrix \\(R(\\theta_i)\\), and then extend it to the full block diagonal matrix.\nFor the base case (k = 1), we have:\n\\[R(\\theta_i)^1 = R(\\theta_i)\\],\nwhich is equivalent to \\(R(1 \\cdot \\theta_i) = R(\\theta_i)\\). Thus, the base case holds.\nAssume that for some positive integer \\(k\\), the property holds:\n\\[R(\\theta_i)^k = R(k \\theta_i).\\]\nWe need to show that \\(R(\\theta_i)^{k+1} = R((k+1)\\theta_i)\\). Using the definition of matrix exponentiation:\n\\[R(\\theta_i)^{k+1} = R(\\theta_i) R(\\theta_i)^k.\\]\nBy the inductive hypothesis, \\(R(\\theta_i)^k = R(k\\theta_i)\\). Substituting this:\n\\[R(\\theta_i)^{k+1} = R(\\theta_i) R(k \\theta_i).\\]\nThe product of two rotation matrices corresponds to a rotation by the sum of their angles. Therefore:\n\\[R(\\theta_i) R(k \\theta_i) = R(\\theta_i + (k \\theta_i)) = R((k+1)\\theta_i).\\]\nThus, \\(R(\\theta_i)^{k+1} = R((k+1)\\theta_i)\\), completing the inductive step. By induction, the property holds for all ( k ).\n\nConsider the block diagonal matrix ( R() ), where: \\[\nR(\\theta) = \\begin{pmatrix}\nR(\\theta_1) & 0 & \\cdots & 0 \\\\\n0 & R(\\theta_2) & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & R(\\theta_{d/2})\n\\end{pmatrix}.\n\\]\nSince each block \\(R(\\theta_i)\\) is independent of the others, the \\(k\\)-th power of \\(R(\\theta)\\) is the block diagonal matrix with each block raised to the \\(k\\)-th power: \\[\nR(\\theta)^k = \\begin{pmatrix}\nR(\\theta_1)^k & 0 & \\cdots & 0 \\\\\n0 & R(\\theta_2)^k & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & R(\\theta_{d/2})^k\n\\end{pmatrix}.\n\\]\nUsing the result for a single rotation matrix, \\(R(\\theta_i)^k = R(k\\theta_i)\\), we get: \\[\nR(\\theta)^k = \\begin{pmatrix}\nR(k \\theta_1) & 0 & \\cdots & 0 \\\\\n0 & R(k \\theta_2) & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & R(k \\theta_{d/2})\n\\end{pmatrix}.\n\\]\nThis is equivalent to the block diagonal matrix \\(R(k\\theta)\\), where \\(k\\theta = (k\\theta_1, k\\theta_2, \\dots, k\\theta_{d/2})\\).\nThus, by induction and the block diagonal structure, \\(R(\\theta)^k = R(k\\theta)\\) for any positive integer \\(k\\).\n\nNow we want to find the recurrent formulation of rotary embeddings with symmetric power transformers. A simple way we can do that is by including one extra vector in the recurrent state which is now a tuple \\((S, Z, \\mu)\\), where \\(\\mu \\in \\R^{\\frac d 2}\\). The recurrent equations are given by \n\\[\nZ_i = Z_{i-1} + \\phi^p(R(\\mu_i) K_i)^T \\qquad S_i = S_{i-1} + V_i \\phi^p(R(\\mu_i) K_i)^T \\qquad \\mu_i = \\mu_{i-1} + \\theta\n\\]\nNote we rotate the keys by \\(R(\\mu_i)\\) before using them.\nGiven \\(S_i\\) and \\(Z_i\\), the outputs are the same as before, except that we rotate the queries by \\(R(\\mu)\\) before using them: \\[\nY_{i} = \\frac{S_i \\phi^p( R(\\mu_i) Q_i)}{Z_i \\phi^p( R(\\mu_i) Q_i)}\n\\]\n\n\nExpand for a proof of the equivalence between the state and recurrent formulations\n\nWe begin by writing the output \\(Y_i\\) at time step \\(i\\) in the attention formulation. For notational simplicity, let \\(C_i = \\sum_{k=1}^i (Q_i'^T K_k')^p = \\sum_{k=1}^i \\phi^p(Q_i')^T \\phi^p(K_k')\\).\n\\[\n\\begin{align}\nY_i &= \\sum_{j=1}^i \\frac{\\left( Q_i^T (R^{i-j})^T K_j\\right) ^p V_j}{C_i} \\\\\n    &= \\sum_{j=1}^i \\frac{\\left( Q_i^T (R^i)^T R^j K_j\\right)^p V_j}{C_i} \\\\\n    &= \\sum_{j=1}^i \\frac{\\left(Q_i^T R(\\mu_i)^T R(\\mu_j) K_j\\right)^p V_j}{C_i} \\\\\n    &= \\sum_{j=1}^i \\frac{\\left((R(\\mu_i) Q_i)^T R(\\mu_j) K_j\\right)^p V_j}{C_i} \\\\\n    &= \\sum_{j=1}^i \\frac{\\left(\\phi^p(R(\\mu_i) Q_i)^T \\phi^p(R(\\mu_j) K_j)\\right) V_j}{C_i} \\\\\n    &= \\sum_{j=1}^i \\frac{V_j \\phi^p(R(\\mu_j) K_j)^T \\phi^p(R(\\mu_i) Q_i)}{C_i} \\\\\n    &= \\frac{\\left( \\sum_{j=1}^i V_j \\phi^p(R(\\mu_j) K_j)^T \\right)  \\phi^p(R(\\mu_i) Q_i) }{C_i} \\\\\n    &= \\frac{S_i  \\phi^p(R(\\mu_i) Q_i) }{Z_i \\phi^p(R(\\mu_i) Q_i)}\n\\end{align}\n\\]\nwhich is the recurrent formulation of the output \\(Y_i\\). The last line above uses the fact that\n\\[\n\\sum_{j=1}^i V_j \\phi^p(R(\\mu_j) K_j)^T = S_i\n\\]\nand\n\\[\n\\begin{align}\nC_i &= \\sum_{k=1}^i \\phi^p(Q_i')^T \\phi^p(K_k') \\\\\n    &= \\sum_{k=1}^i \\phi^p(R(\\mu_i) Q_i)^T \\phi^p(R(\\mu_k) K_k) \\\\\n    &= \\sum_{k=1}^i \\phi^p(R(\\mu_k) K_k)^T \\phi^p(R(\\mu_i) Q_i) \\\\\n    &= Z_i \\phi^p(R(\\mu_i) Q_i)\n\\end{align}\n\\]"
  },
  {
    "objectID": "articles/optimizing-symmetric-power-transformers/index.html#gating",
    "href": "articles/optimizing-symmetric-power-transformers/index.html#gating",
    "title": "Improving Symmetric Power Transformers with Conformal Transformations",
    "section": "2. Gating",
    "text": "2. Gating\nThe basic idea of gating is that at each time step the state matrix \\(S\\in \\R^{d \\times D}\\) will be discounted by a scalar \\(\\gamma \\in [0, 1]\\). Discounting the state ``erases” past information stored in the state. This technique has been used extensively throughout the linear transformer literature [8], [9], [10]. One common approach to implement gating is to manually pick a gating value for each head, usually using a range of large and small \\(\\gamma\\) for different heads to allow the model to keep track of short and long term interactions. But, naturally, the gating values can also be learnable parameters or even data dependent values, as has been thoroughly explored in prior work [1], [2], [3], [4].\nAfter exploring with a few variations of gating, including fixed, learnable and data dependent versions, we ended up converging to the technique used in [3]. The discount value at timestep \\(i\\) is \\(\\gamma_i = \\sigma(W_\\gamma X_i)\\) where \\(\\sigma\\) refers to the sigmoid function, \\(W_\\gamma \\in \\mathbb{R}^{d \\times 1}\\) and \\(X_i\\) are the input sequence we used to compute the keys, queries, and values (e.g. \\(K_i = W_K X_i\\)). When using symmetric power attention with power \\(p\\), the recurrent state update is simply \\[\nZ_i = \\gamma_i Z_{i-1} + \\phi^p(K'_i)^T \\qquad S_i = \\gamma_i S_{i-1} + V_i \\phi^p(K'_i)^T \\qquad \\mu_i = \\mu_{i-1} + \\theta\n\\] Recall that \\(K'_i = R(\\mu_i) K_i\\).\nTo write the attention formulation we define \\(b_{ij} = \\Pi_{k=j+1}^i \\gamma_m\\). Then, in the attention formulation, the preattention becomes \\[\nB_{ij} = b_{ij} \\; ( {Q'_i}^T K'_j)^p\n\\qquad \\text{(sympow gated)}\n\\]\n\n\nExpand for a derivation of the equivalence between the state and attention formulations\n\nWe begin by writing the output \\(Y_i\\) at time step \\(i\\) in the attention formulation. For notational simplicity, let \\(C_i = \\sum_{k=1}^i \\beta_{ik} (Q_i'^T K_k')^p = \\sum_{k=1}^i \\beta_{ik} \\phi^p(Q_i')^T \\phi^p(K_k')\\).\n\\[\n\\begin{align}\nY_i &= \\sum_{j=1}^i \\frac{\\beta_{ij} \\left(Q_i'^T K'_j\\right)^p V_j}{C_i} \\\\\n    &= \\sum_{j=1}^i \\frac{\\beta_{ij} \\left(\\phi^p(Q'_i)^T \\phi^p(K'_j)\\right) V_j}{C_i} \\\\\n    &= \\sum_{j=1}^i \\frac{\\beta_{ij} V_j \\phi^p(K'_j)^T \\phi^p(Q'_i)}{C_i} \\\\\n    &= \\frac{\\left( \\sum_{j=1}^i \\beta_{ij} V_j \\phi^p(K'_j)^T \\right)  \\phi^p(Q'_i) }{C_i} \\\\\n    &= \\frac{S_i  \\phi^p(Q'_i) }{Z_i \\phi^p(Q'_i)}\n\\end{align}\n\\]\nwhich is the recurrent formulation of the output \\(Y_i\\). The last line above uses the fact that\n\\[\n\\sum_{j=1}^i \\beta_{ij} V_j \\phi^p(K'_j)^T = S_i\n\\]\nand\n\\[\n\\begin{align}\nC_i &= \\sum_{k=1}^i \\beta_{ik} \\phi^p(Q_i')^T \\phi^p(K_k') \\\\\n    &= \\sum_{k=1}^i \\beta_{ik} \\phi^p(K_k')^T \\phi^p(Q_i') \\\\\n    &= Z_i \\phi^p(Q'_i)\n\\end{align}\n\\]\nWe prove that \\(S_i = \\sum_{j=1}^i \\beta_{ij} V_j \\phi^p(K'_j)^T\\) by induction.\nAs the base case, note that \\(S_1 =  V_1 \\phi^p(K'_1)^T\\)\nFor the inductive step, suppose that for \\(k &gt; 1\\), \\(S_k = \\sum_{j=1}^k \\beta_{kj} V_j \\phi^p(K'_j)^T\\). Then \\[\n\\begin{align}\nS_{k+1} &= \\gamma_{k+1} S_k + V_{k+1}\\phi^p(K'_{k+1})^T \\\\\n        &= \\gamma_{k+1} \\left( \\sum_{j=1}^k \\beta_{kj} V_j \\phi^p(K'_j)^T \\right) + V_{k+1}\\phi^p(K'_{k+1})^T \\\\\n        &= \\left( \\sum_{j=1}^k \\beta_{(k+1)j} V_j \\phi^p(K'_j)^T \\right) + V_{k+1}\\phi^p(K'_{k+1})^T \\\\\n        &= \\sum_{j=1}^{k+1} \\beta_{(k+1)j} V_j \\phi^p(K'_j)^T\n\\end{align}\n\\] This completes the inductive step.\n\n\nLet’s see what difference gating makes, and whether it solves the issues we encountered in the intro. We can look at the loss at the end of training on 400k documents. As we grow the train context size, gated sympow stays better than the baseline as far as we were able to test it!\n\n\n\n\n\nWe also see that after adding learned gating, the symmetric power transformer is able to successfully generalize past the train context size."
  },
  {
    "objectID": "articles/optimizing-symmetric-power-transformers/index.html#learned-rotary-embeddings",
    "href": "articles/optimizing-symmetric-power-transformers/index.html#learned-rotary-embeddings",
    "title": "Improving Symmetric Power Transformers with Conformal Transformations",
    "section": "3. Learned Rotary Embeddings",
    "text": "3. Learned Rotary Embeddings\nWe now explore an intuitive idea of learning the rotation rates in rotary positional embeddings. There are many ways we can approach this. For example, the network could independently decide how much to rotate each of the 2D subspaces. We found that an efficient way to implement learning rotation rates that results in performance improvements is for the network to scale the fixed \\(\\theta\\) vector that RoPE usually applies. Similar to gating, we add parameters \\(W_\\beta\\) to each attention head. The network outputs a scalar \\(\\beta_i = 1 + \\text{tanh}(W_\\beta X_i)\\) and multiplies this value with the original fixed vector \\(\\theta\\). In the recurrent formulation, this produces the equation \\[\n\\mu_i = \\mu_{i-1} + \\beta_i \\theta\n\\] and in the attention formulation \\[\nB_{ij} = \\left(Q_i R(c_{ij}\\theta) K_j \\right)^p \\qquad c_{ij} = \\sum_{k=j+1}^i \\beta_i  \\qquad \\text{(conformal-sympow)}\n\\] For reasons discussed in Section 4, we refer to this approach as conformal-sympow.\nWe can see that learning the rotary embeddings in addition to learning gating values further improves performance over sympow+gating with fixed rotary embeddings:\n\n\n\n\nand generalizes past the train context size:\n\n\n\n\n\nFull Training Curves\nHere we display all the training curves for all methods using symmetric power \\(p=2\\) and \\(p=4\\) at different context lengths. We can see that sympow+gating and conformal-sympow improves optimization throughout training."
  },
  {
    "objectID": "articles/optimizing-symmetric-power-transformers/index.html#conformal-state-transformations-optional-reading",
    "href": "articles/optimizing-symmetric-power-transformers/index.html#conformal-state-transformations-optional-reading",
    "title": "Improving Symmetric Power Transformers with Conformal Transformations",
    "section": "4. Conformal State Transformations (Optional Reading)",
    "text": "4. Conformal State Transformations (Optional Reading)\n*Note: this section is conceptual and has no practical implications. In this section, we show that gating and rotations can be unified into a single mathematical idea: a conformal state transformation.\nWe refer to the combination of gating and rotary embeddings as conformal-sympow. The reason stems from the fact that the combination of gating and rotary embeddings can be interpreted as applying a conformal linear transformation to the state in the recurrent formulation. A conformal linear transformation is a type of linear transformation that preserves angles between vectors while allowing uniform scaling of lengths. Mathematically, a conformal linear transformation in \\(n\\)-dimensional Euclidean space can be expressed as: \\[\n\\mathbf{T}(\\mathbf{x}) = s \\mathbf{R} \\mathbf{x},\n\\] where \\(s &gt; 0\\) is a scalar representing the scaling factor, \\(\\mathbf{R}\\) is an orthogonal matrix \\(\\mathbf{R}^T \\mathbf{R} = \\mathbf{I}\\), and \\(\\mathbf{x}\\) is the input vector.\nWe will show that updating the recurrent state of a sympow transformer by applying gating and rotary embeddings is equivalent to right multiplying the state with a conformal linear transformation before adding new information:\n\\[\nS_{i} = S_{i-1} (s\\mathbf{R}) + V_i \\phi^p(K_i)^T\n\\]\nWhen applying gating, we can see that the discount value \\(\\gamma\\) plays the role of \\(s\\) in the above equation.\nNow, recall that the recurrent state update when applying rotary embeddings is \\[\nZ_i = Z_{i-1} + \\phi^p(K'_i)^T \\qquad S_i = S_{i-1} + V_i \\phi^p(K'_i)^T \\qquad \\mu_i = \\mu_{i-1} + \\theta\n\\] where \\(K'_i = R(\\mu_i) K_i\\).\nWe will show that the update equation \\(S_i = S_{i-1} + V_i \\phi^p(R(\\mu_i)K_i)^T\\) is equivalent to the following update equation: \\[\nS_i = S_{i-1} \\bar{R}(\\theta) + V_i \\phi^p(K_i)^T\n\\] for some rotation matrix \\(\\bar{R}(\\theta)\\).\nThis equivalence stems from the following result. If \\(P\\in \\R^{d\\times d}\\) is a rotation matrix, then there exists another rotation matrix \\(\\bar P \\in \\R^{D \\times D}\\) s.t. \\[\n\\phi^p( P k) = \\bar P \\phi^p(k)\n\\]\n\n\nExpand for a proof of this fact\n\nNote that the symmetric power embedding function is equivalent to applying the tensor product and removing redundant information resulting from symmetry. For mathematical simplicity, we prove the corresponding result for which the embedding function is the repeated tensor product \\(\\otimes^p\\). The corresponding proposition is stated below.\nLet \\(V\\) be a vector space with dimension \\(d\\) and basis vectors \\(\\{ v_1, v_2, \\dots, v_d \\}\\), and let \\(P \\in \\mathbb{R}^{d \\times d}\\) be a rotation matrix. Define the linear map \\(\\bar{P} : V^{\\otimes p} \\to V^{\\otimes p}\\) (the tensor product of \\(p\\) copies of \\(V\\)) by its action on the basis elements as \\[\n\\bar{P}(v_{i_1} \\otimes v_{i_2} \\otimes \\dots \\otimes v_{i_p}) = (P v_{i_1}) \\otimes (P v_{i_2}) \\otimes \\dots \\otimes (P v_{i_p}) \\quad \\text{for all } i_1, i_2, \\dots, i_p.\n\\] Then \\(\\bar{P} \\in \\mathbb{R}^{d^p \\times d^p}\\) is a rotation matrix.\nWe need to show that \\(\\bar{P}\\) satisfies the properties of a rotation matrix, namely: 1. \\(\\bar{P}\\) is an orthogonal matrix, i.e., \\(\\bar{P}^T \\bar{P} = I\\). 2. \\(\\det(\\bar{P}) = 1\\), so that \\(\\bar{P}\\) represents a proper rotation.\nStep 1: Orthogonality of \\(\\bar{P}\\).\nSince \\(\\bar{P}\\) is defined by its action on the basis elements of \\(V^{\\otimes k}\\) as \\[\n\\bar{P}(v_{i_1} \\otimes v_{i_2} \\otimes \\dots \\otimes v_{i_p}) = (P v_{i_1}) \\otimes (P v_{i_2}) \\otimes \\dots \\otimes (P v_{i_p}),\n\\] and \\(P\\) is an orthogonal matrix, i.e., \\(P^T P = I_d\\), where \\(I_d\\) is the identity matrix in \\(\\mathbb{R}^{d}\\), we need to verify that \\(\\bar{P}\\) preserves the inner product in the tensor product space. The inner product of two basis elements \\(v_{i_1} \\otimes v_{i_2} \\otimes \\dots \\otimes v_{i_p}\\) and \\(v_{j_1} \\otimes v_{j_2} \\otimes \\dots \\otimes v_{j_p}\\) in \\(V^{\\otimes p}\\) is given by: \\[\n\\langle v_{i_1} \\otimes v_{i_2} \\otimes \\dots \\otimes v_{i_p}, v_{j_1} \\otimes v_{j_2} \\otimes \\dots \\otimes v_{j_k} \\rangle = \\prod_{p=1}^{p} \\langle v_{i_p}, v_{j_p} \\rangle.\n\\] Applying \\(\\bar{P}\\) to this inner product, we get: \\[\n\\langle \\bar{P}(v_{i_1} \\otimes \\dots \\otimes v_{i_p}), \\bar{P}(v_{j_1} \\otimes \\dots \\otimes v_{j_p}) \\rangle = \\prod_{p=1}^{p} \\langle P v_{i_p}, P v_{j_p} \\rangle.\n\\] Since \\(P\\) is orthogonal, we have \\(\\langle P v_{i_p}, P v_{j_p} \\rangle = \\langle v_{i_p}, v_{j_p} \\rangle\\) for each \\(p\\). Therefore, \\(\\bar{P}\\) preserves the inner product, meaning that \\(\\bar{P}\\) is an orthogonal matrix, i.e., \\(\\bar{P}^T \\bar{P} = I_{d^p}\\).\nStep 2: Determinant of \\(\\bar{P}\\).\nNext, we show that \\(\\det(\\bar{P}) = 1\\). Since \\(\\bar{P} = P \\otimes P \\otimes \\dots \\otimes P\\) (a \\(p\\)-fold tensor product of \\(P\\) with itself), we can use the property of the determinant for tensor products of matrices. Specifically, if \\(A\\) and \\(B\\) are square matrices, then: \\[\n\\det(A \\otimes B) = \\det(A)^{\\dim(B)} \\det(B)^{\\dim(A)}.\n\\] In our case, since \\(\\bar{P} = P \\otimes P \\otimes \\dots \\otimes P\\), we have: \\[\n\\det(\\bar{P}) = \\det(P)^{p \\cdot d}.\n\\] Since \\(P\\) is a rotation matrix in \\(\\mathbb{R}^d\\), we know that \\(\\det(P) = 1\\). Therefore: \\[\n\\det(\\bar{P}) = 1^{p \\cdot d} = 1.\n\\] Thus, \\(\\bar{P}\\) is a proper rotation matrix.\n\nUsing the above result, the conformal-sympow recurrent state update can be written as follows:\n\\[\nZ_i = Z_{i-1} (\\gamma_i \\bar{R}(\\theta, \\beta_i)) + \\phi^p(K_i)^T \\qquad S_i = S_{i-1} (\\gamma_i \\bar{R}(\\theta, \\beta_i)) + V_i \\phi^p(K_i)^T\n\\] where \\(\\bar{R}(\\theta, \\beta_i)\\) is a rotation matrix that depends on the fixed rotation rates \\(\\theta\\) and the scalar \\(\\beta_i\\)."
  },
  {
    "objectID": "articles/optimizing-symmetric-power-transformers/index.html#equivalance-between-gating-and-alibi-optional-reading",
    "href": "articles/optimizing-symmetric-power-transformers/index.html#equivalance-between-gating-and-alibi-optional-reading",
    "title": "Improving Symmetric Power Transformers with Conformal Transformations",
    "section": "5. Equivalance between Gating and ALiBi (Optional Reading)",
    "text": "5. Equivalance between Gating and ALiBi (Optional Reading)\nAttention with Linear Biases (ALiBi) [6] is a type of positional encoding that significantly improves the ability of softmax transformers to extrapolate to evaluation contexts longer than the training context size. ALiBi biases query-key attention scores with a penalty that is proportional to the distance between the query and key. We now show that ALiBi is equivalent to applying scalar gating.\nIn a softmax transformer, the attention scores are computed as \\[\nA_{ij}  = \\frac{B_{ij}}{\\sum_{k=1}^i B_{ik}} \\qquad B_{ij} = \\text{exp}(Q_i^T K_j)\n\\qquad \\text{(softmax)}\n\\] Recall that we refer to \\(A_{ij}\\) as the attention scores and \\(B_{ij}\\) as the pre-attention scores.\nThe pre-attention scores after applying ALiBi are \\[\nB_{ij} = \\text{exp}(Q_i^T K_j + m(j -i))\n\\qquad \\text{(softmax + ALiBi)}\n\\] where \\(0 &lt; m &lt; 1\\) is a head-specific value that is fixed before training.\nNote that \\[\\text{exp}(Q_i^T K_j + m(j - i)) = \\gamma^{(i - j)} \\text{exp}(Q_i^T K_j)\\] where \\(\\gamma = \\text{exp}(-m)\\). Since \\(-m &lt; 0\\), \\(0 &lt; \\gamma &lt; 1\\). Thus, the application of ALiBi is equivalent to applying scalar gating."
  },
  {
    "objectID": "articles/compute-optimal-context-size/index.html",
    "href": "articles/compute-optimal-context-size/index.html",
    "title": "Compute-Optimal Context Size",
    "section": "",
    "text": "The objective of language modeling is to predict each token in a sequence. Each prediction is conditional on a subset of previous tokens, which are called the context for the prediction. Intuitively, expanding the context should make prediction task strictly easier. If an extra token provides relevant information, the model can learn to use it; otherwise, the model can simply ignore it. Therefore, given any well-trained language model, we expect the average loss to be lower on longer-context predictions. To verify this, we trained a 772-million-parameter transformer1 with context size 32 kibitokens (KiT).\nWe refer to this as a contextwise loss curve, and the general phenomenon of the loss improving on longer contexts as inference-time context scaling. This trend is not specific to our training setup, and has been observed elsewhere in the literature. For example, below is a plot from Gemini [1], illustrating the same effect.\nInference-time context scaling provides a quantitative justification for increasing the training context. Intuitively, training on longer contexts increases the extent to which we can leverage inference-time context scaling, ultimately decreasing loss. This motivates an approach to selecting the size of the training context: choose the context size that optimizes loss given training budget.2 This is a natural complement to existing research on scaling laws. For example, Kaplan et al [2] and Hoffmann et al [3] investigated the optimal way to scale the model size & amount of tokens seen, but both works held context length fixed. To complete this analysis, one must optimize over context length as well.\nIn Section 3, we will do exactly this, using GPT-2-style transformers at scales ranging from 124 million to 1.6 billion parameters. The results show that the optimal training-context length increases with larger training budgets. But devising a proper experimental setting to compare between train-context lengths is surprisingly tricky. It turns out that popular datasets (such as openwebtext or C4) and standard metrics (average train loss) are inappropriate. We begin by discussing these two subtle but important details: in Section 1, we address the choice of dataset, and in Section 2, we address the choice of evaluation metric.\nWe conclude with Section 4, a discussion of some applications that are unlocked by models with ultra-long contexts, from kilotokens up to petatokens. But the vast potential of models with ultra-long contexts cannot be realized if they are trained in a setting that is far from compute-optimal. And so, we need research focused on increasing the optimal training-context size. We believe that careful evaluation of context scaling will be an essential ingredient in progress, and hope that the dataset, ideas, and evaluations presented in this article will prove useful towards that objective."
  },
  {
    "objectID": "articles/compute-optimal-context-size/index.html#data-with-long-term-structure",
    "href": "articles/compute-optimal-context-size/index.html#data-with-long-term-structure",
    "title": "Compute-Optimal Context Size",
    "section": "1. Data with Long-term Structure",
    "text": "1. Data with Long-term Structure\nBelow is a contextwise loss curve similar to the ones in the introduction. It shows the average loss at every context length for a 1.6-billion-parameter model trained using 8 KiT of context on openwebtext. In the first part of the curve, this plot shows contextwise scaling, with performance improving as more tokens are seen. But the trend of improvement tapers off. After around 2 KiT, additional tokens no longer improve the loss.\n\n\n\n\nTo understand the reason for this, one only need look at the document-length distribution of the openwebtext dataset.\n\n\n\n\nOver 90% of the documents are less than 2 KiT long. In order to train train 8-KiT-context models on this dataset, somehow longer documents must be constructed out of smaller ones (in our experiments, we simply concatenated multiple documents). But the resulting “long” documents do not truly contain any long-term structure, and so there is no benefit to seeing additional tokens at inference-time.\nThis problem is not restricted to openwebtext. Many other popular datasets, such as C4 and RedPajama, have similar document-length distributions. This is insufficient for our goals, because it does not allow one to thoroughly evaluate contextwise scaling properties.\nTo solve this issue, we created LongCrawl64, a large natural langauge dataset composed entirely of documents of length 64 KiT. This data is a subset of Common Crawl, tokenized using OpenAI’s TikToken and with short documents filtered out. The end result is a 6661465 x 65336 Zarr array of uint16s, representing 6,661,465 documents each of size 64 KiT. The total token count is 435 billion, two orders of magnitude larger than openwebtext (6 billion). Read our release for the details around the construction and usage of the dataset; for example, how to efficiently load documents when training at context lengths shorter than 64 KiT.\nArmed with this new dataset, we can repeat our experiment and again compute the contextwise loss curve of a 1.6-billion-parameter transformer with context size 8 KiT:\n\n\n\n\nOn LongCrawl64, we see consistent contextwise scaling throughout the train context. With this first issue resolved, let’s move on to the second."
  },
  {
    "objectID": "articles/compute-optimal-context-size/index.html#the-training-loss-is-misleading",
    "href": "articles/compute-optimal-context-size/index.html#the-training-loss-is-misleading",
    "title": "Compute-Optimal Context Size",
    "section": "2. The Training Loss is Misleading",
    "text": "2. The Training Loss is Misleading\nBelow, we show the contextwise loss curves for two trained transformers. The average training loss of each model is given by a dotted line. The details of training are not relevant for this section3, so we will simply call them Model A and Model B. But an important difference between the two is that Model A is trained with 4 KiT of context, and Model B with 16 KiT.\n\n\n\n\nModel B has better training loss (2.244) than Model A (2.253). But do we truly prefer Model B? Note that Model A makes better predictions than Model B at every context length where they can be compared. Furthermore, Model A with a 4 KiT context reaches a lower loss than the 16 KiT model ever does. This means that at inference time, if we had 16 KiT of context available, we would be better off throwing away the first 12 KiT of context and feeding the remainder to Model A, instead of feeding all 16 KiT to Model B. Doing so would result in better predictions. In fact, there is no situation where we prefer Model B.\nWhy does the training loss mislead us? The training loss can be computed as the average of the contextwise loss curve, where the x-axis ranges from 1 to the training-context size. For a 16 KiT model, a much larger component of the training loss comes from situations where the model has a large amount of information in its context. For example, if we look at the proportion of the training loss coming from predictions with at least 3 KiT of context, we see that for Model A this is only 25%, whereas for Model B it is over 80%.\nThe upshot is: when comparing models trained with different context sizes, the training loss inaccurately ranks their performance. In order to select the optimal training-context size, we must find a more reliable metric to optimize.\nIntuitively, we want our metric to reflect the model’s ability to predict the next token at inference time. If we make the assumption that the users of the model have access to arbitrarily many tokens to put in the context, then a natural metric would be the lowest loss that the model attains at any context size. We refer to this as the best-context loss. To measure the best-context loss, compute the contextwise loss curve, and take its minimum.\n\n\nConsider, for example, the common practice of “prompting” a chatbot: pre-placing tokens into the context ahead of the user’s query. Conventional wisdom holds that longer and more thorough prompts improve final performance. If a maximum-length prompt is always utilized, our assumption is fulfilled, and best-context loss drives performance.\nIn fact, since the transformer we’ve been working with uses rotary embeddings, we can evaluate it beyond its training context. And, with the LongCrawl64 dataset, we have data with long-term structure up to 64 KiT. Thus, we can extend the contextwise scaling plots up to 64 KiT:\n\n\n\n\nBeyond the context size used during training, there is a rapid deterioration of prediction ability. Clearly, this model does not generalize well to the beyond-train-context regime. We’ve observed this exact same phenomenon for transformers of all sizes trained on all sorts of context sizes.\n\n\nEven though there have many claims that language models can generalize beyond their training context [4]–[6], to the best of our knowledge, nobody has shown a model for which the loss on natural-language text monotonically decreases with the context size. We consider this to be the true criterion for “sequence length generalization”.\nThis empirical fact is unfortunate, but has a silver lining: it simplifies measurement of the best-context loss. For models that do not generalize beyond their training context, we can measure the best-context loss by simply reporting the loss at the largest context size seen during training.4 This is the approach that we take in this article. But note that it is merely a convenient heuristic, and is valid only when working with models that fail to generalize in this way."
  },
  {
    "objectID": "articles/compute-optimal-context-size/index.html#context-scaling-experiments",
    "href": "articles/compute-optimal-context-size/index.html#context-scaling-experiments",
    "title": "Compute-Optimal Context Size",
    "section": "3. Context Scaling Experiments",
    "text": "3. Context Scaling Experiments\nWith our experimental setting established, it is time to evaluate scaling trends for the train-context size of transformers. The basic experiment we conducted is: train GPT-2 + rotary embeddings + flash attention, for a variety of parameter counts (124 million, 354 million, 772 million, 1.6 billion) and a variety of train-context sizes (128 tokens, 256 tokens, 512 tokens, …, 64 KiT). Each training run used 8 H100 GPUs with data parallel, and ran for 160 GPU-hours. We kept the batch size (i.e. number of tokens per gradient step) constant, so that, as the context size ranged from 128 to 64KiT, the number of documents per update varied from 2048 to 8.\nThe results of this experiment are visualized in the plot below. The x-axis of is the context size used during training. The y-axis is the best-context loss. Every line corresponds to a different model size. The training resources (in terms of GPU hours) can be interactively controlled via the slider. The colored circles show the optimal train context at each model size, and the dashed line shows the overall optimum.\n\n\n\n\nYou can see that varying the context size tends draw a U-shaped curve at all resource levels. Picking too small or too large a context size results in severely degraded performance. By playing with the slider you can adjust amount of training resources and confirm that this trend holds generally.\nIt is clear from this data that for any model size we should grow the context size with the training resources. We can directly visualize this trend with a second plot. For each model size, we plot a line with the hours of training on the x-axis, and the optimal context size on the y-axis.5\n\n\n\n\nClearly, as more resources become available we should train our models using longer context sizes. Also, an interesting observation is that the optimal context size grows more slowly for larger models.\nSo far, we’ve just been looking at the optimal context size for a given model scale. What if we select for the optimal combination of model size and context size?\n\n\n\n\n\n\nIdeally, we would quantify these trends and propose scaling laws. This would merely require extending our methodology by a few additional orders of magnitude of model scale, and to sweep over a few other hyperparamters (e.g. learning rate). This is beyond our current capacity, and we cannot meaningfully extrapolate from existing experiments, so we leave quantitative context-size scaling laws to future work.\nAs we expected, we see that as resources grow one wants to increase both model size and train-context size. But, relative to the previous plot (where we held model size fixed), the growth of the optimal context size noticeably slows down. This seems to be a consequence of the fact that, with a larger GPU hour budget, we want to use larger model sizes, and the optimal context size for those larger models tends to grow slower."
  },
  {
    "objectID": "articles/compute-optimal-context-size/index.html#final-thoughts-on-context-scaling",
    "href": "articles/compute-optimal-context-size/index.html#final-thoughts-on-context-scaling",
    "title": "Compute-Optimal Context Size",
    "section": "4. Final Thoughts On Context Scaling",
    "text": "4. Final Thoughts On Context Scaling\nAt Manifest AI, we believe that context size is one of the most important bottlenecks the field of AI is facing. Many of the most important applications of the technology are just waiting to be unlocked once sufficiently-long-context models become available. Most likely, we will be surprised by which applications end up being most important, but here are some guesses as to what type of use cases will become possible at every context-size scale:\n\nkilotoken scale: Read & write emails. Hold a short chatbot-style conversation. Customize behavior with a prompt. Few-shot learning with a small number of examples.\nmegatoken scale: Write books. Review news articles. Read & edit code. Answer questions from a large scientific literature. Navigate web interfaces.\ngigatoken scale: Read everything tweeted in a day and summarize global opinion. Execute a full software engineering workflow. In-context learning of entire datasets (replacing fine-tuning). Solve complex mathematical problems by iteratively improving over many proof attempts.\nteratoken scale: Manipulate all the data created by a corporation (contracts, documents, emails, etc).\npetatoken scale: Coordinate the affairs of an entire society by integrating all information it produces.\n\nIn light of this astonishing potential, it is tempting to simply always train on the longest context that is computationally feasible. But, as our experiments indicate, naively increasing the train context merely leads to models which are massively under-performant – able to ingest long contexts but unable to use their contents to make good predictions. The goal is not merely to train on long contexts, but to efficiently train on long contexts, by finding a setting where long contexts are compute-optimal. This is what it will take to truly leverage vast context sizes.\nSuch a setting will likely require radical algorithmic and architectural changes. An example of research that has successfully pushed the context size frontier is flash attention [7]. The reason is that it can decrease the cost of training with long contexts. That is also why we are excited about linear transformers, which reduce the cost of training on a context of length \\(t\\) from \\(O(t^2)\\) to \\(O(t)\\). Another angle that seems important is to develop models that generalize beyond the training context, in the specific sense that the contextwise loss curve keeps improving beyond the context size used for training.\nWe hope that the mindset, methodology, and dataset introduced in this article will be helpful in progressing to the petatoken scale and beyond.\n\n  \n  Subscribe to be notified of new posts:"
  },
  {
    "objectID": "articles/compute-optimal-context-size/index.html#footnotes",
    "href": "articles/compute-optimal-context-size/index.html#footnotes",
    "title": "Compute-Optimal Context Size",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor full experimental details, see Section 3.↩︎\nOur approach can be contrasted with the common mindset of train models with the largest context that the training budget will permit.↩︎\nFor those who are curious: both models are 124 million parameters and were trained on LongCrawl64 for 50,000 steps.↩︎\nIn practice, we take the average loss for the final 10% of the training context, which is less noisy.↩︎\nThe optimal context size tends to jump around due to noise in the loss, so this plot is smoothed by taking the most common context size in any given window.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Manifest AI",
    "section": "",
    "text": "writing\n\n\n\nDate\nTitle\n\n\n\n\nJan 1, 2024\nOur Mission\n\n\nJan 5, 2024\nLinear Transformers Are Faster\n\n\nMay 16, 2024\nCompute-Optimal Context Size\n\n\nAug 14, 2024\nLongCrawl64: A Long-Context Dataset\n\n\nAug 15, 2024\nSymmetric Power Transformers\n\n\nSep 23, 2024\nWhy Gradient Descent Minimizes Training Loss\n\n\nSep 30, 2024\nOptimizing Symmetric Power Transformers\n\n\n\n\n  \n  Subscribe to posts:\n  \n  \n  \n  \n\n\n\nabout us\nCarles Gelada and Jacob Buckman. Each of us has 8+ years of deep learning research at institutions like OpenAI, Google Brain, and Mila. Our research has been published at NeurIPS, ICLR, ICML, etc., and been cited 1000+ times. After five years of academic collaboration, we founded Manifest AI in March 2023, backed by Decibel.\n\n\njoin us\nWe are hiring core technical team members.\nThe role has elements of both software engineering and research. Responsibilities include implementing deep learning architectures, deriving algorithms, developing research infrastructure, running large-scale experiments, and interpreting and communicating results.\nWe will work well together if you are independent-minded, capable of self-teaching, and value thinking from first principles. Skills we are looking for include comfort with mathematics, strong communication, deep knowledge in areas like CUDA, XLA/MLIR, Jax, or distributed systems/HPC, and experience training large-scale deep learning models.\nWe do not care about formal credentials. If you share our vision and would like to get involved, please send an example of some technical work that you are proud of to 777b7a607577605479757a7d72716760757d3a777b79"
  }
]