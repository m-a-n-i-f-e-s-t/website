<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Jacob Buckman">
<meta name="author" content="Carles Gelada">
<meta name="author" content="Sean Zhang">
<meta name="dcterms.date" content="2024-08-15">

<title>Symmetric Power Transformers – Manifest AI</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<link href="../../favicon.png" rel="icon" type="image/png">
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-V0D26E23Q3"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-V0D26E23Q3', { 'anonymize_ip': true});
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Symmetric Power Transformers - Manifest AI">
<meta property="og:description" content="A linear transformer that learns like a regular transformer with a state that fits on a GPU.">
<meta property="og:image" content="/thumbnails/symmetric-power-transformers.png">
<meta property="og:site_name" content="Manifest AI">
<meta name="twitter:title" content="Symmetric Power Transformers - Manifest AI">
<meta name="twitter:description" content="A linear transformer that learns like a regular transformer with a state that fits on a GPU.">
<meta name="twitter:image" content="https://manifestai.com/thumbnails/symmetric-power-transformers.png">
<meta name="twitter:creator" content="@manifest__ai">
<meta name="twitter:site" content="@manifest__ai">
<meta name="twitter:card" content="summary">
<meta name="citation_title" content="Symmetric Power Transformers">
<meta name="citation_author" content="Jacob Buckman">
<meta name="citation_author" content="Carles Gelada">
<meta name="citation_author" content="Sean Zhang">
<meta name="citation_publication_date" content="2024-08-15">
<meta name="citation_cover_date" content="2024-08-15">
<meta name="citation_year" content="2024">
<meta name="citation_online_date" content="2024-08-15">
<meta name="citation_language" content="en">
<meta name="citation_publisher" content="Manifest AI">
<meta name="citation_reference" content="citation_title=Polysketchformer: Fast transformers via sketches for polynomial kernels;,citation_author=Praneeth Kacham;,citation_author=Vahab Mirrokni;,citation_author=Peilin Zhong;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_journal_title=arXiv preprint arXiv:2310.01655;">
<meta name="citation_reference" content="citation_title=The hedgehog &amp;amp;amp; the porcupine: Expressive linear attentions with softmax mimicry;,citation_author=Michael Zhang;,citation_author=Kush Bhatia;,citation_author=Hermann Kumbong;,citation_author=Christopher Ré;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_journal_title=arXiv preprint arXiv:2402.04347;">
<meta name="citation_reference" content="citation_title=Simple linear attention language models balance the recall-throughput tradeoff;,citation_author=Simran Arora;,citation_author=Sabri Eyuboglu;,citation_author=Michael Zhang;,citation_author=Aman Timalsina;,citation_author=Silas Alberti;,citation_author=Dylan Zinsley;,citation_author=James Zou;,citation_author=Atri Rudra;,citation_author=Christopher Ré;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_journal_title=arXiv preprint arXiv:2402.18668;">
<meta name="citation_reference" content="citation_title=Linear transformers are secretly fast weight programmers;,citation_author=Imanol Schlag;,citation_author=Kazuki Irie;,citation_author=Jürgen Schmidhuber;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_conference_title=International conference on machine learning;,citation_conference=PMLR;">
<meta name="citation_reference" content="citation_title=Linformer: Self-attention with linear complexity;,citation_author=Sinong Wang;,citation_author=Belinda Z Li;,citation_author=Madian Khabsa;,citation_author=Han Fang;,citation_author=Hao Ma;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_journal_title=arXiv preprint arXiv:2006.04768;">
<meta name="citation_reference" content="citation_title=Transformers are rnns: Fast autoregressive transformers with linear attention;,citation_author=Angelos Katharopoulos;,citation_author=Apoorv Vyas;,citation_author=Nikolaos Pappas;,citation_author=François Fleuret;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_conference_title=International conference on machine learning;,citation_conference=PMLR;">
<meta name="citation_reference" content="citation_title=Roformer: Enhanced transformer with rotary position embedding;,citation_author=Jianlin Su;,citation_author=Murtadha Ahmed;,citation_author=Yu Lu;,citation_author=Shengfeng Pan;,citation_author=Wen Bo;,citation_author=Yunfeng Liu;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_volume=568;,citation_journal_title=Neurocomputing;,citation_publisher=Elsevier;">
<meta name="citation_reference" content="citation_title=Flashattention: Fast and memory-efficient exact attention with io-awareness;,citation_author=Tri Dao;,citation_author=Dan Fu;,citation_author=Stefano Ermon;,citation_author=Atri Rudra;,citation_author=Christopher Ré;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_volume=35;,citation_journal_title=Advances in Neural Information Processing Systems;">
<meta name="citation_reference" content="citation_title=Gated linear attention transformers with hardware-efficient training;,citation_author=Songlin Yang;,citation_author=Bailin Wang;,citation_author=Yikang Shen;,citation_author=Rameswar Panda;,citation_author=Yoon Kim;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_journal_title=arXiv preprint arXiv:2312.06635;">
<meta name="citation_reference" content="citation_title=Hgrn2: Gated linear rnns with state expansion;,citation_author=Zhen Qin;,citation_author=Songlin Yang;,citation_author=Weixuan Sun;,citation_author=Xuyang Shen;,citation_author=Dong Li;,citation_author=Weigao Sun;,citation_author=Yiran Zhong;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_journal_title=arXiv preprint arXiv:2404.07904;">
<meta name="citation_reference" content="citation_title=Random feature attention;,citation_author=Hao Peng;,citation_author=Nikolaos Pappas;,citation_author=Dani Yogatama;,citation_author=Roy Schwartz;,citation_author=Noah A Smith;,citation_author=Lingpeng Kong;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_journal_title=arXiv preprint arXiv:2103.02143;">
<meta name="citation_reference" content="citation_title=Rethinking attention with performers;,citation_author=Krzysztof Choromanski;,citation_author=Valerii Likhosherstov;,citation_author=David Dohan;,citation_author=Xingyou Song;,citation_author=Andreea Gane;,citation_author=Tamas Sarlos;,citation_author=Peter Hawkins;,citation_author=Jared Davis;,citation_author=Afroz Mohiuddin;,citation_author=Lukasz Kaiser;,citation_author=others;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_journal_title=arXiv preprint arXiv:2009.14794;">
<meta name="citation_reference" content="citation_title=Linear Transformers Are Faster;,citation_author=Jacob Buckman;,citation_author=Carles Gelada;,citation_publication_date=2024-01-05;,citation_cover_date=2024-01-05;,citation_year=2024;,citation_language=en;,citation_publisher=Manifest AI;">
<meta name="citation_reference" content="citation_title=LongCrawl64: A Long-Context Natural-Language Dataset;,citation_author=Jacob Buckman;,citation_publication_date=2024-05-16;,citation_cover_date=2024-05-16;,citation_year=2024;,citation_language=en;,citation_publisher=Manifest AI;">
</head>

<body class="floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../m-logo-tight.png" alt="" class="navbar-logo">
    </a>
  </div>
          <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://discord.gg/aFsCgDraGP"> <i class="bi bi-discord" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/manifest__ai"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
   
  <ul>
  <li><a href="#linear-transformers-with-embeddings" id="toc-linear-transformers-with-embeddings" class="nav-link active" data-scroll-target="#linear-transformers-with-embeddings">1. Linear Transformers with Embeddings</a></li>
  <li><a href="#experiments" id="toc-experiments" class="nav-link" data-scroll-target="#experiments">2. Experiments</a></li>
  <li><a href="#tensor-product" id="toc-tensor-product" class="nav-link" data-scroll-target="#tensor-product">3. Tensor Product</a>
  <ul class="collapse">
  <li><a href="#mathematical-background" id="toc-mathematical-background" class="nav-link" data-scroll-target="#mathematical-background">3.1. Mathematical Background</a></li>
  <li><a href="#implementation" id="toc-implementation" class="nav-link" data-scroll-target="#implementation">3.2. Implementation</a></li>
  </ul></li>
  <li><a href="#symmetric-power-transformers" id="toc-symmetric-power-transformers" class="nav-link" data-scroll-target="#symmetric-power-transformers">4. Symmetric Power Transformers</a>
  <ul class="collapse">
  <li><a href="#mathematical-background-1" id="toc-mathematical-background-1" class="nav-link" data-scroll-target="#mathematical-background-1">4.1. Mathematical Background</a></li>
  <li><a href="#implementation-1" id="toc-implementation-1" class="nav-link" data-scroll-target="#implementation-1">4.2 Implementation</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">5. Conclusion</a>
  <ul class="collapse">
  
  </ul></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Symmetric Power Transformers</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Authors</div>
    <div class="quarto-title-meta-contents">
             <p>Jacob Buckman </p>
             <p>Carles Gelada </p>
             <p>Sean Zhang </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">August 15, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<div class="hidden">
<p><span class="math display">\[
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\sft}{\text{softmax}}
\newcommand{\List}{\text{List}}
\newcommand{\Seq}{\text{Seq}}
\newcommand{\SeqT}{\text{SeqT}}
\newcommand{\CSeqT}{\text{CSeqT}}
\newcommand{\Dist}{\text{Dist}}
\newcommand{\SM}{\text{SM}}
\newcommand{\Fn}{\text{Fn}}
\newcommand{\Tok}{\text{Tok}}
\newcommand{\Aij}{ A_{[i,j]}}
\newcommand{\ten}{\small\text{tensor}}
\newcommand{\sym}{\small\text{symmetric}}
\newcommand{\flat}[1]{\text{flat}\left(#1\right)}
\newcommand{\stab}{\text{stab}}
\newcommand{\orbit}{\text{orbit}}
\]</span></p>
</div>
<style>
summary {
  cursor: pointer;
  padding: 8px;
  background-color: #f0f0f0; /* Light grey background */
  border-radius: 8px; /* Rounded corners for consistency */
}

/* Hover state changes the background color to a lighter grey */
summary:hover {
  background-color: #e0e0e0; /* Lighter grey on hover */
}

/* Override hover effect when details is open */
details[open] summary:hover {
  background-color: #f0f0f0; /* Maintain the same color as the non-hover state */
}

details[open] summary {
  padding: 8px;
}

details {
  padding: 0; /* No padding when details is not open */
  border-radius: 8px; /* Rounded corners for consistency */
  background-color: #f0f0f0; /* Light grey background */
}

details[open] {
  padding: 8px; /* Padding inside the details for content alignment */
}
</style>
<p>Linear transformers <span class="citation" data-cites="katharopoulos2020transformers"><a href="#ref-katharopoulos2020transformers" role="doc-biblioref">[1]</a></span> can be formulated as linear-cost RNNs, which have better theoretical context scaling than ordinary transformers. <a href="https://manifestai.com/articles/linear-transformers-are-faster/">In our previous article</a> <span class="citation" data-cites="buckman2024a"><a href="#ref-buckman2024a" role="doc-biblioref">[2]</a></span>, we presented an efficient chunked algorithm that turns this theoretical advantage into practical speedups when the context is long: 10x faster training for a 64k-token context. Unfortunately, we also found that vanilla linear transformers suffer from <a href="https://manifestai.com/articles/linear-transformers-are-faster/#learning-performance">degraded performance</a>, especially at long contexts, rendering any benefits from the speedup useless. This article advances our previous discussion by introducing a linear transformer variant that solves the degraded performance issue while still enabling an efficient linear-cost implementation.</p>
<p>Behind all the algorithms explored in this post there is a central idea: for RNNs, thinking about the size of the model purely in terms of the number of parameters misses something important. An RNN encodes all the information from the past inputs <span class="math inline">\(X_1,...,X_{t}\)</span> into a finite-dimensional vector <span class="math inline">\(S_t\)</span> called the state. If the states are too small, the model will struggle to store all the information it will later require. Could this be the cause of the poor performance of the GPT-2-style linear transformers we evaluated in our previous article? If we look at the state sizes, we notice they are many orders of magnitude smaller than the weights of the architecture:</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>A generic RNN equation would compute next states via <span class="math inline">\(S_t = g(S_{t-1}, X_t)\)</span> and produce outputs via <span class="math inline">\(Y_t = f(S_t, X_t)\)</span>.</p>
</div></div><div style="width: 65%; margin: auto; text-align: center;">
<table class="caption-top table">
<thead>
<tr class="header">
<th></th>
<th>Weights</th>
<th>State Size</th>
<th>State-Weight Ratio</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Small</strong></td>
<td>124M</td>
<td>589K</td>
<td>0.0048</td>
</tr>
<tr class="even">
<td><strong>Medium</strong></td>
<td>335M</td>
<td>1.5M</td>
<td>0.0045</td>
</tr>
<tr class="odd">
<td><strong>Large</strong></td>
<td>774M</td>
<td>2.9M</td>
<td>0.0037</td>
</tr>
<tr class="even">
<td><strong>XL</strong></td>
<td>1.6B</td>
<td>4.9M</td>
<td>0.0031</td>
</tr>
</tbody>
</table>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<p>The formula for the state size of a vanilla linear transformer is <code>layer_n * head_count * key_size * value_size</code>.</p>
</div></div><p>Fortunately, since the architecture is a linear transformer, this imbalance has a straightforward remedy. The size of the state of a linear transformer can be controlled by simply embedding the keys and queries in a higher-dimensional space. (The larger the space in which we embed the key, the larger the state becomes). Previous work <span class="citation" data-cites="wang2020linformer schlag2021linear kacham2023polysketchformer zhang2024hedgehog qin2024hgrn2 peng2021random choromanski2020rethinking arora2024simple"><a href="#ref-wang2020linformer" role="doc-biblioref">[3]</a>–<a href="#ref-arora2024simple" role="doc-biblioref">[10]</a></span> has already observed that this improves the performance of linear transformers, but the resulting architectures are still not competitive with standard transformers.</p>
<p>In this article we introduce <em>symmetric power transformers</em>, which are a variant of linear transformers with an embedding function based on the theory of symmetric tensors. They have a hyperparameter <span class="math inline">\(p\)</span> that controls the state size. For <span class="math inline">\(p=4\)</span> and above, they outperform the transformer baseline, and at <span class="math inline">\(p=4\)</span> and below, they have a state size small enough to fit on a modern GPU. A second advantage is that, unlike other variants of linear transformers, one can combine symmetric power transformers with the commonly used rotary embeddings <span class="citation" data-cites="su2024roformer"><a href="#ref-su2024roformer" role="doc-biblioref">[11]</a></span>. Below, we see a performance comparison (full experimental details are given in Section 2).</p>
<div class="column-body" style="text-align: center;">
<iframe width="600" height="350" src="plots/01_symmetric_power.html">
</iframe>
</div>
<p>In this article, our experiments used the attention formulation of linear transformers (with <span class="math inline">\(O(t^2)\)</span> cost) rather than the more efficient chunked formulation (with <span class="math inline">\(O(t)\)</span> cost). This allowed us to validate the learning ability of the architecture without writing custom CUDA kernels (which an efficient implementation of chunked symmetric power transformers requires). We will release the efficient chunked implementation in an upcoming article.</p>
<section id="linear-transformers-with-embeddings" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="linear-transformers-with-embeddings">1. Linear Transformers with Embeddings</h2>
<p>We begin with a review of linear transformers. The inputs to a transformer layer are sequences of <span class="math inline">\(Q_i, K_i, V_i \in \R^d\)</span> of queries, keys, and values, where <span class="math inline">\(i\)</span> ranges from <span class="math inline">\(1\)</span> to the sequence length <span class="math inline">\(t\)</span>. The outputs are a sequence <span class="math inline">\(Y_i\in \R^d\)</span>. The formula for the output vectors is: <span class="math display">\[
Y_i = \sum_{j=1}^i A_{ij} V_j \qquad A_{ij}  = \frac{ \phi(Q_i)^T \phi(K_j)}{\sum_{k=1}^i \phi(Q_i)^T \phi(K_k) }
\]</span> where <span class="math inline">\(\phi : \R^d \to \R^D\)</span> is an embedding function that maps keys or queries into vectors of dimension <span class="math inline">\(D\)</span>. This formulation is what we call the <em>attention formulation</em> of a linear transformer, because it involves explicitly computing the attention scores used to weight the values <span class="math inline">\(V_j\)</span>.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>This linear transformer is the same architecture as in <a href="https://manifestai.com/articles/linear-transformers-are-faster/">our previous article</a>, but here we present the complete formula in its full generality, including both the normalizing term and embedding function (previously suppressed for clarity).</p>
</div></div><p>The exact same outputs can be computed via a <em>recurrent formulation</em>: <span class="math display">\[
Y_{i} = \frac{S_i \phi(Q_i)}{Z_i \phi(Q_i)} \qquad Z_i = Z_{i-1} + \phi(K_i)^T \qquad S_i = S_{i-1} + V_i \phi(K_i)^T
\]</span> where <span class="math inline">\(Z_0\)</span> and <span class="math inline">\(S_0\)</span> are <span class="math inline">\(\mathcal 0\)</span> vectors in their respective spaces. Since <span class="math inline">\(S_i \in \R^{d \times D}\)</span> and <span class="math inline">\(Z_i \in \R^{D}\)</span>, the size of the state is <span class="math inline">\(D(d+1)\)</span>.</p>

<div class="no-row-height column-margin column-container"><div class="">
<div style="margin-top: 100px;">
<p>Note that <span class="math inline">\(D(d+1)\)</span> gives the size of the state for any one linear transformer head. To compute the state size for an entire multi-head architecture, one must multiply by the number of layers and number of heads per layer.</p>
</div>
</div></div><details>
<summary>
Expand for a derivation of the recurrent formulation.
</summary>
If we expand the recurrent the definitions of <span class="math inline">\(S_i\)</span> and <span class="math inline">\(Z_i\)</span> we get that <span class="math display">\[
Z_i = \sum_{j=1}^i \phi(K_j)^T \qquad S_i = \sum_{j=1}^i V_j \phi(K_j)^T
\]</span> Then, starting with the attention formulation <span class="math display">\[
\begin{aligned}
Y_i &amp;= \sum_{j=1}^i \frac{ \phi(Q_i)^T \phi(K_j)}{\sum_{k=1}^i \phi(Q_i)^T \phi(K_k) } V_j \\
    &amp;= \sum_{j=1}^i V_j \frac{ \phi(K_j)^T \phi(Q_i)}{\sum_{k=1}^i \phi(K_k)^T \phi(Q_i) } \\
    &amp;= \frac{ \left( \sum_{j=1}^i V_j  \phi(K_j)^T \right) \phi(Q_i)}{ \left(\sum_{m=1}^i \phi(K_m)^T \right) \phi(Q_i) } \\
    &amp;= \frac{S_i\phi(Q_i)}{Z_i\phi(Q_i)} \\
\end{aligned}
\]</span>
</details>
<p>These two forms give rise to a variety of algorithms for training linear transformers, with differing computational properties. <a href="https://manifestai.com/articles/linear-transformers-are-faster/">Read our earlier article on linear transformers for a detailed explanation.</a> In particular, there are two algorithms that are relevant to our present discussion: <em>parallel attention</em> and <em>chunked</em>. The parallel attention algorithm is the standard algorithm used to train transformers. It does not require using the state, but its cost is <span class="math inline">\(O(t^2d)\)</span>, where <span class="math inline">\(t\)</span> is the sequence length. The chunked algorithm can be used to train only linear transformers. It has a cost of <span class="math inline">\(O(tdD)\)</span>, and it requires materializing the state. The chunked algorithm is primarily what makes linear transformers interesting, because it is what allows them to be trained much more efficiently than softmax transformers when <span class="math inline">\(t\)</span> is large.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p><em>Materializing</em> an object refers to storing it in the GPU main memory. Sometimes, a mathematical object is necessary to perform a computation, and yet one can avoid having to store the whole thing in RAM. The most prominent example of this is Flash Attention <span class="citation" data-cites="dao2022flashattention"><a href="#ref-dao2022flashattention" role="doc-biblioref">[12]</a></span>, which avoids having to materialize the <code>[t,t]</code> attention matrix.</p>
</div></div><p>With these computational considerations in mind, how should we choose <span class="math inline">\(\phi\)</span>? Here are some attributes that we want:</p>
<ul>
<li><strong>Adjustable dimensionality.</strong> To balance the size of the state with the size of the weights, there should be some hyperparameter controlling the dimension <span class="math inline">\(D\)</span>.</li>
<li><strong>Efficient dot product.</strong> In certain parts of the algorithm, <span class="math inline">\(\phi(Q_i)\)</span> and <span class="math inline">\(\phi(K_j)\)</span> appear as intermediate steps in the computation of <span class="math inline">\(\phi(Q_i)^T\phi(K_j)\)</span>. For some choices of <span class="math inline">\(\phi\)</span>, there is a more efficient formula for <span class="math inline">\(\phi(Q_i)^T\phi(K_j)\)</span> that does not require computing these intermediate objects.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></li>
<li><strong>Positive dot product.</strong> We want <span class="math inline">\(\phi(Q_i)^T \phi(K_j)\)</span> to always be positive. This ensures that each normalized output <span class="math inline">\(Y_i\)</span> is a convex combination of all preceding values <span class="math inline">\(V_1, \cdots, V_i\)</span>. We found this to be essential for stable and performant learning.</li>
<li><strong>Compatible with rotary positional encoding.</strong> Rotary encodings take <span class="math inline">\(Q_i \to R Q_i\)</span> and <span class="math inline">\(K_j \to R K_j\)</span>. To preserve their translational symmetry, we want <span class="math inline">\(\phi(R Q_i)^T \phi (R K_j) = \phi(Q_i)^T \phi (K_j)\)</span>.</li>
</ul>
<p>Subject to these four constraints, we are searching for the embedding function with the best empirical performance at each state size. Since our ultimate goal is to replace transformers (trained with attention) with linear transformers (trained with the chunked algorithm), the bar for success is simple: an embedding whose performance matches that of a transformer baseline at a state size small enough to be tractable. We’ll use 80 GB as our limit for the state size because that is the entire memory capacity of A100 and H100 GPUs.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<p>Many possible embedding functions have already been investigated in the literature <span class="citation" data-cites="wang2020linformer schlag2021linear kacham2023polysketchformer zhang2024hedgehog"><a href="#ref-wang2020linformer" role="doc-biblioref">[3]</a>–<a href="#ref-zhang2024hedgehog" role="doc-biblioref">[6]</a></span>, but we are not aware of one that satisfies all of our requirements. In the next section, we’ll describe a variant of attention whose empirical performance is competitive with softmax attention. In the sections that follow, we will show that this can be implemented via a linear transformer that satisfies our desiderata.</p>
</section>
<section id="experiments" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="experiments">2. Experiments</h2>
<p>At first, it might not be evident that the following attention layer corresponds to a linear transformer, but it should be clear that it is a reasonable modification to the standard softmax transformer. Let <span class="math inline">\(p\in\N\)</span> be even, <span class="math display">\[
Y_i = \sum_{j=1}^i A_{ij} V_j \qquad A_{ij}  = \frac{ (Q_i^T K_j)^p}{\sum_{k=1}^i (Q_i^T K_k)^p }
\]</span> A classic softmax transformer would guarantee that all the attention <span class="math inline">\(A_{i1}, \cdots, A_{ii}\)</span> are positive and sum to one by applying the exponential <span class="math inline">\(e^{Q_i^T K_j}\)</span>. Instead, this variant does <span class="math inline">\((Q_i^T K_j)^p\)</span> to achieve the same result. Raising each inner product to an even power makes the term positive, and dividing by the sum ensures each row of the attention is a distribution. We call this variant <em>even-power attention</em>, it has been studied before in the literature <span class="citation" data-cites="kacham2023polysketchformer"><a href="#ref-kacham2023polysketchformer" role="doc-biblioref">[5]</a></span>.</p>
<p>Like softmax attention, even-power attention is compatible with rotary embeddings. A key motivation for using rotary embeddings is that they are <em>relative</em>, meaning that only the difference in time <span class="math inline">\(i-j\)</span> influences the attention score <span class="math inline">\(A_{ij}\)</span>. See <span class="citation" data-cites="su2024roformer"><a href="#ref-su2024roformer" role="doc-biblioref">[11]</a></span> for the full details, but in short, the relative property of rotary embeddings is guaranteed when the attention scores <span class="math inline">\(A_{ij}\)</span> are unchanged by the rotation of <span class="math inline">\(Q_i\)</span> and <span class="math inline">\(K_j\)</span> by the same rotation matrix <span class="math inline">\(R \in \R^{d \times d}\)</span>. This holds for even-power attention: <span class="math display">\[
\left( (R Q_i)^T R K_j \right)^p =  \left( Q_i^T R^T R K_j \right)^p = \left( Q_i^T K_j \right)^p
\]</span> Many other variants of linear transformers do not have this property <span class="citation" data-cites="katharopoulos2020transformers"><a href="#ref-katharopoulos2020transformers" role="doc-biblioref">[1]</a></span>.</p>
<p>Here is a simple JAX implementation of even-power attention:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="kw">def</span> even_power_attention(Q, K, V, p):</span>
<span id="cb1-2"><a href="#cb1-2"></a>    <span class="co"># even only</span></span>
<span id="cb1-3"><a href="#cb1-3"></a>    <span class="cf">assert</span> p <span class="op">%</span> <span class="dv">2</span> <span class="op">==</span> <span class="dv">0</span></span>
<span id="cb1-4"><a href="#cb1-4"></a>    <span class="co"># compute inner products</span></span>
<span id="cb1-5"><a href="#cb1-5"></a>    C <span class="op">=</span> Q <span class="op">@</span> K.T</span>
<span id="cb1-6"><a href="#cb1-6"></a>    <span class="co"># raise to power</span></span>
<span id="cb1-7"><a href="#cb1-7"></a>    B <span class="op">=</span> D<span class="op">**</span>p</span>
<span id="cb1-8"><a href="#cb1-8"></a>    <span class="co"># apply causal mask</span></span>
<span id="cb1-9"><a href="#cb1-9"></a>    B <span class="op">=</span> where(tril(ones(B.shape)), B, <span class="dv">0</span>)</span>
<span id="cb1-10"><a href="#cb1-10"></a>    <span class="co"># project to simplex</span></span>
<span id="cb1-11"><a href="#cb1-11"></a>    A <span class="op">=</span> B <span class="op">/</span> B.<span class="bu">sum</span>(<span class="op">-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-12"><a href="#cb1-12"></a>    <span class="co"># compute output</span></span>
<span id="cb1-13"><a href="#cb1-13"></a>    Y <span class="op">=</span> A <span class="op">@</span> V</span>
<span id="cb1-14"><a href="#cb1-14"></a>    <span class="cf">return</span> Y</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This implementation turns out to be more pedagogical than practical, because numerical stability is an important empirical consideration. Expand below for an implementation that addresses these issues.</p>
<details>
<summary>
Numerically-stable implementation of power attention.
</summary>
<p>Numerical instabilities come from numbers underflowing (too small) or overflowing (too large). Solutions typically fall into a few main categories:</p>
<ul>
<li>Make sure a number is not too small or too large, e.g.&nbsp;turn <code>log(x)</code> into <code>log(x + ε)</code>. This prevents overflow.</li>
<li>Accumulate in <code>fp32</code>. When accumulating a long list of small values in half-precision, it is sometimes the case that each addition will underflow and no accumulation will occur at all.</li>
<li>Manipulate an equation to cancel out some common factors algebraically, rather than letting them cancel out computationally. For example, to calculate <span class="math inline">\(\frac{x}{y}\)</span> where <span class="math inline">\(x = Am\)</span> and <span class="math inline">\(y = An\)</span> for some large <span class="math inline">\(A\)</span>, compute <code>m / n</code> instead of <code>x / y</code>.</li>
<li>Separate magnitude and sign, and work with magnitude in log-space, i.e.&nbsp;manipulate <code>sign(x)</code> and <code>log(abs(x))</code> instead of working with <code>x</code> directly. Convert back to linear-space with <code>sign(x) * exp(f(log(abs(x))))</code>, where <code>f</code> has internally completed the relevant cancellations, so <code>f(log(abs(x)))</code> is small enough to avoid overflow.</li>
</ul>
<p>With these techniques in mind, we can implement a numerically-stable version of the attention algorithm. This is the code used to generate the experimental results in this article.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="kw">def</span> even_power_attn(Q, K, V, p, ε):</span>
<span id="cb2-2"><a href="#cb2-2"></a>     <span class="co"># even only</span></span>
<span id="cb2-3"><a href="#cb2-3"></a>     <span class="cf">assert</span> p <span class="op">%</span> <span class="dv">2</span> <span class="op">==</span> <span class="dv">0</span></span>
<span id="cb2-4"><a href="#cb2-4"></a></span>
<span id="cb2-5"><a href="#cb2-5"></a>    <span class="co"># compute inner products</span></span>
<span id="cb2-6"><a href="#cb2-6"></a>    D <span class="op">=</span> Q <span class="op">@</span> K.T</span>
<span id="cb2-7"><a href="#cb2-7"></a></span>
<span id="cb2-8"><a href="#cb2-8"></a>    <span class="co"># raise to power, in log space for numerical stability</span></span>
<span id="cb2-9"><a href="#cb2-9"></a>    log_C <span class="op">=</span> p <span class="op">*</span> log(<span class="bu">abs</span>(D) <span class="op">+</span> ε)</span>
<span id="cb2-10"><a href="#cb2-10"></a>    <span class="co"># apply causal mask</span></span>
<span id="cb2-11"><a href="#cb2-11"></a>    log_C <span class="op">=</span> where(tril(ones(log_C.shape)), log_C, <span class="op">-</span>inf)</span>
<span id="cb2-12"><a href="#cb2-12"></a>    <span class="co"># subtract rowmax for numerical stability</span></span>
<span id="cb2-13"><a href="#cb2-13"></a>    log_C <span class="op">-=</span> log_C.<span class="bu">max</span>(axis<span class="op">=-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-14"><a href="#cb2-14"></a>    </span>
<span id="cb2-15"><a href="#cb2-15"></a>    <span class="co"># Return to linear space</span></span>
<span id="cb2-16"><a href="#cb2-16"></a>    B <span class="op">=</span> exp(log_C)</span>
<span id="cb2-17"><a href="#cb2-17"></a>    <span class="co"># Compute the normalizing term, accumulating in float32 for numerical stability</span></span>
<span id="cb2-18"><a href="#cb2-18"></a>    denom <span class="op">=</span> B.<span class="bu">sum</span>(<span class="op">-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>, dtype<span class="op">=</span>float32).astype(B.dtype)</span>
<span id="cb2-19"><a href="#cb2-19"></a>    </span>
<span id="cb2-20"><a href="#cb2-20"></a>    <span class="co"># project to simplex, adding ε for numerical stability</span></span>
<span id="cb2-21"><a href="#cb2-21"></a>    A <span class="op">=</span> B <span class="op">/</span> (denom <span class="op">+</span> ε)</span>
<span id="cb2-22"><a href="#cb2-22"></a>    <span class="co"># compute output</span></span>
<span id="cb2-23"><a href="#cb2-23"></a>    Y <span class="op">=</span> A <span class="op">@</span> V</span>
<span id="cb2-24"><a href="#cb2-24"></a>    <span class="cf">return</span> Y</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<p>Now we are ready to train some models. For this experiment, we used the <a href="https://manifestai.com/articles/longcrawl64/">LongCrawl64 dataset</a> <span class="citation" data-cites="buckman2024b"><a href="#ref-buckman2024b" role="doc-biblioref">[13]</a></span>, a context length of 4096, a batch size of 524288 tokens. The architecture was similar to the 124M-parameter GPT-2 architecture, but with rotational positional encoding and an additional layernorm after input embeddings. The optimization was conducted in bf16 mixed-precision using Adam with learning rate .0006 and no scheduling. Each model was trained on a node of 8 H100s.</p>
<div class="column-body" style="text-align: center;">
<iframe width="600" height="350" src="plots/01_symmetric_power.html">
</iframe>
</div>
<p>Performance seems to improve consistently as we increase <span class="math inline">\(p\)</span>. For large enough <span class="math inline">\(p\)</span>, the performance of even-power transformers matches or even surpasses that of the softmax transformer baseline. This architecture is looking promising!</p>
<p>In Section 3, we will show how to use the tensor product to implement even-power attention as a linear transformer, albeit one with a state size so large as to be impractical. In Section 4, we will see that the tensor product embedding is highly symmetric and contains a lot of redundant information. We will exploit this structure to construct an embedding function for even-power attention with tractable state size.</p>
</section>
<section id="tensor-product" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="tensor-product">3. Tensor Product</h2>
<p>In this section, we show how the <a href="https://en.wikipedia.org/wiki/Tensor_product">tensor product</a>, a deep and ubiquitous mathematical idea, can be used to construct embeddings for the even-power transformer.</p>
<section id="mathematical-background" class="level3">
<h3 class="anchored" data-anchor-id="mathematical-background">3.1. Mathematical Background</h3>
<p>The tensor product of vectors generalizes the outer product and formalizes the concepts of multi-dimensional arrays. Given two vectors <span class="math inline">\(v\in \R^{d_1}\)</span> and <span class="math inline">\(w\in \R^{d_2}\)</span> one can think of their tensor product <span class="math inline">\(v\otimes w\)</span> as the matrix <span class="math inline">\(v w^T \in \R^{d_1\times d_2}\)</span>, <span class="math display">\[
v w^T = \left[
\begin{array}{cccc}
v_1w_1 &amp; v_1w_2 &amp; \cdots &amp; v_1w_m \\
v_2w_1 &amp; v_2w_2 &amp; \cdots &amp; v_2w_m \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
v_nw_1 &amp; v_nw_2 &amp; \cdots &amp; v_nw_m \\
\end{array}
\right]
\]</span> Intuitively, <span class="math inline">\(v \otimes w \otimes u\)</span> would be a 3-dimensional table containing all possible entries of the sort <span class="math inline">\(v_i
w_j u_k\)</span>. But let’s make the intuition of multi-dimensional tables more rigorous.</p>
<p><strong>Multi-indices.</strong> A multi-index <span class="math inline">\(\alpha\)</span> specifies a location in a <span class="math inline">\(p\)</span> dimensional table with dimension sizes <span class="math inline">\(d_1,
\cdots, d_p \in \N\)</span>. Let <span class="math inline">\(\N_d\)</span> denote the set <span class="math inline">\(\{1,2,\cdots,d\}\)</span>. Then, the space of multi-indices is <span class="math inline">\(\N_{d_1} \times \cdots \times \N_{d_p}\)</span> and we refer to a generic multi-index as <span class="math inline">\(\alpha = [\alpha_1, \cdots, \alpha_p] \in \N_{d_1} \times \cdots \times \N_{d_p}\)</span>.</p>
<p><strong>Tensors.</strong> A tensor <span class="math inline">\(T\in \R^{d_1 \times \cdots d_p}\)</span> corresponds to a high-dimensional table where every location has a numerical value assigned to it. In other words, it is a map from multi-indices to the reals. <span class="math display">\[
T: \N_{d_1} \times \cdots \times \N_{d_p} \to \R
\]</span> By convention, we index tensors with subscript notation <span class="math inline">\(T_\alpha\)</span> instead of functional notation <span class="math inline">\(T(\alpha)\)</span>, but the meaning is the same.</p>
<p><strong>Tensor product of vectors.</strong> Given a list of <span class="math inline">\(p\)</span> vectors <span class="math inline">\(v_i \in \R^{n_i}\)</span>, we denote by <span class="math inline">\(v_1 \otimes \cdots
\otimes v_p\)</span> (or alternatively <span class="math inline">\(\bigotimes_{i=1}^p v_i\)</span>) as the tensor in <span class="math inline">\(\R^{n_1 \times \cdots \times n_p}\)</span> with entries given by the following formula: <span class="math display">\[
\left[\bigotimes_{i=1}^p v_i\right]_\alpha = \prod_{i=1}^p v_{i, \alpha_i}
\]</span> Where <span class="math inline">\(v_{i,j}\in \R\)</span> denotes the <span class="math inline">\(j\)</span>th entry of the <span class="math inline">\(i\)</span>th vector.</p>
<p><strong>Flattening.</strong> To build embedding functions <span class="math inline">\(\phi\)</span>, we are going to use the tensor product to embed lists of vectors into <span class="math inline">\(\R^{d_1\times \cdots d_p}\)</span> tensors. But once we’ve done that, we will no longer care about the tensor structure and we will prefer to think of them as vectors in <span class="math inline">\(\R^D\)</span>, where <span class="math inline">\(D=\prod_{i=1}^p d_i\)</span>. The map <span class="math inline">\(\text{flat}: \R^{d_1\times \cdots d_p} \to \R^D\)</span> implements this transformation by writing every entry of the array into a flat vector. This can be done with any bijective function <span class="math inline">\(\sigma: D \to \N_{d_1} \times \cdots \times \N_{d_p}\)</span> which effectively imposes an (arbitrary) ordering on the multi-indices.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> The flattening is defined as: <span class="math display">\[
\flat{T}_i = T_{\sigma(i)}
\]</span> The dot product of flattened tensors satisfies the following property: <span class="math display">\[
\flat{\bigotimes_{i=1}^p v_i}^T \flat{\bigotimes_{i=1}^p w_i} = \prod_{i=1}^p v_i^T w_i \qquad \text{(Result 1)}
\]</span></p>
<details>
<summary>
Expand to see a proof.
</summary>
We can just check the both sides of the equation match. First, <span class="math display">\[\begin{align}
\flat{\bigotimes_{i=1}^p v_i}^T \flat{\bigotimes_{i=1}^p w_i}
&amp;= \sum_{l=1}^D \left[ \bigotimes_{i=1}^p v_i \right]_{\sigma(l)} \left [ \bigotimes_{i=1}^p w_i \right]_{\sigma(l)}  \\
&amp;= \sum_{l=1}^D \prod_{i=1}^p v_{i, \sigma(l)_i} w_{i, \sigma(l)_i} \\
&amp;= \sum_{j_1=1}^{d_1} \cdots \sum_{j_p=1}^{d_p} \prod_{i=1}^p v_{i, j_i} w_{i, j_i} \\
\end{align}\]</span> Where we used the assumption that <span class="math inline">\(\sigma(l)\)</span> ranges over every possible combination of <span class="math inline">\([j_1, \cdots, j_p]\)</span>. On the other hand, <span class="math display">\[\begin{align}
\prod_{i=1}^p v_i^T w_i
&amp;= \prod_{i=1}^p \sum_{j=1}^{d_i}  v_{i, j} w_{i, j}  \\
&amp;= \sum_{j_1=1}^{d_1} \cdots \sum_{j_p=1}^{d_p} \prod_{i=1}^p v_{i, j_i} w_{i, j_i}
\end{align}\]</span> Where the last step used a generalization of the distributive property: <span class="math inline">\(\prod_{i=1}^p \sum_{j=1}^{d_i}  v_{i, j} = \sum_{j_1=1}^{d_1} \cdots \sum_{j_p=1}^{d_p} \prod_{i=1}^p v_{i, j_i}\)</span>
</details>
</section>
<section id="implementation" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="implementation">3.2. Implementation</h3>
<p>Armed with the tensor product, we are ready to define an embedding <span class="math inline">\(\phi^p_{\text{TP}}\)</span>, and in doing so define a linear transformer architecture. The definition is simple: embed a key by taking its tensor product with itself, <span class="math inline">\(p\)</span> times.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>In this section, we focus on the effect of <span class="math inline">\(\phi\)</span> on keys <span class="math inline">\(k\)</span>, but wlog all discussion applies equally to queries.</p>
</div></div><p><span class="math display">\[
\phi^p_{\text{TP}}(k) =
\text{flat}\left(\bigotimes_{i=1}^p k\right) \in \mathbb{R}^{d^p}
\]</span></p>
<p>If this embedding is used with even <span class="math inline">\(p\)</span>, the resulting architecture is an even-power transformer:</p>
<p><span class="math display">\[
\phi^p_{\text{TP}}(q)^T \phi^p_{\text{TP}}(k) =
\text{flat}\left(\bigotimes_{i=1}^p q\right)^T \text{flat}\left(\bigotimes_{i=1}^p k\right) =
\prod_{i=1}^p q^T k = (q^T k)^p
\]</span></p>
<p>where in the second step we used Result 1.</p>
<p>The implementation is straightforward.</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" role="tab" aria-controls="tabset-1-1" aria-selected="true">Embedding</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" role="tab" aria-controls="tabset-1-2" aria-selected="false">Test</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1-1-tab">
<div class="sourceCode" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="kw">def</span> tensor_power_embedding(k, p):</span>
<span id="cb3-2"><a href="#cb3-2"></a>    expanded_k <span class="op">=</span> k</span>
<span id="cb3-3"><a href="#cb3-3"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(p<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb3-4"><a href="#cb3-4"></a>        expanded_k <span class="op">=</span> expanded_k[...,<span class="va">None</span>] <span class="op">@</span> k[<span class="va">None</span>,:]</span>
<span id="cb3-5"><a href="#cb3-5"></a>    <span class="cf">return</span> expanded_k.flatten()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="tabset-1-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-2-tab">
<div class="sourceCode" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="kw">def</span> even_power(q, k, p):</span>
<span id="cb4-2"><a href="#cb4-2"></a>  <span class="cf">return</span> np.inner(q, k) <span class="op">**</span> p</span>
<span id="cb4-3"><a href="#cb4-3"></a></span>
<span id="cb4-4"><a href="#cb4-4"></a><span class="kw">def</span> tensor_power_inner_product(q, k, p):</span>
<span id="cb4-5"><a href="#cb4-5"></a>  embedded_q <span class="op">=</span> tensor_power_embedding(q, p)</span>
<span id="cb4-6"><a href="#cb4-6"></a>  expanded_k <span class="op">=</span> tensor_power_embedding(k, p)</span>
<span id="cb4-7"><a href="#cb4-7"></a>  <span class="cf">return</span> (embedded_q <span class="op">*</span> expanded_k).<span class="bu">sum</span>()</span>
<span id="cb4-8"><a href="#cb4-8"></a></span>
<span id="cb4-9"><a href="#cb4-9"></a>d <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb4-10"><a href="#cb4-10"></a><span class="cf">for</span> p <span class="kw">in</span> [<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">6</span>, <span class="dv">8</span>]:</span>
<span id="cb4-11"><a href="#cb4-11"></a>  q <span class="op">=</span> np.random.random(d)</span>
<span id="cb4-12"><a href="#cb4-12"></a>  k <span class="op">=</span> np.random.random(d)</span>
<span id="cb4-13"><a href="#cb4-13"></a>  <span class="cf">assert</span> np.allclose(</span>
<span id="cb4-14"><a href="#cb4-14"></a>    even_power(q, k, p), </span>
<span id="cb4-15"><a href="#cb4-15"></a>    tensor_power_inner_product(q, k, p)</span>
<span id="cb4-16"><a href="#cb4-16"></a>  )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
<p>Embedding in hand, we can return to our main objective. Have we found an linear transformer whose performance is competitive with that of a strong transformer baseline, while, at the same time, having a state size small enough to fit on a GPU?</p>
<p>The table below shows the size of a single state, as measured in bytes (assuming fp16/bf16 precision), for a 124M-parameter GPT-2 tensor power transformer at various <span class="math inline">\(p\)</span>.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>The formula for the state size of a linear transformer with the tensor power embedding is <code>layer_n * head_count *  key_size**p * value_size</code>.</p>
</div></div><div style="width: 90%; margin: auto; text-align: center;">
<table class="caption-top table">
<colgroup>
<col style="width: 3%">
<col style="width: 15%">
<col style="width: 22%">
<col style="width: 36%">
<col style="width: 22%">
</colgroup>
<thead>
<tr class="header">
<th>p</th>
<th>State Size</th>
<th>Memory ≤ 80 GB?</th>
<th>Relative Loss at 100K Steps</th>
<th>Loss ≤ baseline?</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>2</td>
<td>77 MB</td>
<td>✔</td>
<td>1.03x</td>
<td>✘</td>
</tr>
<tr class="even">
<td>4</td>
<td>314 GB</td>
<td>✘</td>
<td>.98x</td>
<td>✔</td>
</tr>
<tr class="odd">
<td>6</td>
<td>1.3 PB</td>
<td>✘</td>
<td>.97x</td>
<td>✔</td>
</tr>
</tbody>
</table>
</div>
<p>The settings of <span class="math inline">\(p\)</span> that improve upon the baseline have states that are far too large. So the embedding <span class="math inline">\(\phi^p_{\text{TP}}\)</span> still does not satisfy all the properties we are after. But we are close.</p>
</section>
</section>
<section id="symmetric-power-transformers" class="level2">
<h2 class="anchored" data-anchor-id="symmetric-power-transformers">4. Symmetric Power Transformers</h2>
<p>The missing piece is to realize the huge embeddings we’ve been working with are highly symmetric. The theory of symmetric powers will help us compress the same information into much smaller objects. We will begin with an introduction to the relevant mathematical ideas. Then, we will put them to use by proposing <em>symmetric power transformers</em>, whose embedding is <span class="math inline">\(\phi^p_{\text{SYM}}\)</span>.</p>
<p>To build intuition, observe that the embedding <span class="math inline">\(\phi^2_{\text{TP}}(v)= \flat {v v^T}\)</span> is somewhat wasteful. The matrix <span class="math inline">\(v v^T\)</span> is symmetric, so all the information we need can be found in the upper triangular part of the matrix. <span class="math display">\[
v v^T = \left[
\begin{array}{cccc}
v_1v_1 &amp; v_1v_2 &amp; \cdots &amp; v_1v_m \\
v_2v_1 &amp; v_2v_2 &amp; \cdots &amp; v_2v_m \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
v_nv_1 &amp; v_nv_2 &amp; \cdots &amp; v_nv_m \\
\end{array}
\right]
\]</span> Entries at indices <span class="math inline">\((i,i)\)</span> appear a single time, but due to the commutativity of scalar multiplication (i.e.&nbsp;<span class="math inline">\(v_i v_j =
v_j v_i\)</span>), the entries at indices <span class="math inline">\((i,j)\)</span> each appear twice (if <span class="math inline">\(i\neq j\)</span>).</p>
<p>Noticing this symmetry in the matrix <span class="math inline">\(v v^T\)</span> allows us to create an alternative embedding, <span class="math inline">\(\phi^2_\text{SYM}: \R^d \to \R^{\frac{d^2 +d} 2}\)</span>, which can be implemented as:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="kw">def</span> sym_2_embedding(v):</span>
<span id="cb5-2"><a href="#cb5-2"></a>  x, d <span class="op">=</span> [], v.size</span>
<span id="cb5-3"><a href="#cb5-3"></a>  <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(d):</span>
<span id="cb5-4"><a href="#cb5-4"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(i, d):</span>
<span id="cb5-5"><a href="#cb5-5"></a>      count <span class="op">=</span> <span class="dv">1</span> <span class="cf">if</span> i<span class="op">==</span>j <span class="cf">else</span> <span class="dv">2</span></span>
<span id="cb5-6"><a href="#cb5-6"></a>      x.append(sqrt(count) <span class="op">*</span> v[i] <span class="op">*</span> v[j])</span>
<span id="cb5-7"><a href="#cb5-7"></a>  <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This construction of <span class="math inline">\(\phi^2_\text{SYM}\)</span> guarantees that <span class="math inline">\(\phi^2_\text{SYM}(v)^T \phi^2_\text{SYM}(w) = \phi^2_\text {TP}
(v)^T \phi^2_\text{TP}(w)= (v^T w)^2\)</span>. Recall that in the attention formulation of the linear transformer the embedding <span class="math inline">\(\phi\)</span> only influences the outputs via the attention scores, which were defined as <span class="math display">\[
A_{ij}  = \frac{ \phi(Q_i)^T \phi(K_j)}{\sum_{k=1}^i \phi(Q_i)^T \phi(K_k) }
\]</span> Then two linear transformers with embeddings <span class="math inline">\(\phi^2_\text{TP}(v)\)</span> and <span class="math inline">\(\phi^2_\text{SYM}(v)\)</span> will have exactly the same outputs, since they have the same inner products <span class="math inline">\(\phi(Q_i)^T \phi(K_j)\)</span> (namely, <span class="math inline">\((Q_i^T K_j)^p\)</span>). We’ve been able to exploit the symmetry of <span class="math inline">\(v v^T\)</span> to construct an equivalent embedding function with approximately half the dimensionality!</p>
<p>In this section, we will generalize this idea to arbitrary powers.</p>
<section id="mathematical-background-1" class="level3">
<h3 class="anchored" data-anchor-id="mathematical-background-1">4.1. Mathematical Background</h3>
<p>We begin by introducing some key tools.</p>
<p><strong>Permutation group.</strong> The first thing we need is the <a href="https://en.wikipedia.org/wiki/Permutation">permutation group</a> of <span class="math inline">\(p\)</span> elements, which is defined as the set of all functions <span class="math inline">\(\rho: \N_p \to \N_p\)</span> that are invertible and denoted by <span class="math inline">\(G_p\)</span>. We also overload the notation slightly. For a multi-index <span class="math inline">\(\alpha = [\alpha_1, \cdots, \alpha_p]\)</span> define the permutation of the multi-index as <span class="math inline">\(\rho(\alpha) = [\alpha_{\rho(1)}, \cdots, \alpha_{\rho(p)}]\)</span>. This is useful to define symmetric tensors.</p>
<p><strong>Symmetric tensors.</strong> A tensor <span class="math inline">\(T \in \R^{\underbrace{d\times \cdots \times d}_p}\)</span> is symmetric if for all multi-indices <span class="math inline">\(\alpha \in \N_d \times \cdots \times \N_d\)</span> and permutations <span class="math inline">\(\rho \in G_p\)</span> we have that: <span class="math display">\[
T_\alpha = T_{\rho(\alpha)}
\]</span></p>
<p><strong>Symmetric power of vectors.</strong> We use the notation <span class="math inline">\(v^{\otimes p}\)</span> to refer to <span class="math inline">\(\otimes^p_{i=1} v\)</span> (the tensor product of <span class="math inline">\(p\)</span> copies of <span class="math inline">\(v\)</span>), and call it the <span class="math inline">\(p\)</span>th symmetric power of <span class="math inline">\(v\)</span>. Due to the commutativity of multiplication, all symmetric powers of vectors are symmetric tensors. For example, for a multi-index <span class="math inline">\([1, 2, 3]\)</span>, the entrie <span class="math inline">\([v^{\otimes 3}]_{[1, 2, 3]} = v_1 v_2 v_3\)</span> will equal <span class="math inline">\([v^{\otimes 3}]_{[3, 2, 1]} = v_3 v_2 v_1\)</span>. Showing that a general tensor <span class="math inline">\(v^{\otimes p}\)</span> is symmetric is simple: <span class="math display">\[
\left[ v^{\otimes p} \right ]_{\rho(\alpha)} = \prod_{i=1}^p v_{\alpha_{\rho(i)}} = \prod_{i=1}^p v_{\alpha_i} = \left[ v^{\otimes p} \right ]_{\alpha}
\]</span></p>
<p>To construct embeddings that exploit the symmetries of <span class="math inline">\(T=v^{\otimes p}\)</span> we will need some key properties about symmetric tensors:</p>
<ul>
<li><strong>Duplication counts:</strong> If <span class="math inline">\(T\)</span> is symmetric, the entry <span class="math inline">\(T_\alpha\)</span> might have duplicate entries. To know how many duplicates a multi-index <span class="math inline">\(\alpha\)</span> has, we first need to count how many times each number <span class="math inline">\(i\in \{1, \cdots, d\}\)</span> occurs in <span class="math inline">\(\alpha\)</span>. Define the counts <span class="math inline">\(c_i = \sum_{j=1}^p \delta(\alpha_j, i)\)</span>. Then, the number of multi-indices containing the same data as <span class="math inline">\(\alpha\)</span> is given by the formula <span class="math inline">\(\frac{d!}{c_1 ! \; \cdots \; c_p!}\)</span>.</li>
<li><strong>Unique multi-indices:</strong> No data is lost if we restrict ourselves to only looking at entries <span class="math inline">\(T_\alpha\)</span> for multi-indices <span class="math inline">\(\alpha\)</span> that are non-decreasing (i.e.&nbsp;<span class="math inline">\(\alpha_i \le \alpha_{i+1}\)</span>). The intuition is that an arbitrary multi-index <span class="math inline">\(\beta\)</span> can always be transformed into a non-decreasing multi-index <span class="math inline">\(\alpha\)</span> by applying some permutation <span class="math inline">\(\rho\)</span>. Using the defining property of symmetric tensors, <span class="math inline">\(T_\beta = T_{\rho(\beta)} = T_\alpha\)</span>. Thus, we lose no information by excluding every multi-index that isn’t non-decreasing.</li>
<li><strong>Dimension:</strong> The space of symmetric tensors has dimension <span class="math inline">\(\binom{d+p-1}{p}\)</span>. This can be derived via a classic combinatorial argument counting the number of non-decreasing sequences.</li>
</ul>
<details>
<summary>
Expand to see a complete derivation of these properties and a few other relevant facts about symmetric tensors.
</summary>
<p><strong>Duplicate counts.</strong> By definition, the only constraint a symmetric tensor has, is that all the entries <span class="math inline">\(T_{\rho(\alpha)}\)</span> must be the same for all permutations <span class="math inline">\(\rho \in G_p\)</span>. Now we want to understand the amount of duplication that that any specific <span class="math inline">\(\alpha\)</span> has. Since the number of permutations of the multi-indices is <span class="math inline">\(|G_p| = p!\)</span>, a naive estimate would be that every entrie <span class="math inline">\(T_\alpha\)</span> appears <span class="math inline">\(p!\)</span> times in the tensor. And indeed, that is the case for some multi-indices. For example, every permutation <span class="math inline">\(\rho \in G_3\)</span> sends the multi-index <span class="math inline">\([1,4,6]\)</span> to a different multi-index, so there are <span class="math inline">\(3!\)</span> entries with the same value. But, on the other hand, for the multi-index <span class="math inline">\([1,1,1]\)</span> it doesn’t matter what permutation <span class="math inline">\(\rho\)</span> we apply, we always have that <span class="math inline">\(\rho \alpha = \alpha\)</span>. So the entrie at <span class="math inline">\([1,1,1]\)</span> has no duplicates.</p>
<p>To count the number of duplicates for a generic mulit-index <span class="math inline">\(\alpha\)</span> we are going to use the <a href="https://proofwiki.org/wiki/Orbit-Stabilizer_Theorem">orbit stabilizer theorem</a>. This theorem tells us that the number of elements in the set <span class="math inline">\(\orbit(\alpha) = \{ \rho(\alpha) \; | \; \rho \in G_p \}\)</span> is given by the formula: <span class="math display">\[
|\orbit(\alpha) | =\frac {|G_p|} {|\stab(\alpha)|}
\]</span> where the stabilizer <span class="math inline">\(\stab(\alpha) = \{ \rho \in G_p \; | \; \rho(\alpha) = \alpha \}\)</span> is the set of permutations that fix <span class="math inline">\(\alpha\)</span>. Working out the size of the stabilizer is not hard. For a permutation <span class="math inline">\(\rho \in G_p\)</span> to leave <span class="math inline">\(\alpha\)</span> fixed it must satisfy that <span class="math inline">\(\alpha_{\rho(i)} = \alpha_i\)</span> for all <span class="math inline">\(i\in \N_p\)</span>. In other words, <span class="math inline">\(\rho\)</span> must only interchange entreis of <span class="math inline">\(\alpha\)</span> that hold the same index. Say <span class="math inline">\(\alpha = [1,1,2]\)</span>, then we can only exchange the fist and second element. Generically, if index <span class="math inline">\(i\in\N_d\)</span> appears <span class="math inline">\(c_i\)</span> times in <span class="math inline">\(\alpha\)</span>, then there are <span class="math inline">\(c_i!\)</span> permutations that move around entries of <span class="math inline">\(\alpha\)</span> with value <span class="math inline">\(i\)</span> while keeping the rest fixed. From this, it is clear that: <span class="math display">\[
|\stab(\alpha)| = \prod_{i=1}^d c_i !
\]</span> For the example, the counts for <span class="math inline">\([1,1,2]\)</span> are <span class="math inline">\(c_1=2, \; c_2 = 1\)</span> so <span class="math inline">\(|\stab([1,1,2])| = 2! \: 1! =2\)</span>. With this, we can get the formula for the number of replicated entries of <span class="math inline">\(\alpha\)</span> by applying the orbit stabilizer theorem: <a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> <span class="math display">\[
|\orbit(\alpha) | = \frac {p!} {\prod_{i=1}^d c_i!}
\]</span></p>
<p><strong>Basis of symmetric tensors.</strong> We know that a lot of multi-indices of a symmetric tensor are redundant. To understand the true structure (like the dimensionality) of symmetric tensors we need to find a way to select an instance of each, non redundant, multi-indices. One way to do that is to restrict oursleves to <em>non decreasing multi-indices</em>. Denote them by <span class="math inline">\(P = \{\alpha \in \N_d^{\times p} \; | \; \alpha_i \le \alpha_{i+1} \}\)</span>. Then we can construct a basis for the space of symmetric tensors out of <span class="math inline">\(\alpha \in P\)</span> like <span class="math display">\[
S^\alpha = \sum_{\rho \in G_p} E^{\rho(\alpha)}
\]</span> where <span class="math inline">\(E^\alpha = \bigotimes^p_{i=1} e_{\alpha_i}\)</span>. (the tensors <span class="math inline">\(E^\alpha\)</span> are a the natural way to construct a basis for the non-symmetric tensors out of the basis <span class="math inline">\(e_i \in \R^d\)</span>) To convince ourselves that <span class="math inline">\(\{S^\alpha \; | \; \alpha \in P \}\)</span> forms a basis of the symmetric tensors we need to check that the set is linearly independent and that it spans all symmetirc tensors. Let’s check linear independence first. Assume that we have some coefficients <span class="math inline">\(x_\alpha \in \R\)</span> s.t. <span class="math display">\[
\sum_{\alpha \in P}  x_\alpha S^\alpha = 0
\]</span> Then, for any <span class="math inline">\(\beta \in P\)</span> <span class="math display">\[\begin{align}
\left[\sum_{\alpha \in P}  x_\alpha S^\alpha\right]_\beta
&amp;= \sum_{\alpha \in P}  x_\alpha S^\alpha_\beta
= \sum_{\alpha \in P} x_\alpha \sum_{\rho \in G_p} E^{\rho(\alpha)}_\beta \\
&amp;= \sum_{\alpha \in P} x_\alpha \sum_{\rho \in G_p} \delta(\rho(\alpha) = \beta) \\
\end{align}\]</span> Since <span class="math inline">\(\alpha, \beta \in P\)</span> the only way there can exist a <span class="math inline">\(\rho \in G_p\)</span> such that <span class="math inline">\(\rho(\alpha) = \beta\)</span> is when <span class="math inline">\(\alpha = \beta\)</span>. So <span class="math display">\[\begin{align}
0 &amp;= \left[\sum_{\alpha \in P}  x_\alpha S^\alpha\right]_\beta \\
&amp;= x_\beta \sum_{\rho \in G_p} \delta(\rho(\beta) = \beta) \\
&amp;= x_\beta \; | \stab (\alpha) | \\
\end{align}\]</span> And, since <span class="math inline">\(| \stab (\alpha) | \ge 1\)</span> that implies that <span class="math inline">\(x_\alpha = 0\)</span> and we have that the set of <span class="math inline">\(S^\alpha\)</span> is linearly independent. To show <span class="math inline">\(S^\alpha\)</span> span all symmetric tensors it we can just show that, for any symmetric tensor <span class="math inline">\(T\)</span>, if we define <span class="math display">\[
Q = \sum_{\alpha \in P} \frac {T_\alpha} {\stab(\alpha)} S^\alpha
\]</span> Then <span class="math inline">\(T = Q\)</span>. That can be easily seen by noticing that <span class="math inline">\(Q\)</span> is a symmetric tensor and that, evaluating <span class="math inline">\(Q\)</span> at <span class="math inline">\(\beta \in P\)</span> <span class="math display">\[\begin{align}
Q_\beta &amp;= \left[\sum_{\alpha \in P} \frac {T_\alpha} {\stab(\alpha)} S^\alpha \right]_\beta
= \sum_{\alpha \in P} \frac {T_\alpha} {\stab(\alpha)} \sum_{\rho \in G_p} E^{\rho(\alpha)}_\beta \\
&amp;= \sum_{\alpha \in P} \frac {T_\alpha} {\stab(\alpha)} \sum_{\rho \in G_p} \delta(\rho(\alpha) = \beta) \\
&amp;= \frac {T_\beta} {\stab(\beta)} \sum_{\rho \in G_p} \delta(\rho(\beta) = \beta) \\
&amp;= T_\beta
\end{align}\]</span></p>
<p><strong>Dimension of symmetric tensors.</strong> Since we’ve created a basis for the space of symmetric tensors out of non-decreasing sequences we can establish the dimension of the space by counting all such sequences. This is a standard combinatorial problem solved via the method of <a href="https://en.wikipedia.org/wiki/Stars_and_bars_(combinatorics)">stars and bars</a>. Which tells us that the dimension is <span class="math display">\[\binom{d+p-1}{p}\]</span></p>
<p>The only thing we must note to apply the standard combinatorial results is that there is a 1-1 correspondance between non-decreasing sequences and multisets of <span class="math inline">\(\N_d\)</span> with cardinality <span class="math inline">\(p\)</span>. This is because there is a unique way to lay out a multi-set into a non-decreasing sequence. However many <span class="math inline">\(1\)</span>s there are in the multi-set, they will all come first, then all the <span class="math inline">\(2\)</span> etc…</p>
<p><strong>Symmetric powers of vectors span the symmetric tensors</strong> Showing that all tensors of the form <span class="math inline">\(v^{\otimes p}\)</span> are symmetric was tivial, but there is a harder question we might ask ourselves. Do we actually need all the <span class="math inline">\(\binom {d + p - 1} p\)</span> dimensions of the symmetric tensors? The way to formalize this question is to ask whether the space of all symmetric tensors is spanned by rank-1 symmetric tensors <span class="math inline">\(v^{\otimes p}\)</span>. We will prove that the answer is yes by building every basis vector <span class="math inline">\(S^\alpha\)</span> that way. Concretely, if we define <span class="math display">\[
Z^\alpha
= \sum_{b_1, \cdots, b_p = 0}^1
\prod_{i=1}^p (-1)^{b_i -1}
\left ( \sum_{j=1}^p b_j e_{\alpha_j}  \right)^{\otimes p}
\]</span> Turns out that <span class="math inline">\(Z^\alpha = S^\alpha\)</span>. It is evident that both, the <span class="math inline">\(Z^\alpha\)</span> and <span class="math inline">\(S^\alpha\)</span> are symmetric tensors so, to convince ourselves that they are equivalent, we just need to index them at a non-decreasing multi-index <span class="math inline">\(\beta \in P\)</span>. First, see that <span class="math display">\[\begin{align}
S^\alpha_\beta
&amp;= \sum_{\rho \in G_p} \left [ \bigotimes^p_{i=1} e_{\rho(\alpha)_i} \right]_\beta \\
&amp;= \sum_{\rho \in G_p} \prod^p_{i=1} \delta(\rho(\alpha)_i = \beta_i)  \\
&amp;= \sum_{\rho \in G_p} \delta(\rho(\alpha) = \beta)  \\
&amp;= |\stab(\alpha) | \; \delta(\alpha = \beta)  \\
\end{align}\]</span> And, on the other hand, <span class="math display">\[\begin{align}
Z^\alpha_\beta &amp;= \sum_{b_1, \cdots, b_p = 0}^1
\prod_{i=1}^p (-1)^{b_i -1}
\left [ \left ( \sum_{j=1}^p b_j e_{\alpha_j} \right)^{\otimes p} \; \right ]_\beta \\
&amp;= \sum_{b_1, \cdots, b_p = 0}^1
\prod_{i=1}^p (-1)^{b_i -1}
\prod_{i=1}^p \left ( \sum_{j=1}^p b_j \delta(\alpha_j = \beta_i)  \right)  \\
&amp;= \sum_{b_1, \cdots, b_p = 0}^1
\prod_{i=1}^p (-1)^{b_i -1}
\sum_{j_1, \cdots, j_p = 1}^d  \prod_{i=1}^p b_{j_i} \delta(\alpha_{j_i} = \beta_i)  \\
&amp;= \sum_{j_1, \cdots, j_p = 1}^d  \sum_{b_1, \cdots, b_p = 0}^1
\prod_{i=1}^p (-1)^{b_i -1}
\prod_{i=1}^p b_{j_i} \delta(\alpha_{j_i} = \beta_i)  \\
\end{align}\]</span> During the sum over all combinations of <span class="math inline">\(j_1, \cdots, j_p\)</span>, if there is any <span class="math inline">\(l \in \N_p\)</span> that does not appear in the set <span class="math inline">\(j_1, \cdots, j_p\)</span>, then that term of the sum will drop out. This is because the only place where <span class="math inline">\(b_l\)</span> will appear is in the term <span class="math inline">\((-1)^{b_l-1}\)</span>, so since we are summing over <span class="math inline">\(b_l \in \{0, 1\}\)</span>, the terms will cancel out. Thus, we can restrict ourselves to summing over <span class="math inline">\(j_1, \cdots, j_p\)</span> that contain every element in <span class="math inline">\(\N_p\)</span>. In other words, the <span class="math inline">\(j\)</span> terms must be a permutation <span class="math inline">\([j_1, \cdots, j_p] = \rho([1, \cdots, p])\)</span> for some <span class="math inline">\(\rho \in G_p\)</span>. So we can continue <span class="math display">\[\begin{align}
Z^\alpha_\beta &amp;=  \sum_{\rho \in G_p} \sum_{b_1, \cdots, b_p = 0}^1
\prod_{i=1}^p (-1)^{b_i -1}
\prod_{i=1}^p b_{\rho(i)} \delta(\alpha_{\rho(i)} = \beta_i)  \\
&amp;= \sum_{\rho \in G_p}  \prod_{i=1}^p  \delta(\alpha_{\rho(i)} = \beta_i)  \\
\end{align}\]</span> Where we used the fact that, since the inner term is multiplied by every <span class="math inline">\(b_i\)</span>, the only way the term can be non <span class="math inline">\(0\)</span> is if every single <span class="math inline">\(b_i =1\)</span>. Finally, we can wrap up the proof, <span class="math display">\[\begin{align}
Z^\alpha_\beta
&amp;= \sum_{\rho \in G_p} \prod_{i=1}^p \delta(\alpha_{\rho(i)} = \beta_i)  \\
&amp;= \sum_{\rho \in G_p} \delta(\rho(\alpha) = \beta)  \\
&amp;= |\stab(\alpha) | \; \delta(\alpha = \beta)  \\
&amp;= S^\alpha_\beta
\end{align}\]</span></p>
</details>
</section>
<section id="implementation-1" class="level3">
<h3 class="anchored" data-anchor-id="implementation-1">4.2 Implementation</h3>
<p>The symmetric power embedding <span class="math inline">\(\phi^p_\text{SYM}(v)\)</span> will give a list of <span class="math inline">\(\binom{d+p-1}{p}\)</span> numbers, each corresponding to <span class="math inline">\([v^{\otimes p}]_\alpha\)</span> for a non-decreasing <span class="math inline">\(\alpha\)</span>. Just as we did in the example of <span class="math inline">\(\phi^2_\text{SYM}\)</span>, we also need to apply a correction that is the square root of the duplicate count of that particular <span class="math inline">\(\alpha\)</span>. The inner product of two vectors embedded in this way is identical to the tensor power embedding, <span class="math inline">\(\phi^2_\text{SYM}(v)^T \phi^2_\text{SYM}(w) = \phi^2_\text {TP}(v)^T \phi^2_\text {TP}(w)\)</span>. The following is an example implementation of this embedding:</p>
<div class="tabset-margin-container"></div><div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-2-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-1" role="tab" aria-controls="tabset-2-1" aria-selected="true">Embedding</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-2-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-2-2" role="tab" aria-controls="tabset-2-2" aria-selected="false">Test</a></li></ul>
<div class="tab-content">
<div id="tabset-2-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-2-1-tab">
<div class="sourceCode" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a><span class="kw">def</span> symmetric_power_embedding(k, p):</span>
<span id="cb6-2"><a href="#cb6-2"></a>    d <span class="op">=</span> <span class="bu">len</span>(k)</span>
<span id="cb6-3"><a href="#cb6-3"></a>    x <span class="op">=</span> []</span>
<span id="cb6-4"><a href="#cb6-4"></a>    <span class="cf">for</span> midx <span class="kw">in</span> non_decreasing_multiindices(p, d):</span>
<span id="cb6-5"><a href="#cb6-5"></a>        c <span class="op">=</span> count(midx, d)</span>
<span id="cb6-6"><a href="#cb6-6"></a>        xi <span class="op">=</span> np.sqrt(multinomial(c))</span>
<span id="cb6-7"><a href="#cb6-7"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(p):</span>
<span id="cb6-8"><a href="#cb6-8"></a>            xi <span class="op">*=</span> k[midx[j]]</span>
<span id="cb6-9"><a href="#cb6-9"></a>        x.append(xi)</span>
<span id="cb6-10"><a href="#cb6-10"></a>    <span class="cf">return</span> np.array(x)</span>
<span id="cb6-11"><a href="#cb6-11"></a></span>
<span id="cb6-12"><a href="#cb6-12"></a><span class="co"># -- helper functions --</span></span>
<span id="cb6-13"><a href="#cb6-13"></a><span class="co"># generates list of non-decreasing multiindices</span></span>
<span id="cb6-14"><a href="#cb6-14"></a><span class="kw">def</span> non_decreasing_multiindices(n, max_idx, starting_from<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb6-15"><a href="#cb6-15"></a>    <span class="cf">if</span> n <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb6-16"><a href="#cb6-16"></a>        <span class="cf">return</span> [[i] <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(starting_from, max_idx)]</span>
<span id="cb6-17"><a href="#cb6-17"></a>    seqs <span class="op">=</span> []</span>
<span id="cb6-18"><a href="#cb6-18"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(starting_from, max_idx):</span>
<span id="cb6-19"><a href="#cb6-19"></a>        seqs <span class="op">+=</span> [[i, <span class="op">*</span>remainder] <span class="cf">for</span> remainder <span class="kw">in</span></span>
<span id="cb6-20"><a href="#cb6-20"></a>                    non_decreasing_multiindices(n<span class="op">-</span><span class="dv">1</span>, max_idx, starting_from<span class="op">=</span>i)]</span>
<span id="cb6-21"><a href="#cb6-21"></a>    <span class="cf">return</span> seqs</span>
<span id="cb6-22"><a href="#cb6-22"></a></span>
<span id="cb6-23"><a href="#cb6-23"></a><span class="co"># computes multinomial coefficient</span></span>
<span id="cb6-24"><a href="#cb6-24"></a><span class="kw">def</span> multinomial(lst):</span>
<span id="cb6-25"><a href="#cb6-25"></a>    res, i <span class="op">=</span> <span class="dv">1</span>, <span class="dv">1</span></span>
<span id="cb6-26"><a href="#cb6-26"></a>    <span class="cf">for</span> a <span class="kw">in</span> lst:</span>
<span id="cb6-27"><a href="#cb6-27"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, a <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb6-28"><a href="#cb6-28"></a>            res <span class="op">*=</span> i</span>
<span id="cb6-29"><a href="#cb6-29"></a>            res <span class="op">//=</span> j</span>
<span id="cb6-30"><a href="#cb6-30"></a>            i <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb6-31"><a href="#cb6-31"></a>    <span class="cf">return</span> res</span>
<span id="cb6-32"><a href="#cb6-32"></a></span>
<span id="cb6-33"><a href="#cb6-33"></a><span class="co"># given a multiindex, counts how many times each index appears</span></span>
<span id="cb6-34"><a href="#cb6-34"></a><span class="kw">def</span> count(midx, d):</span>
<span id="cb6-35"><a href="#cb6-35"></a>    c <span class="op">=</span> [<span class="dv">0</span>] <span class="op">*</span> d</span>
<span id="cb6-36"><a href="#cb6-36"></a>    <span class="cf">for</span> i <span class="kw">in</span> midx:</span>
<span id="cb6-37"><a href="#cb6-37"></a>      c[i] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb6-38"><a href="#cb6-38"></a>    <span class="cf">return</span> c</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="tabset-2-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-2-2-tab">
<div class="sourceCode" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a><span class="kw">def</span> even_power(q, k, p):</span>
<span id="cb7-2"><a href="#cb7-2"></a>  <span class="cf">return</span> np.inner(q, k) <span class="op">**</span> p</span>
<span id="cb7-3"><a href="#cb7-3"></a></span>
<span id="cb7-4"><a href="#cb7-4"></a><span class="kw">def</span> symmetric_power_inner_product(q, k, p):</span>
<span id="cb7-5"><a href="#cb7-5"></a>  embedded_q <span class="op">=</span> symmetric_power_embedding(q, p)</span>
<span id="cb7-6"><a href="#cb7-6"></a>  expanded_k <span class="op">=</span> symmetric_power_embedding(k, p)</span>
<span id="cb7-7"><a href="#cb7-7"></a>  <span class="cf">return</span> (embedded_q <span class="op">*</span> expanded_k).<span class="bu">sum</span>()</span>
<span id="cb7-8"><a href="#cb7-8"></a></span>
<span id="cb7-9"><a href="#cb7-9"></a>d <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb7-10"><a href="#cb7-10"></a><span class="cf">for</span> p <span class="kw">in</span> [<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">6</span>, <span class="dv">8</span>]:</span>
<span id="cb7-11"><a href="#cb7-11"></a>  q <span class="op">=</span> np.random.random(d)</span>
<span id="cb7-12"><a href="#cb7-12"></a>  k <span class="op">=</span> np.random.random(d)</span>
<span id="cb7-13"><a href="#cb7-13"></a>  <span class="cf">assert</span> np.allclose(</span>
<span id="cb7-14"><a href="#cb7-14"></a>    even_power(q, k, p), </span>
<span id="cb7-15"><a href="#cb7-15"></a>    symmetric_power_inner_product(q, k, p)</span>
<span id="cb7-16"><a href="#cb7-16"></a>  )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
<p>Using this embedding produces a massive dimensionality reduction compared to the dimensionality of <span class="math inline">\(\phi_\text{TP} ^p\)</span>. The table below compares the size of the state between repeated tensor products and symmetric powers, as measured in bytes (assuming half-precision), for a 124M-parameter GPT-2 transformer at various <span class="math inline">\(p\)</span>.</p>
<div style="width: 60%; margin: auto; text-align: center;">
<table class="caption-top table">
<thead>
<tr class="header">
<th>p</th>
<th>Tensor Power</th>
<th>Symmetric Power</th>
<th>Savings</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>2</td>
<td>77 MB</td>
<td>39 MB</td>
<td>49%</td>
</tr>
<tr class="even">
<td>4</td>
<td>314 GB</td>
<td>14 GB</td>
<td>96%</td>
</tr>
<tr class="odd">
<td>6</td>
<td>1.3 PB</td>
<td>2.2 TB</td>
<td>99.8%</td>
</tr>
<tr class="even">
<td>8</td>
<td>5.3 EB</td>
<td>199 TB</td>
<td>99.996%</td>
</tr>
</tbody>
</table>
</div>
<p>We can evaluate each symmetric power architecture against our two metrics, state size (under 80 GB) and performance (loss below baseline).</p>
<table class="caption-top table" style="width:90%; margin: auto; text-align:center; margin-bottom: 12px">
<thead>
<tr>
<th>
p
</th>
<th>
State Size
</th>
<th>
Memory ≤ 80 GB?
</th>
<th>
Relative Loss at 100K Steps
</th>
<th>
Loss ≤ baseline?
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
2
</td>
<td>
39 MB
</td>
<td>
✓
</td>
<td>
1.03x
</td>
<td>
✗
</td>
</tr>
<tr style="background: #d4ffe6">
<td>
4
</td>
<td>
14 GB
</td>
<td>
✓
</td>
<td>
0.98x
</td>
<td>
✓
</td>
</tr>
<tr>
<td>
6
</td>
<td>
2.2 TB
</td>
<td>
✗
</td>
<td>
0.97x
</td>
<td>
✓
</td>
</tr>
</tbody>
</table>
<p>The symmetric power transformer with <span class="math inline">\(p=4\)</span> passes our bar.</p>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">5. Conclusion</h2>
<p>In this article, we have introduced the <em>symmetric power transformer</em>, a linear transformer which closes the performance gap to classic softmax transformers using a tractably-small state. We replace the exponentiation in a traditional softmax transformer with an even power, and then show that this is equivalent to a linear transformer with the symmetric power embedding. We expect this approach will provide transformer-level performance at greatly reduced training costs when combined with the <a href="https://manifestai.com/articles/linear-transformers-are-faster/#chunked-formulation">chunked algorithm</a>. It will also enjoy cheaper inference, thanks to the constant-time inference costs common to all RNNs. In an upcoming article, we plan to release an open-source model that uses a symmetric power transformer at its core, together with an efficient CUDA kernel implementation. Stay tuned!</p>
<form action="https://buttondown.email/api/emails/embed-subscribe/manifestai" method="post" target="popupwindow" onsubmit="window.open('https://buttondown.email/manifestai', 'popupwindow')" class="embeddable-buttondown-form">
  <label for="bd-email" style="font-weight:bold;margin-right:20px;margin-top:20px;">
  Subscribe to be notified of new posts:
  </label>
  <input style="width: 35%;" type="email" name="email" id="bd-email" placeholder="Email">
  
  <input style="width: 80px;" type="submit" value="Subscribe">
</form>

</section>


<div id="quarto-appendix" class="default"><section id="acknowledgments" class="level3 appendix"><h2 class="anchored quarto-appendix-heading">Acknowledgments</h2><div class="quarto-appendix-contents">

<p>We would like to thank Warfa Jibril, Jono Ridgway, Saurabh Kumar, Justin Dieter, Fabrice Normandin, and Imanol Schlag for their feedback on an earlier draft of this post, and Txus Bach for correcting the state size calculations.</p>



</div></section><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body" data-entry-spacing="0" role="list">
<div id="ref-katharopoulos2020transformers" class="csl-entry" role="listitem">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret, <span>“Transformers are rnns: Fast autoregressive transformers with linear attention,”</span> in <em>International conference on machine learning</em>, PMLR, 2020, pp. 5156–5165.</div>
</div>
<div id="ref-buckman2024a" class="csl-entry" role="listitem">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">J. Buckman and C. Gelada, <span>“Linear <span>Transformers</span> <span>Are</span> <span>Faster</span>.”</span> Manifest AI, Jan. 05, 2024.</div>
</div>
<div id="ref-wang2020linformer" class="csl-entry" role="listitem">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">S. Wang, B. Z. Li, M. Khabsa, H. Fang, and H. Ma, <span>“Linformer: Self-attention with linear complexity,”</span> <em>arXiv preprint arXiv:2006.04768</em>, 2020.</div>
</div>
<div id="ref-schlag2021linear" class="csl-entry" role="listitem">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline">I. Schlag, K. Irie, and J. Schmidhuber, <span>“Linear transformers are secretly fast weight programmers,”</span> in <em>International conference on machine learning</em>, PMLR, 2021, pp. 9355–9366.</div>
</div>
<div id="ref-kacham2023polysketchformer" class="csl-entry" role="listitem">
<div class="csl-left-margin">[5] </div><div class="csl-right-inline">P. Kacham, V. Mirrokni, and P. Zhong, <span>“Polysketchformer: Fast transformers via sketches for polynomial kernels,”</span> <em>arXiv preprint arXiv:2310.01655</em>, 2023.</div>
</div>
<div id="ref-zhang2024hedgehog" class="csl-entry" role="listitem">
<div class="csl-left-margin">[6] </div><div class="csl-right-inline">M. Zhang, K. Bhatia, H. Kumbong, and C. Ré, <span>“The hedgehog &amp; the porcupine: Expressive linear attentions with softmax mimicry,”</span> <em>arXiv preprint arXiv:2402.04347</em>, 2024.</div>
</div>
<div id="ref-qin2024hgrn2" class="csl-entry" role="listitem">
<div class="csl-left-margin">[7] </div><div class="csl-right-inline">Z. Qin <em>et al.</em>, <span>“Hgrn2: Gated linear rnns with state expansion,”</span> <em>arXiv preprint arXiv:2404.07904</em>, 2024.</div>
</div>
<div id="ref-peng2021random" class="csl-entry" role="listitem">
<div class="csl-left-margin">[8] </div><div class="csl-right-inline">H. Peng, N. Pappas, D. Yogatama, R. Schwartz, N. A. Smith, and L. Kong, <span>“Random feature attention,”</span> <em>arXiv preprint arXiv:2103.02143</em>, 2021.</div>
</div>
<div id="ref-choromanski2020rethinking" class="csl-entry" role="listitem">
<div class="csl-left-margin">[9] </div><div class="csl-right-inline">K. Choromanski <em>et al.</em>, <span>“Rethinking attention with performers,”</span> <em>arXiv preprint arXiv:2009.14794</em>, 2020.</div>
</div>
<div id="ref-arora2024simple" class="csl-entry" role="listitem">
<div class="csl-left-margin">[10] </div><div class="csl-right-inline">S. Arora <em>et al.</em>, <span>“Simple linear attention language models balance the recall-throughput tradeoff,”</span> <em>arXiv preprint arXiv:2402.18668</em>, 2024.</div>
</div>
<div id="ref-su2024roformer" class="csl-entry" role="listitem">
<div class="csl-left-margin">[11] </div><div class="csl-right-inline">J. Su, M. Ahmed, Y. Lu, S. Pan, W. Bo, and Y. Liu, <span>“Roformer: Enhanced transformer with rotary position embedding,”</span> <em>Neurocomputing</em>, vol. 568, p. 127063, 2024.</div>
</div>
<div id="ref-dao2022flashattention" class="csl-entry" role="listitem">
<div class="csl-left-margin">[12] </div><div class="csl-right-inline">T. Dao, D. Fu, S. Ermon, A. Rudra, and C. Ré, <span>“Flashattention: Fast and memory-efficient exact attention with io-awareness,”</span> <em>Advances in Neural Information Processing Systems</em>, vol. 35, pp. 16344–16359, 2022.</div>
</div>
<div id="ref-buckman2024b" class="csl-entry" role="listitem">
<div class="csl-left-margin">[13] </div><div class="csl-right-inline">J. Buckman, <span>“LongCrawl64: <span>A</span> <span>Long-Context</span> <span>Natural-Language</span> <span>Dataset</span>.”</span> Manifest AI, May 16, 2024.</div>
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>This is also known as the <a href="https://en.wikipedia.org/wiki/Kernel_method">kernel method</a>. It is essential when one works with infinite-dimensional embeddings, and it’s also useful in this case to avoid materializing large-but-finite embeddings.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>This state-size threshold is admittedly somewhat arbitrary. In principle, larger states are possible with clever sharding; but for excessively large states, which must be sharded across a huge number of GPUs, the hardware cost of sharding becomes completely prohibitive. In this article, we are training models whose parameters fit on a single GPU, so it seems reasonable to use the memory of a single GPU as the threshold for tractability.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>A natural choice for the ordering <span class="math inline">\(\sigma\)</span> is <a href="https://en.wikipedia.org/wiki/Row-_and_column-major_order#:~:text=In%20row%2Dmajor%20order%2C%20the,column%20in%20column%2Dmajor%20order.">row major ordering</a>.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>You might have previously seen this expression in the <a href="https://en.wikipedia.org/wiki/Multinomial_theorem">multinomial theorem</a>. This connection is no coincidence. Symmetric powers are highly related to polynomails.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{buckman2024,
  author = {Buckman, Jacob and Gelada, Carles and Zhang, Sean},
  publisher = {Manifest AI},
  title = {Symmetric {Power} {Transformers}},
  date = {2024-08-15},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-buckman2024" class="csl-entry quarto-appendix-citeas" role="listitem">
<div class="">J.
Buckman, C. Gelada, and S. Zhang, <span>“Symmetric Power
Transformers.”</span> Manifest AI, Aug. 15, 2024.</div>
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="m-a-n-i-f-e-s-t/website" data-repo-id="R_kgDOLA6vSg" data-category="Announcements" data-category-id="DIC_kwDOLA6vSs4Cgv3x" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" data-loading="lazy" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>© 2024 Manifest AI</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>