<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.54">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Jacob Buckman">
<meta name="author" content="Carles Gelada">
<meta name="author" content="Sean Zhang">
<meta name="dcterms.date" content="2024-07-25">

<title>Symmetric Power Transformers – Manifest AI</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<link href="../../favicon.png" rel="icon" type="image/png">
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-V0D26E23Q3"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-V0D26E23Q3', { 'anonymize_ip': true});
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Symmetric Power Transformers – Manifest AI">
<meta property="og:description" content="">
<meta property="og:image" content="https://manifestai.com/favicon.png">
<meta property="og:site_name" content="Manifest AI">
<meta name="citation_title" content="Symmetric Power Transformers">
<meta name="citation_author" content="Jacob Buckman">
<meta name="citation_author" content="Carles Gelada">
<meta name="citation_author" content="Sean Zhang">
<meta name="citation_publication_date" content="2024-07-25">
<meta name="citation_cover_date" content="2024-07-25">
<meta name="citation_year" content="2024">
<meta name="citation_online_date" content="2024-07-25">
<meta name="citation_language" content="en">
<meta name="citation_publisher" content="Manifest AI">
<meta name="citation_reference" content="citation_title=Polysketchformer: Fast transformers via sketches for polynomial kernels;,citation_author=Praneeth Kacham;,citation_author=Vahab Mirrokni;,citation_author=Peilin Zhong;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_journal_title=arXiv preprint arXiv:2310.01655;">
<meta name="citation_reference" content="citation_title=The hedgehog &amp;amp;amp; the porcupine: Expressive linear attentions with softmax mimicry;,citation_author=Michael Zhang;,citation_author=Kush Bhatia;,citation_author=Hermann Kumbong;,citation_author=Christopher Ré;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_journal_title=arXiv preprint arXiv:2402.04347;">
<meta name="citation_reference" content="citation_title=Linear transformers are secretly fast weight programmers;,citation_author=Imanol Schlag;,citation_author=Kazuki Irie;,citation_author=Jürgen Schmidhuber;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_conference_title=International conference on machine learning;,citation_conference=PMLR;">
<meta name="citation_reference" content="citation_title=Linformer: Self-attention with linear complexity;,citation_author=Sinong Wang;,citation_author=Belinda Z Li;,citation_author=Madian Khabsa;,citation_author=Han Fang;,citation_author=Hao Ma;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_journal_title=arXiv preprint arXiv:2006.04768;">
<meta name="citation_reference" content="citation_title=Transformers are rnns: Fast autoregressive transformers with linear attention;,citation_author=Angelos Katharopoulos;,citation_author=Apoorv Vyas;,citation_author=Nikolaos Pappas;,citation_author=François Fleuret;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_conference_title=International conference on machine learning;,citation_conference=PMLR;">
</head>

<body class="floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../m-logo-tight.png" alt="" class="navbar-logo">
    </a>
  </div>
          <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/manifest__ai"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
   
  <ul>
  <li><a href="#linear-transformers-with-embeddings" id="toc-linear-transformers-with-embeddings" class="nav-link active" data-scroll-target="#linear-transformers-with-embeddings">1. Linear Transformers with Embeddings</a></li>
  <li><a href="#tensor-product-transformers" id="toc-tensor-product-transformers" class="nav-link" data-scroll-target="#tensor-product-transformers">2. Tensor Product Transformers</a>
  <ul class="collapse">
  <li><a href="#mathematical-background" id="toc-mathematical-background" class="nav-link" data-scroll-target="#mathematical-background">2.1. Mathematical Background</a></li>
  <li><a href="#transformer-architectures" id="toc-transformer-architectures" class="nav-link" data-scroll-target="#transformer-architectures">2.2. Transformer Architectures</a></li>
  </ul></li>
  <li><a href="#symmetric-power-transformers" id="toc-symmetric-power-transformers" class="nav-link" data-scroll-target="#symmetric-power-transformers">3. Symmetric Power Transformers</a>
  <ul class="collapse">
  <li><a href="#mathematical-background-1" id="toc-mathematical-background-1" class="nav-link" data-scroll-target="#mathematical-background-1">3.1. Mathematical Background</a></li>
  <li><a href="#implementation" id="toc-implementation" class="nav-link" data-scroll-target="#implementation">3.2 Implementation</a></li>
  </ul></li>
  <li><a href="#conclusion-upcoming-work" id="toc-conclusion-upcoming-work" class="nav-link" data-scroll-target="#conclusion-upcoming-work">Conclusion &amp; Upcoming Work</a>
  <ul class="collapse">
  
  </ul></li>
  
  
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Symmetric Power Transformers</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Authors</div>
    <div class="quarto-title-meta-contents">
             <p>Jacob Buckman </p>
             <p>Carles Gelada </p>
             <p>Sean Zhang </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">July 25, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p><span class="math display">\[
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\sft}{\text{softmax}}
\newcommand{\List}{\text{List}}
\newcommand{\Seq}{\text{Seq}}
\newcommand{\SeqT}{\text{SeqT}}
\newcommand{\CSeqT}{\text{CSeqT}}
\newcommand{\Dist}{\text{Dist}}
\newcommand{\SM}{\text{SM}}
\newcommand{\Fn}{\text{Fn}}
\newcommand{\Tok}{\text{Tok}}
\newcommand{\Aij}{ A_{[i,j]}}
\newcommand{\ten}{\small\text{tensor}}
\newcommand{\sym}{\small\text{symmetric}}
\newcommand{\flat}[1]{\text{flat}\left(#1\right)}
\newcommand{\stab}{\text{stab}}
\]</span></p>
<style>
summary {
  cursor: pointer;
  padding: 8px;
  background-color: #f0f0f0; /* Light grey background */
  border-radius: 8px; /* Rounded corners for consistency */
}

/* Hover state changes the background color to a lighter grey */
summary:hover {
  background-color: #e0e0e0; /* Lighter grey on hover */
}

/* Override hover effect when details is open */
details[open] summary:hover {
  background-color: #f0f0f0; /* Maintain the same color as the non-hover state */
}

details[open] summary {
  padding: 8px;
}

details {
  padding: 0; /* No padding when details is not open */
  border-radius: 8px; /* Rounded corners for consistency */
  background-color: #f0f0f0; /* Light grey background */
}

details[open] {
  padding: 8px; /* Padding inside the details for content alignment */
}
</style>
<p>Linear transformers <span class="citation" data-cites="katharopoulos2020transformers"><a href="#ref-katharopoulos2020transformers" role="doc-biblioref">[1]</a></span> can be formulated as linear-cost RNNs, and so have better theoretical FLOP scaling than ordinary transformers. <a href="https://manifestai.com/articles/linear-transformers-are-faster/">In our previous article</a>, we presented an efficient chunked algorithm that turns this theoretical advantage into practical speedups when the context is long: 10x faster training for a 64k-token context. Unfortunately, we also found that vanilla linear transformers suffer from <a href="https://manifestai.com/articles/linear-transformers-are-faster/#learning-performance">degraded performance</a>, especially at long contexts, rendering any benefits from the speedup useless. This article advances our previous discussion by introducing a linear transformer variant that solves the degraded performance issue while still enabling an efficient linear-cost implementation.</p>
<p>Behind all the algorithms explored in this post there is a central idea. That for RNNs, thinking of the size of the model just in terms of the number of parameters misses something important. An RNN encodes all the information from the past inputs <span class="math inline">\(X_1,...,X_{t}\)</span> into a finite-dimensional vector <span class="math inline">\(S_t\)</span> called the state. If the states are too small, the model will struggle to store all the information it will later require. Could this be the cause of the poor performance of the GPT-2-style linear transformers we evaluated in our previous article? If we look at the state sizes, we notice they are many orders of magnitude smaller than the weights of the architecture:</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>A generic RNN equation would compute next states via <span class="math inline">\(S_t = g(S_{t-1}, X_t)\)</span> and produce outputs via <span class="math inline">\(Y_t = f(S_t, X_t)\)</span>.</p>
</div></div><div style="width: 65%; margin: auto; text-align: center;">
<table class="caption-top table">
<thead>
<tr class="header">
<th></th>
<th>Weights</th>
<th>State Size</th>
<th>State-Weight Ratio</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Small</strong></td>
<td>124M</td>
<td>589K</td>
<td>0.0048</td>
</tr>
<tr class="even">
<td><strong>Medium</strong></td>
<td>335M</td>
<td>1.5M</td>
<td>0.0045</td>
</tr>
<tr class="odd">
<td><strong>Large</strong></td>
<td>774M</td>
<td>2.9M</td>
<td>0.0037</td>
</tr>
<tr class="even">
<td><strong>XL</strong></td>
<td>1.6B</td>
<td>4.9M</td>
<td>0.0031</td>
</tr>
</tbody>
</table>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<p>The formula for the state size of a vanilla linear transformer is <code>layer_n * head_count * key_size * value_size</code>.</p>
</div></div><p>Fortunately, since the architecture is a linear transformer, this imbalance has a straightforward remedy. The size of the state of a linear transformer can be controlled by simply embedding the keys and queries in a higher-dimensional space. (The larger the space in which we embed the key, the larger the state becomes). Previous work <span class="citation" data-cites="wang2020linformer schlag2021linear kacham2023polysketchformer zhang2024hedgehog"><a href="#ref-wang2020linformer" role="doc-biblioref">[2]</a>–<a href="#ref-zhang2024hedgehog" role="doc-biblioref">[5]</a></span> has already observed that this improves the performance of linear transformers, but the resulting architectures are still not competitive with standard transformers.</p>
<p>Of crucial importance is the choice of embedding function. In this article we focus on embeddings based on the <em>tensor product</em> and also the closely related <em>symmetric power</em>. In each case, we have a hyper-parameter <span class="math inline">\(p\)</span> that controls the embedding dimension without affecting the weights of the model, allowing us to select the optimal state-weight ratio.</p>
<p>Our central takeaway is that one family of architectures, <em>symmetric power transformers</em>, is a promising candidate for long-context modeling. For <span class="math inline">\(p=4\)</span> and above, it outperforms the transformer baseline, and at <span class="math inline">\(p=4\)</span> and below, it has a state size small enough to fit on a modern GPU.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>All experiments in this article used the LongCrawl64 dataset, the 124M GPT-2 architecture, a context length of 4096, a batch size of 524288 tokens, and were run on a node of 8 H100s.</p>
</div></div><div class="column-body" style="text-align: center;">
<iframe width="600" height="350" src="plots/01_symmetric_power.html">
</iframe>
</div>
<p>Note that in this article we run all the experiments with the quadratic-cost attention formulation, and so do not benefit from any speedup compared to the transformer baseline. This is because the goal of the research presented in this article is only to identify a promising candidate architecture. Implementing an efficient, linear-cost algorithm requires significant engineering effort, and so it is important to first be confident that the architecture will learn well. In a subsequent article, we will show how to combine the symmetric power embedding function with the efficient chunked algorithm.</p>
<section id="linear-transformers-with-embeddings" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="linear-transformers-with-embeddings">1. Linear Transformers with Embeddings</h2>
<p>We begin with a review of linear transformers. The inputs to a transformer layer are sequences of <span class="math inline">\(Q_i, K_i, V_i \in \R^d\)</span> of queries, keys, and values, where <span class="math inline">\(i\)</span> ranges from <span class="math inline">\(1\)</span> to the sequence length <span class="math inline">\(t\)</span>. The outputs are a sequence <span class="math inline">\(Y_i\in \R^d\)</span>. The formula for the output vectors is: <span class="math display">\[
Y_i = \sum_{j=1}^i A_{ij} V_j \qquad A_{ij}  = \frac{ \phi(Q_i)^T \phi(K_j)}{\sum_{k=1}^i \phi(Q_i)^T \phi(K_k) }
\]</span> where <span class="math inline">\(\phi : \R^d \to \R^D\)</span> is an embedding function that maps keys or queries into vectors of dimension <span class="math inline">\(D\)</span>. This formulation is what we call the <em>attention formulation</em> of a linear transformer, because it involves computing the the inner product between embedded keys and queries, akin to the self-attention operation in a standard transformer.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>This linear transformer is the same architecture as in <a href="ttps://manifestai.com/articles/linear-transformers-are-faster/">our previous article</a>, but here we present the complete formula in its full generality, including both the normalizing term and embedding function (previously suppressed for clarity).</p>
</div></div><p>The exact same outputs can be computed via a <em>recurrent formulation</em>: <span class="math display">\[
Y_{i} = \frac{S_i \phi(Q_i)}{Z_i \phi(Q_i)} \qquad Z_i = Z_{i-1} + \phi(K_i)^T \qquad S_i = S_{i-1} + V_i \phi(K_i)^T
\]</span> where <span class="math inline">\(Z_0\)</span> and <span class="math inline">\(S_0\)</span> are <span class="math inline">\(\mathcal 0\)</span> vectors in their respective spaces. Since <span class="math inline">\(S_i \in \R^{d \times D}\)</span> and <span class="math inline">\(Z_i \in \R^{D}\)</span>, the size of the state is <span class="math inline">\(D(d+1)\)</span>.</p>

<div class="no-row-height column-margin column-container"><div class="">
<div style="margin-top: 100px;">
<p>Note that <span class="math inline">\(D(d+1)\)</span> gives the size of the state for any one linear transformer head. To compute the state size for an entire multi-head architecture, one must multiply by the number of layers and number of heads per layer.</p>
</div>
</div></div><details>
<summary>
Expand for a derivation of the recurrent formulation.
</summary>
If we expand the recurrent the definitions of <span class="math inline">\(S_i\)</span> and <span class="math inline">\(Z_i\)</span> we get that <span class="math display">\[
Z_i = \sum_{j=1}^i \phi(K_j)^T \qquad S_i = \sum_{j=1}^i V_j \phi(K_j)^T
\]</span> Then, starting with the attention formulation <span class="math display">\[
\begin{aligned}
Y_i &amp;= \sum_{j=1}^i \frac{ \phi(Q_i)^T \phi(K_j)}{\sum_{k=1}^i \phi(Q_i)^T \phi(K_k) } V_j \\
    &amp;= \sum_{j=1}^i V_j \frac{ \phi(K_j)^T \phi(Q_i)}{\sum_{k=1}^i \phi(K_k)^T \phi(Q_i) } \\
    &amp;= \frac{ \left( \sum_{j=1}^i V_j  \phi(K_j)^T \right) \phi(Q_i)}{ \left(\sum_{k=1}^i \phi(K_k)^T \right) \phi(Q_i) } \\
    &amp;= \frac{S_i\phi(Q_i)}{Z_i\phi(Q_i)} \\
\end{aligned}
\]</span>
</details>
<p>How should we choose <span class="math inline">\(\phi\)</span>? Here are some attributes that we want:</p>
<ul>
<li><strong>Adjustable dimensionality.</strong> To balance the size of the state with the size of the weights, there should be some hyperparameter controlling the dimension <span class="math inline">\(D\)</span>.</li>
<li><strong>Efficient dot product.</strong> In the attention algorithm, <span class="math inline">\(\phi(Q_i)\)</span> and <span class="math inline">\(\phi(K_j)\)</span> appear only as intermediate steps in the computation of <span class="math inline">\(\phi(Q_i)^T\phi(K_j)\)</span>. For some choices of <span class="math inline">\(\phi\)</span>, there is a more efficient formula for <span class="math inline">\(\phi(Q_i)^T\phi(K_j)\)</span> that does not require computing these intermediate objects.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></li>
<li><strong>Positive dot product.</strong> We want <span class="math inline">\(\phi(Q_i)^T \phi(K_j)\)</span> to always be positive. This ensures that each normalized output <span class="math inline">\(Y_i\)</span> is a convex combination of all preceding values <span class="math inline">\(V_1, \cdots, V_i\)</span>. We found this to be essential for stable and performant learning.</li>
</ul>
<p>We are searching for the embedding function with the best empirical performance at each state size, subject to these three constraints. Since our ultimate goal is to replace transformers (trained with attention) with linear transformers (trained with the chunked algorithm), the bar for success is simple: a <span class="math inline">\(\phi\)</span> whose performance matches that of a transformer baseline at a state size small enough to be tractable. Concretely, let’s use 80 GB, which is the memory capacity of A100 and H100 GPUs.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<p>Many possible embedding functions have already been investigated in the literature <span class="citation" data-cites="wang2020linformer schlag2021linear kacham2023polysketchformer zhang2024hedgehog"><a href="#ref-wang2020linformer" role="doc-biblioref">[2]</a>–<a href="#ref-zhang2024hedgehog" role="doc-biblioref">[5]</a></span>. In this article, we will explore embeddings that use the <em>tensor product</em> and <em>symmetric product</em> to turn small vectors into large ones.</p>
</section>
<section id="tensor-product-transformers" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="tensor-product-transformers">2. Tensor Product Transformers</h2>
<p>In this section, we explore architectures based on the <a href="https://en.wikipedia.org/wiki/Tensor_product">tensor product</a>, a deep and ubiquitous mathematical idea. We begin with a brief introduction to the core mathematics, and then describe two ways to use the tensor products to embed vectors. (A third approach is discussed in Appendix A.)</p>
<section id="mathematical-background" class="level3">
<h3 class="anchored" data-anchor-id="mathematical-background">2.1. Mathematical Background</h3>
<p>The tensor product of vectors generalizes the outer product and formalizes the concepts of multi-dimensional arrays. Given two vectors <span class="math inline">\(v\in \R^{d_1}\)</span> and <span class="math inline">\(w\in \R^{d_2}\)</span> one can think of their tensor product <span class="math inline">\(v\otimes w\)</span> as the matrix <span class="math inline">\(v w^T \in \R^{d_1\times d_2}\)</span>, <span class="math display">\[
v w^T = \left[
\begin{array}{cccc}
v_1w_1 &amp; v_1w_2 &amp; \cdots &amp; v_1w_m \\
v_2w_1 &amp; v_2w_2 &amp; \cdots &amp; v_2w_m \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
v_nw_1 &amp; v_nw_2 &amp; \cdots &amp; v_nw_m \\
\end{array}
\right]
\]</span> Intuitively, <span class="math inline">\(v \otimes w \otimes u\)</span> would be a 3-dimensional table containing all possible entries of the sort <span class="math inline">\(v_i
w_j u_k\)</span>. But let’s make the intuition of multi-dimensional tables more rigorous.</p>
<p><strong>Multi-indices.</strong> A multi-index <span class="math inline">\(\alpha\)</span> specifies a location in a <span class="math inline">\(p\)</span> dimensional table with dimension sizes <span class="math inline">\(d_1,
\cdots, d_p \in \N\)</span>. Let <span class="math inline">\(\N_d\)</span> denote the set <span class="math inline">\(\{1,2,\cdots,d\}\)</span>. Then, the space of multi-indices is <span class="math inline">\(N_{d_1} \times \cdots \times \N_{d_p}\)</span> and we refer to a generic multi-index as <span class="math inline">\(\alpha = [\alpha_1, \cdots, \alpha_p] \in \N_{d_1} \times \cdots \times \N_{d_p}\)</span>.</p>
<p><strong>Tensors.</strong> A tensor <span class="math inline">\(T\in \R^{d_1 \times \cdots d_p}\)</span> corresponds to a high dimensional table where every location has a numerical value assigned to it. In other words, it is a map from multi-indices to the reals. <span class="math display">\[
T: \N_{d_1} \times \cdots \times \N_{d_p} \to \R
\]</span> By convention, we index tensors with subscript notation <span class="math inline">\(T_\alpha\)</span> instead of functional notation <span class="math inline">\(T(\alpha)\)</span>, but the meaning is the same.</p>
<p><strong>Tensor product of vectors.</strong> Given a list of <span class="math inline">\(p\)</span> vectors <span class="math inline">\(v_i \in \R^{n_i}\)</span>, we denote by <span class="math inline">\(v_1 \otimes \cdots
\otimes v_p\)</span> (or alternatively <span class="math inline">\(\bigotimes_{i=1}^p v_i\)</span>) as the tensor in <span class="math inline">\(\R^{n_1 \times \cdots \times n_p}\)</span> with entries given by the following formula: <span class="math display">\[
\left[\bigotimes_{i=1}^p v_i\right]_\alpha = \prod_{i=1}^p v_{i, \alpha_i}
\]</span> Where <span class="math inline">\(v_{i,j}\in \R\)</span> denotes the <span class="math inline">\(j\)</span>th entry of the <span class="math inline">\(i\)</span>th vector.</p>
<p><strong>Flattening.</strong> To build embedding functions <span class="math inline">\(\phi\)</span>, we are going to use the tensor product to embed lists of vectors into <span class="math inline">\(\R^{d_1\times \cdots d_p}\)</span> tensors. But once we’ve done that, we will no longer care about the tensor structure and we will prefer to think of them as vectors in <span class="math inline">\(\R^D\)</span>, where <span class="math inline">\(D=\prod_{i=1}^p d_i\)</span>. The map <span class="math inline">\(\text{flat}: \R^{d_1\times \cdots d_p} \to \R^D\)</span> implements this transformation by writing every entry of the array into a flat vector. This can be done with any bijective function <span class="math inline">\(\sigma: D \to \N_{d_1} \times \cdots \times \N_{d_p}\)</span> which effectively imposes an (arbitrary) ordering on the multi-indices.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> The flattening is defined as: <span class="math display">\[
\flat{T}_i = T_{\sigma(i)}
\]</span> The dot product of flattened tensors satisfies the following property: <span class="math display">\[
\flat{\bigotimes_{i=1}^p v_i}^T \flat{\bigotimes_{i=1}^p w_i} = \prod_{i=1}^p v_i^T w_i \qquad \text{(Eq 1)}
\]</span></p>
<details>
<summary>
Expand to see a proof.
</summary>
<span class="math display">\[\begin{align}
\flat{\bigotimes_{i=1}^p v_i}^T \flat{\bigotimes_{i=1}^p w_i}
&amp;= \sum_{l=1}^D \left[ \bigotimes_{i=1}^p v_i \right]_{\sigma(l)} \left [ \bigotimes_{i=1}^p w_i \right]_{\sigma(l)}  \\
&amp;= \sum_{l=1}^D \prod_{i=1}^p v_{i, \sigma(l)_i} w_{i, \sigma(l)_i}
\end{align}\]</span> On the other hand, if we apply the distributive property to <span class="math inline">\(\prod_{i=1}^p \sum_{j=1}^{d_i}  v_{i, j} w_{i, j}\)</span> we have to sum over all possible combinations that the second index <span class="math inline">\(j\)</span> can take, so that: <span class="math display">\[\begin{align}
\prod_{i=1}^p v_i^T w_i
&amp;= \prod_{i=1}^p \sum_{j=1}^{d_i}  v_{i, j} w_{i, j}
= \sum_{l=1}^D \prod_{i=1}^p v_{i, \sigma(l)_i} w_{i, \sigma(l)_i} \\
&amp;= \flat{\bigotimes_{i=1}^p v_i}^T \flat{\bigotimes_{i=1}^p w_i}
\end{align}\]</span>
</details>
</section>
<section id="transformer-architectures" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="transformer-architectures">2.2. Transformer Architectures</h3>
<p>Armed with the tensor product, we are ready to define an embedding, and in doing so define an architecture. Thanks to Equation 1, it is easy to ensure that embeddings based on flattened tensor products will have efficient dot products. Taking the two other criteria – adjustable state size and positive dot products – into account, there are a few natural choices of embedding to consider.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>In this section, we focus on the effect of <span class="math inline">\(\phi\)</span> on keys <span class="math inline">\(k\)</span>, but wlog all discussion applies equally to queries.</p>
</div></div><section id="softplus-repeated-tensor-product" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="softplus-repeated-tensor-product">2.2.1. Softplus repeated tensor product</h4>
<p>The <em>softplus-repeat</em> embedding uses an initial softplus to ensure positivity, and then repeatedly takes the tensor product of a <span class="math inline">\(d\)</span>-sized key or query with itself <span class="math inline">\(p\)</span> times, yielding a tensor of size <span class="math inline">\(\mathbb{R}^
{d^p}\)</span>. Formally, we define: <span class="math display">\[
\phi^p_{\text{softplus-repeat}}(k) =
\text{flat}\left(\bigotimes_{i=1}^p \text{softplus}(k)\right)
\]</span> A JAX implementation of softplus-repeat attention is simple. Note that in line 4, we are invoking Equation 1 to turn the dot product of flattened <span class="math inline">\(p\)</span>-dimensional tensors into a product of <span class="math inline">\(p\)</span> vector dot products.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="kw">def</span> softplus_repeat_tensor_product_attn(K, Q, V, p):</span>
<span id="cb1-2"><a href="#cb1-2"></a>    Q, K <span class="op">=</span> softplus(Q), softplus(K)</span>
<span id="cb1-3"><a href="#cb1-3"></a>    C <span class="op">=</span> Q <span class="op">@</span> K.T</span>
<span id="cb1-4"><a href="#cb1-4"></a>    B <span class="op">=</span> C<span class="op">**</span>p</span>
<span id="cb1-5"><a href="#cb1-5"></a>    B <span class="op">=</span> where(mask, B, <span class="dv">0</span>)</span>
<span id="cb1-6"><a href="#cb1-6"></a>    A <span class="op">=</span> B <span class="op">/</span> B.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-7"><a href="#cb1-7"></a>    Y <span class="op">=</span> A <span class="op">@</span> V</span>
<span id="cb1-8"><a href="#cb1-8"></a>    <span class="cf">return</span> Y</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This implementation is more pedagogical than practical, as it is not numerically stable. It is common for numerical issues to emerge when training at scale, and especially when training in mixed precision. In Appendix B, we provide an improved implementation that addresses these issues.</p>
<p>Running experiments, we see:</p>
<div class="column-body" style="text-align: center;">
<iframe width="600" height="400" src="plots/03_softplus_repeat.html">
</iframe>
</div>
<p>Performance improves meaningfully with increased <span class="math inline">\(p\)</span>, starting to approach the baseline. This supports our hypothesis that state size is a major cause of the gap between the linear transformer and the softmax transformer. However, we have still not succeeded at our goal of closing the gap completely, and have already far surpassed our memory quota, as the below table shows.</p>
<div style="width: 90%; margin: auto; text-align: center;">
<table class="caption-top table">
<colgroup>
<col style="width: 3%">
<col style="width: 14%">
<col style="width: 20%">
<col style="width: 34%">
<col style="width: 26%">
</colgroup>
<thead>
<tr class="header">
<th>p</th>
<th>State Size</th>
<th>Memory ≤ 80 GB?</th>
<th>Relative Loss at 100K Steps</th>
<th>Performs ≥ baseline?</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>589 KB</td>
<td>✔</td>
<td>1.23x</td>
<td>✘</td>
</tr>
<tr class="even">
<td>2</td>
<td>18 MB</td>
<td>✔</td>
<td>1.16x</td>
<td>✘</td>
</tr>
<tr class="odd">
<td>4</td>
<td>309 GB</td>
<td>✘</td>
<td>1.11x</td>
<td>✘</td>
</tr>
<tr class="even">
<td>6</td>
<td>1.2 TB</td>
<td>✘</td>
<td>1.05x</td>
<td>✘</td>
</tr>
<tr class="odd">
<td>8</td>
<td>5.2 EB</td>
<td>✘</td>
<td>-</td>
<td>-</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="repeated-tensor-product" class="level4 page-columns page-full">
<h4 class="anchored" data-anchor-id="repeated-tensor-product">2.2.2. Repeated tensor product</h4>
<p>The <em>repeat</em> embedding is identical to the softplus-repeat embedding, but with no softplus. Since the softplus was used to guarantee positivity, its removal would seem to pose a problem; but one can easily check that, if <span class="math inline">\(p\)</span> is even, positivity of the inner products is guaranteed even without softmax. As a result, this embedding can be used only with even <span class="math inline">\(p\)</span>.</p>
<p><span class="math display">\[
\phi^p_{\text{repeat}}(k) =
\text{flat}\left(\bigotimes_{i=1}^p k\right) \in \mathbb{R}^{d^p}
\]</span></p>
<p>The implementation is familiar, as it’s the same as the implementation in Section 2.1 but with the softplus removed:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="kw">def</span> repeat_tensor_product_attn(K, Q, V, p):</span>
<span id="cb2-2"><a href="#cb2-2"></a>    C <span class="op">=</span> Q <span class="op">@</span> K.T</span>
<span id="cb2-3"><a href="#cb2-3"></a>    B <span class="op">=</span> C<span class="op">**</span>p</span>
<span id="cb2-4"><a href="#cb2-4"></a>    B <span class="op">=</span> where(mask, B, <span class="dv">0</span>)</span>
<span id="cb2-5"><a href="#cb2-5"></a>    A <span class="op">=</span> B <span class="op">/</span> B.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-6"><a href="#cb2-6"></a>    Y <span class="op">=</span> A <span class="op">@</span> V</span>
<span id="cb2-7"><a href="#cb2-7"></a>    <span class="cf">return</span> Y</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>See Appendix B for a numerically-stable implementation. Running experiments, we see an exciting result:</p>
<div class="column-body" style="text-align: center;">
<iframe width="600" height="350" src="plots/04_repeat.html">
</iframe>
</div>
<p>This embedding gives performance which matches or even surpasses that of the baseline.</p>
<p>Contrasting this result with the curves in Section 3.1 yields an important insight: softplus massively degrades performance. Most existing literature on linear transformers has used softplus (or similar transformations) to uphold the positivity constraints, see for example <span class="citation" data-cites="katharopoulos2020transformers"><a href="#ref-katharopoulos2020transformers" role="doc-biblioref">[1]</a></span>. One possible explanation for this degradation is that projecting all keys and queries to the positive quadrant (an exponentially small corner of the overall space) decreases the expressivity of the model. Whatever the reason, a side-by-side comparison of <span class="math inline">\(\phi^p_{\text{repeat}}\)</span> to <span class="math inline">\(\phi^p_{\text{softplus-repeat}}\)</span> makes it clear that this effect is large across all <span class="math inline">\(p\)</span>.</p>
<div class="column-body" style="text-align: center;">
<iframe width="600" height="400" src="plots/05_softplus_ablation.html">
</iframe>
</div>
<p>Returning now to our main objective: have we found an embedding function whose performance is competitive with that of a strong transformer baseline, while, at the same time, having a state size small enough to fit on a GPU?</p>
<p>Unfortunately, not yet. The table below shows the size of a single state, as measured in bytes (assuming fp16/bf16 precision), for a 124M-parameter GPT-2 repeated-tensor-product transformer at various <span class="math inline">\(p\)</span>.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>The formula for the state size of a linear transformer with the repeated product embedding is <code>layer_n * head_count *  key_size**p * value_size</code>.</p>
</div></div><div style="width: 90%; margin: auto; text-align: center;">
<table class="caption-top table">
<colgroup>
<col style="width: 3%">
<col style="width: 14%">
<col style="width: 20%">
<col style="width: 34%">
<col style="width: 26%">
</colgroup>
<thead>
<tr class="header">
<th>p</th>
<th>State Size</th>
<th>Memory ≤ 80 GB?</th>
<th>Relative Loss at 100K Steps</th>
<th>Performs ≥ baseline?</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>589 KB</td>
<td>✔</td>
<td>1.23x</td>
<td>✘</td>
</tr>
<tr class="even">
<td>2</td>
<td>18 MB</td>
<td>✔</td>
<td>1.03x</td>
<td>✘</td>
</tr>
<tr class="odd">
<td>4</td>
<td>309 GB</td>
<td>✘</td>
<td>.98x</td>
<td>✔</td>
</tr>
<tr class="even">
<td>6</td>
<td>1.2 TB</td>
<td>✘</td>
<td>.97x</td>
<td>✔</td>
</tr>
<tr class="odd">
<td>8</td>
<td>5.2 EB</td>
<td>✘</td>
<td>-</td>
<td>-</td>
</tr>
</tbody>
</table>
</div>
<p>The settings of <span class="math inline">\(p\)</span> that give sufficient performance have states that are far too large, so we still have not found an embedding with the properties that we desire. But we are close.</p>
</section>
</section>
</section>
<section id="symmetric-power-transformers" class="level2">
<h2 class="anchored" data-anchor-id="symmetric-power-transformers">3. Symmetric Power Transformers</h2>
<p>In the next section, we explore a final embedding function, one with mathematically equivalent behavior but much smaller state sizes. Once again, we will begin with an introduction to the relevant mathematical ideas, and then move on to describe a concrete architecture.</p>
<section id="mathematical-background-1" class="level3">
<h3 class="anchored" data-anchor-id="mathematical-background-1">3.1. Mathematical Background</h3>
<p>Observe that the embedding <span class="math inline">\(\phi^2_{\text{repeated}}(v)= \flat {v v^T}\)</span> is somewhat wasteful. The matrix <span class="math inline">\(v v^T\)</span> is symmetric, and so it contains duplicated entries.</p>
<p><span class="math display">\[
v v^T = \left[
\begin{array}{cccc}
v_1v_1 &amp; v_1v_2 &amp; \cdots &amp; v_1v_m \\
v_2v_1 &amp; v_2v_2 &amp; \cdots &amp; v_2v_m \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
v_nv_1 &amp; v_nv_2 &amp; \cdots &amp; v_nv_m \\
\end{array}
\right]
\]</span></p>
<p>Entries at indices <span class="math inline">\((i,i)\)</span> appear a single time, but due to the commutativity of scalar multiplication (i.e.&nbsp;<span class="math inline">\(v_i v_j =
v_j v_i\)</span>), the entries at indices <span class="math inline">\((i,j)\)</span> each appear twice (if <span class="math inline">\(i\neq j\)</span>). Noticing this symmetry allows us to create an alternative embedding, <span class="math inline">\(\phi^2_\text{sym}: \R^d \to \R^{\frac{d^2 +d} 2}\)</span>, which can be implemented as:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="kw">def</span> sym_emb_2(v):</span>
<span id="cb3-2"><a href="#cb3-2"></a>  x, d <span class="op">=</span> [], v.size</span>
<span id="cb3-3"><a href="#cb3-3"></a>  <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(d):</span>
<span id="cb3-4"><a href="#cb3-4"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(i, d):</span>
<span id="cb3-5"><a href="#cb3-5"></a>      count <span class="op">=</span> <span class="dv">1</span> <span class="cf">if</span> i<span class="op">==</span>j <span class="cf">else</span> <span class="dv">2</span></span>
<span id="cb3-6"><a href="#cb3-6"></a>      x.append(sqrt(count) <span class="op">*</span> v[i] <span class="op">*</span> v[j])</span>
<span id="cb3-7"><a href="#cb3-7"></a>  <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This construction <span class="math inline">\(\phi^2_\text{sym}\)</span> guarantees that <span class="math inline">\(\phi^2_\text{sym}(v)^T \phi^2_\text{sym}(w) = \phi^2_\text
{repeat} (v)^T \phi^2_\text{repeat}(w)\)</span>. Recall that in the attention formulation of the linear transformer the embedding <span class="math inline">\(\phi\)</span> only influences the outputs via the attention scores, which were defined as <span class="math display">\[
A_{ij}  = \frac{ \phi(Q_i)^T \phi(K_j)}{\sum_{k=1}^i \phi(Q_i)^T \phi(K_k) }
\]</span> Then two linear transformers with embeddings <span class="math inline">\(\phi^2_\text{repeat}(v)\)</span> and <span class="math inline">\(\phi^2_\text{sym}(v)\)</span> will have exactly the same outputs, since they have the same inner products <span class="math inline">\(\phi(Q_i)^T \phi(K_j)\)</span>. We’ve been able to exploit the symmetry of <span class="math inline">\(v v^T\)</span> to construct an equivalent embedding function with approximately half the dimensionality! Next, we will generalize this idea to higher powers.</p>
<p><strong>Permutation group.</strong> The first thing we need is the <a href="https://en.wikipedia.org/wiki/Permutation">permutation group</a> of <span class="math inline">\(p\)</span> elements, which is defined as the set of all functions <span class="math inline">\(\rho: \N_p \to \N_p\)</span> that are invertible and denoted by <span class="math inline">\(G_p\)</span>. We also overload the notation slightly. For a multi-index <span class="math inline">\(\alpha = [\alpha_1, \cdots, \alpha_p]\)</span> define the permutation of the multi-index as <span class="math inline">\(\rho(\alpha) = [\alpha_{\rho(1)}, \cdots, \alpha_{\rho(p)}]\)</span>. This is useful to define symmetric tensors.</p>
<p><strong>Symmetric tensors.</strong> A tensor <span class="math inline">\(T \in \R^{\underbrace{d\times \cdots \times d}_p}\)</span> is symmetric if for all multi-indices <span class="math inline">\(\alpha \in \N_d \times \cdots \times \N_d\)</span> and permutations <span class="math inline">\(\rho \in G_p\)</span> we have that: <span class="math display">\[
T_\alpha = T_{\rho(\alpha)}
\]</span></p>
<p><strong>Symmetric power of vectors.</strong> One way to produce symmetric tensors is to tensor a vector <span class="math inline">\(v\in \R^d\)</span> with itself <span class="math inline">\(p\)</span> times. Again, this is due to the commutativity of multiplication. For example, for a multi-index <span class="math inline">\([1, 2, 3]\)</span>, the entrie <span class="math inline">\(T_{[1, 2, 3]} = v_1 v_2 v_3\)</span> will equal <span class="math inline">\(T_{[3, 2, 1]} = v_3 v_2 v_1\)</span>. Showing that a tensor <span class="math inline">\(\bigotimes^p v\)</span> is symmetric is simple: <span class="math display">\[
\left[\bigotimes_{i=1}^p v \right ]_{\rho(\alpha)} = \prod_{i=1}^p v_{\alpha_{\rho(i)}} = \prod_{i=1}^p v_{\alpha_i} = \left[\bigotimes_{i=1}^p v \right ]_{\alpha}
\]</span></p>
<p>To construct embeddings that exploit the symmetries of <span class="math inline">\(T = \bigotimes^p v\)</span> we will need some key properties about symmetric tensors:</p>
<ul>
<li><strong>Duplication counts:</strong> If <span class="math inline">\(T\)</span> is symmetric, the entry <span class="math inline">\(T_\alpha\)</span> might have duplicate entries. To know how many duplicates a multi-index <span class="math inline">\(\alpha\)</span> has, we first need to count how many times each number <span class="math inline">\(i\in \{1, \cdots, d\}\)</span> occurs in <span class="math inline">\(\alpha\)</span>. Define the counts <span class="math inline">\(c_i = \sum_{j=1}^p \delta(\alpha_j, i)\)</span>. Then, the number of multi-indices containing the same data as <span class="math inline">\(\alpha\)</span> is given by the formula <span class="math inline">\(\frac{d!}{c_1 ! \; \cdots \; c_p!}\)</span>.</li>
<li><strong>Unique multi-indices:</strong> No data is lost if we restrict ourselves to only looking at entries <span class="math inline">\(T_\alpha\)</span> for multi-indices <span class="math inline">\(\alpha\)</span> that are non-decreasing (i.e <span class="math inline">\(\alpha_i \le \alpha_{i+1}\)</span>). The intuition is that an arbitrary multi-index <span class="math inline">\(\beta\)</span> can always be transformed into a non-decreasing multi-index <span class="math inline">\(\alpha\)</span> by applying some permutation <span class="math inline">\(\rho\)</span>. Using the defining property of symmetric tensors, <span class="math inline">\(T_\beta = T_{\rho(\beta)} = T_\alpha\)</span>. Thus, we loose no information by excluding every multi-index that isn’t non-decreasing.</li>
<li><strong>Dimension:</strong> The space of symmetric tensors has dimension <span class="math inline">\(\binom{d+p-1}{p}\)</span>. This can be derived via a classic combinatorial argument counting the number of non-decreasing sequences.</li>
</ul>
<details>
<summary>
Expand to see a complete derivation of these properties and a few other relevant facts about symmetric tensors.
</summary>
<p><strong>Duplicate counts.</strong> By definition, the only constraint a symmetric tensor has, is that all the entries <span class="math inline">\(T_{\rho(\alpha)}\)</span> must be the same for all permutations <span class="math inline">\(\rho \in G_p\)</span>. Now we want to understand the amount of duplication that that any specific <span class="math inline">\(\alpha\)</span> has. Since the number of permutations of the multi-indices is <span class="math inline">\(|G_p| = p!\)</span>, a naive estimate would be that every entrie <span class="math inline">\(T_\alpha\)</span> appears <span class="math inline">\(p!\)</span> times in the tensor. And indeed, that is the case for some multi-indices. For example, every permutation <span class="math inline">\(\rho \in G_3\)</span> sends the multi-index <span class="math inline">\([1,4,6]\)</span> to a different multi-index, so there are <span class="math inline">\(3!\)</span> entries with the same value. But. on the other hand, for the multi-index <span class="math inline">\([1,1,1]\)</span> it doesn’t matter what permutation <span class="math inline">\(\rho\)</span> we apply, we always have that <span class="math inline">\(\rho \alpha = \alpha\)</span>. So the entrie <span class="math inline">\([1,1,1]\)</span> of symmetric tensor <span class="math inline">\(T\)</span> could only appear a single time.</p>
<p>To answer this question of a generic <span class="math inline">\(\alpha\)</span> we need the <a href="https://proofwiki.org/wiki/Orbit-Stabilizer_Theorem">orbit stabilizer theorem</a>. It tells us that the number of different multi-indices that can be produced by applying permutations to <span class="math inline">\(\alpha\)</span> (a.k.a the size of the orbit) equals the total number of permutations (which is <span class="math inline">\(p!\)</span>) divided by the number of permutations fixing <span class="math inline">\(\alpha\)</span> (the size of the stabilizer). The size of the stabilizer is easy to work out. The only permutations that leave multi-indices the same are the ones that interchange indices with the same value. For example, <span class="math inline">\(\alpha = [\alpha_1, \alpha_2, \alpha_3] = [1,1,2]\)</span>, we could apply a permutation that interchanges the first and second indices. For the generic formula, we need to know how many times each index <span class="math inline">\(i\in \{ 1, \cdots, d\}\)</span> appears in the multi-index <span class="math inline">\(\alpha\)</span>. Define <span class="math display">\[
\text{count}(\alpha, i) = \sum_j \delta(\alpha_j - i)
\]</span> Where <span class="math inline">\(\delta\)</span> is the delta function sending <span class="math inline">\(0\)</span> to <span class="math inline">\(1\)</span> and every other number to <span class="math inline">\(0\)</span>. Clearly, the number of permutations fixing <span class="math inline">\(\alpha\)</span> is <span class="math inline">\(\prod_{i=1}^d \text{count}(\alpha, i)!\)</span>. So applying the orbit stabilizer theorem we get that a symmetric tensor <span class="math inline">\(T\)</span> at multi-index <span class="math inline">\(\alpha\)</span> has <span class="math display">\[
\frac {p!} {\prod_{i=1}^d \text{count}(\alpha, i)!} = \frac {p!}{\stab(\alpha)}
\]</span> duplicated entries. This is also known as the formula for the <a href="https://en.wikipedia.org/wiki/Multinomial_theorem">multinomial theorem</a>.</p>
<p><strong>Basis of symmetric tensors.</strong> We know that a lot of multi-indices of a symmetric tensor are redundant. To understand the true structure (like the dimensionality) of symmetric tensors we need to find a way to select an instance of each, non redundant, multi-indices. One way to do that is to restrict oursleves to <em>non decreasing multi-indices</em>. Denote them by <span class="math inline">\(P = \{\alpha \in \N_d^{\times p} \; | \; \alpha_i \le \alpha_{i+1} \}\)</span>. Then we can construct a basis for the space of symmetric tensors out of <span class="math inline">\(\alpha \in P\)</span> like <span class="math display">\[
S^\alpha = \frac 1 {\stab(\alpha)} \sum_{\rho \in G_p} E^{\rho(\alpha)}
\]</span> where you’ll recall that <span class="math inline">\(E^\alpha = \bigotimes^p_{i=1} e_{\alpha_i}\)</span> is the natural basis for the (non-symmetric) tensors. To convince ourselves that <span class="math inline">\(\{S^\alpha | \alpha \in P \}\)</span> forms a basis of the symmetric tensors we need to check that the set is linearly independent and that it spans all symmetirc tensors. Let’s check linear independence first. Assume that we have some coefficients <span class="math inline">\(x_\alpha \in \R\)</span> s.t. <span class="math display">\[
\sum_{\alpha \in P}  x_\alpha S^\alpha = 0
\]</span> Then, for any <span class="math inline">\(\beta \in P\)</span> <span class="math display">\[\begin{align}
0 &amp;= \left[\sum_{\alpha \in P}  x_\alpha S^\alpha\right]_\beta
= \sum_{\alpha \in P}  x_\alpha S^\alpha_\beta
= \sum_{\alpha \in P} \frac {x_\alpha} {\stab(\alpha)} \sum_{\rho \in G_p} E^{\rho(\alpha)}_\beta \\
&amp;= \sum_{\alpha \in P} \frac {x_\alpha} {\stab(\alpha)} \sum_{\rho \in G_p} 1(\rho(\alpha) = \beta) \\
&amp;= \frac {x_\beta} {\stab(\beta)} \sum_{\rho \in G_p} 1(\rho(\beta) = \beta) \quad \text{Since } \alpha, \beta \in P, \text{the only way that } \rho(\alpha)=\beta \text{ is if } \alpha = \beta \\
&amp;= x_\beta \\
\end{align}\]</span></p>
<p>Thus, all <span class="math inline">\(x_\alpha = 0\)</span> so the set is linearly independent. To show <span class="math inline">\(S^\alpha\)</span> span all symmetric tensors it we can just show that, for any symmetric tensor <span class="math inline">\(T\)</span>, if we define <span class="math display">\[
Q = \sum_{\alpha \in P} T_\alpha S^\alpha
\]</span> Then <span class="math inline">\(T = Q\)</span>. That can be easily seen by noticing that <span class="math inline">\(Q\)</span> is a symmetric tensor and that, evaluating <span class="math inline">\(Q\)</span> at <span class="math inline">\(\beta \in P\)</span> <span class="math display">\[\begin{align}
Q_\beta &amp;= \left[\sum_{\alpha \in P} T_\alpha S^\alpha \right]_\beta
= \sum_{\alpha \in P} \frac {T_\alpha} {\stab(\alpha)} \sum_{\rho \in G_p} E^{\rho(\alpha)}_\beta \\
&amp;= \sum_{\alpha \in P} \frac {T_\alpha} {\stab(\alpha)} \sum_{\rho \in G_p} 1(\rho(\alpha) = \beta) \\
&amp;= \frac {T_\beta} {\stab(\beta)} \sum_{\rho \in G_p} 1(\rho(\beta) = \beta) \\
&amp;= T_\beta
\end{align}\]</span></p>
<p>Concluding the proof. CARLES NOTE: consider flipping the order and showing first that they span all symmetric tensors.</p>
<p><strong>Dimension of symmetric tensors.</strong> Since we’ve created a basis for the space of symmetric tensors out of non-decreasing sequences we can establish the dimension of the space by counting all such sequences. This is a standard combinatorial problem solved via the method of <a href="https://en.wikipedia.org/wiki/Stars_and_bars_(combinatorics)">bars and stars</a>. Which tells us that the dimension is <span class="math display">\[\binom{d+p-1}{p}\]</span></p>
<p>The only thing we must note to apply the standard combinatorial results is that there is a 1-1 correspondance between non-decreasing sequences and multisets of <span class="math inline">\(d\)</span> elements of cardinality <span class="math inline">\(p\)</span>. This is made clear by looking at any example. The multiset <span class="math inline">\(\{ 1,1, 2, 4 \}\)</span> has a unique way in which it can be ordered if the sequence must be non-decreasing. CARLES NOTE: I don’t know how clear this all is. I kind of want to give a proof from scratch presenting the method of bars and stars</p>
<p><strong>Symmetric powers of vectors span the symmetric tensors</strong> Showing that all tensors of the form <span class="math inline">\(\bigotimes^p v\)</span> are symmetric was tivial, but there is a harder question we might ask ourselves. Are all the <span class="math inline">\(\binom {d + p - 1} p\)</span> dimensions of the symmetric tensors actually necessary? Just like the naive way to store <span class="math inline">\(\bigotimes^p v\)</span> contins <span class="math inline">\(d^p\)</span> most of which are duplicates of one another, perhaps there is still redundancy when we just store the entries at non-decreasing multi-indices.</p>
<p>The way to formalize this question is to ask whether the space of all symmetric tensors is spanned by tensors of the sort <span class="math inline">\(\bigotimes^p v\)</span>. The answer is yes, and is proven by showing the following result known as the <em>polarization identity</em>:</p>
</details>
</section>
<section id="implementation" class="level3">
<h3 class="anchored" data-anchor-id="implementation">3.2 Implementation</h3>
<p>Putting these two ideas together, <span class="math inline">\(\phi^p_\text{sym}(v)\)</span> will give a list of <span class="math inline">\(\binom{d+p-1}{p}\)</span> numbers, each corresponding to <span class="math inline">\([\bigotimes^p v]_\alpha\)</span> for non-decreasing <span class="math inline">\(\alpha\)</span>. Just as we did in the example of <span class="math inline">\(\phi^2_\text{sym}\)</span>, we also need to apply a correction that is the square root of the duplicate count of that particular <span class="math inline">\(\alpha\)</span>. The inner product of two vectors embedded in this way is identical to the repeated tensor product embedding, <span class="math inline">\(\phi^2_\text{sym}(v)^T \phi^2_\text{sym}(w) = \phi^2_\text {repeat}(v)^T \phi^2_\text {repeat}(w)\)</span>. The following is an example implementation of this embedding:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1"></a><span class="co"># helper function to iterate over non-decreasing multiindices</span></span>
<span id="cb4-2"><a href="#cb4-2"></a><span class="kw">def</span> index_iterator(a, d, p):</span>
<span id="cb4-3"><a href="#cb4-3"></a>  <span class="cf">assert</span> <span class="bu">len</span>(a) <span class="op">==</span> p</span>
<span id="cb4-4"><a href="#cb4-4"></a>  a <span class="op">=</span> a.copy()</span>
<span id="cb4-5"><a href="#cb4-5"></a>  <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">reversed</span>(<span class="bu">range</span>(p)):</span>
<span id="cb4-6"><a href="#cb4-6"></a>    <span class="cf">assert</span> a[i]<span class="op">&gt;=</span><span class="dv">0</span> <span class="kw">and</span> a[i]<span class="op">&lt;</span>d</span>
<span id="cb4-7"><a href="#cb4-7"></a>    a[i] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb4-8"><a href="#cb4-8"></a>    <span class="cf">if</span> a[i] <span class="op">&lt;</span> d:</span>
<span id="cb4-9"><a href="#cb4-9"></a>      <span class="co"># no need to carry. Just set all indices to the right = to a[i]</span></span>
<span id="cb4-10"><a href="#cb4-10"></a>      <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(i, p): a[j] <span class="op">=</span> a[i] </span>
<span id="cb4-11"><a href="#cb4-11"></a>      <span class="cf">break</span></span>
<span id="cb4-12"><a href="#cb4-12"></a>  <span class="cf">return</span> a</span>
<span id="cb4-13"><a href="#cb4-13"></a></span>
<span id="cb4-14"><a href="#cb4-14"></a><span class="co"># helper function to compute number of times an element appears</span></span>
<span id="cb4-15"><a href="#cb4-15"></a><span class="kw">def</span> count(a, d, p):</span>
<span id="cb4-16"><a href="#cb4-16"></a>  c <span class="op">=</span> []</span>
<span id="cb4-17"><a href="#cb4-17"></a>  <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(d):</span>
<span id="cb4-18"><a href="#cb4-18"></a>    ci <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb4-19"><a href="#cb4-19"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(p):</span>
<span id="cb4-20"><a href="#cb4-20"></a>      <span class="cf">if</span> a[j] <span class="op">==</span> i:</span>
<span id="cb4-21"><a href="#cb4-21"></a>        ci <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb4-22"><a href="#cb4-22"></a>    c.append(ci)</span>
<span id="cb4-23"><a href="#cb4-23"></a>  <span class="cf">return</span> c</span>
<span id="cb4-24"><a href="#cb4-24"></a></span>
<span id="cb4-25"><a href="#cb4-25"></a><span class="kw">def</span> symmetric_emb(v, d, p):</span>
<span id="cb4-26"><a href="#cb4-26"></a>  a <span class="op">=</span> [<span class="dv">0</span>] <span class="op">*</span> p</span>
<span id="cb4-27"><a href="#cb4-27"></a>  D <span class="op">=</span> binomial(d <span class="op">+</span> p <span class="op">-</span><span class="dv">1</span>, p)</span>
<span id="cb4-28"><a href="#cb4-28"></a>  x <span class="op">=</span> []</span>
<span id="cb4-29"><a href="#cb4-29"></a>  <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(D):</span>
<span id="cb4-30"><a href="#cb4-30"></a>    a <span class="op">=</span> index_iterator(a, d, p)</span>
<span id="cb4-31"><a href="#cb4-31"></a>    c <span class="op">=</span> count(a, d, p)</span>
<span id="cb4-32"><a href="#cb4-32"></a>    xi <span class="op">=</span> sqrt(multinomail(d, c))</span>
<span id="cb4-33"><a href="#cb4-33"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(p):</span>
<span id="cb4-34"><a href="#cb4-34"></a>      xi <span class="op">*=</span> v[a[j]]</span>
<span id="cb4-35"><a href="#cb4-35"></a>    x.append(xi)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Using this embedding produces a massive dimensionality reduction compared to the dimensionality of <span class="math inline">\(\phi_\text{repeat} ^p\)</span>. The table below compares the size of the state between repeated tensor products and symmetric powers, as measured in bytes (assuming half-precision), for a 124M-parameter GPT-2 repeated-tensor-product transformer at various <span class="math inline">\(p\)</span>.</p>
<div style="width: 60%; margin: auto; text-align: center;">
<table class="caption-top table">
<thead>
<tr class="header">
<th>p</th>
<th>Repeated Tensor Product</th>
<th>Symmetric Power</th>
<th>Savings</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>2</td>
<td>75 MB</td>
<td>38 MB</td>
<td>49%</td>
</tr>
<tr class="even">
<td>4</td>
<td>309 GB</td>
<td>14 GB</td>
<td>95%</td>
</tr>
<tr class="odd">
<td>6</td>
<td>1.2 PB</td>
<td>2.2 TB</td>
<td>99.8%</td>
</tr>
<tr class="even">
<td>8</td>
<td>5.2 EB</td>
<td>196 TB</td>
<td>99.996%</td>
</tr>
</tbody>
</table>
</div>
<p>Since this embedding produces identical outputs to the repeated tensor product, the learning curves are the same as those we saw in Section 3.2. We can evaluate each symmetric power architecture against our two metrics, state size (under 80 GB) and performance (loss below baseline).</p>
<table class="caption-top table" style="width:90%; margin: auto; text-align:center;">
<thead>
<tr>
<th>
p
</th>
<th>
State Size
</th>
<th>
Memory ≤ 80 GB?
</th>
<th>
Relative Loss at 100K Steps
</th>
<th>
Performs ≥ baseline?
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
2
</td>
<td>
38 MB
</td>
<td>
✓
</td>
<td>
1.03x
</td>
<td>
✗
</td>
</tr>
<tr style="background: #d4ffe6">
<td>
4
</td>
<td>
14 GB
</td>
<td>
✓
</td>
<td>
0.98x
</td>
<td>
✓
</td>
</tr>
<tr>
<td>
6
</td>
<td>
2.2 TB
</td>
<td>
✗
</td>
<td>
0.97x
</td>
<td>
✓
</td>
</tr>
<tr>
<td>
8
</td>
<td>
196 TB
</td>
<td>
✗
</td>
<td>
-
</td>
<td>
-
</td>
</tr>
</tbody>
</table>
<p>Symmetric power with <span class="math inline">\(p=4\)</span> passes our bar. This architecture is looking promising!</p>
</section>
</section>
<section id="conclusion-upcoming-work" class="level2">
<h2 class="anchored" data-anchor-id="conclusion-upcoming-work">Conclusion &amp; Upcoming Work</h2>
<p>In this article, we have shown how to design a linear transformer which closes the performance gap to classic softmax transformers, using a tractably-small state: increase the state size with the tensor product, remove the softplus, and exploit symmetry. We expect this approach will provide transformer-level performance at greatly reduced training costs when combined with the <a href="https://manifestai.com/articles/linear-transformers-are-faster/#chunked-formulation">chunked algorithm</a>, as well as the constant-time inference common to all RNNs. In an upcoming article, we plan to release an open-source model that uses a symmetric power transformer at its core, together with an efficient CUDA kernel implementation. Stay tuned!</p>
<p>Subscribe to our email list below for notifications of releases. Join our discord for discussion and early previews. Thanks for reading.</p>
<form action="https://buttondown.email/api/emails/embed-subscribe/manifestai" method="post" target="popupwindow" onsubmit="window.open('https://buttondown.email/manifestai', 'popupwindow')" class="embeddable-buttondown-form">
  <label for="bd-email" style="font-weight:bold;margin-right:20px;margin-top:20px;">
  Subscribe to be notified of new posts:
  </label>
  <input style="width: 35%;" type="email" name="email" id="bd-email" placeholder="Email">
  
  <input style="width: 80px;" type="submit" value="Subscribe">
</form>

</section>




<div id="quarto-appendix" class="default page-columns page-full"><section id="acknowledgments" class="level3 appendix"><h2 class="anchored quarto-appendix-heading">Acknowledgments</h2><div class="quarto-appendix-contents">

<p>TODO</p>
</div></section><section id="a.-softplus-split-tensor-product" class="level2 appendix page-columns page-full"><h2 class="anchored quarto-appendix-heading">A. Softplus split tensor product</h2><div class="quarto-appendix-contents page-columns page-full">

<p>The <em>softplus-split</em> embedding uses an initial softplus to ensure positivity, then splits the key of length <span class="math inline">\(d\)</span> into <span class="math inline">\(p\)</span> evenly-sized pieces of length <span class="math inline">\(\frac{d}{p}\)</span>, which are then combined using the tensor product. The resulting tensor is in <span class="math inline">\(\R^{\left(\frac{d}{p}\right)^p}\)</span>. Concretely, the equation for the softplus-split embedding is</p>
<p><span class="math display">\[
\phi^p_{\text{softplus-split}}(k) = \flat{\bigotimes_{i=1}^p \text{softplus}(k'_i)}
\]</span></p>
<p>where each <span class="math inline">\(k'_i\)</span> is the vector containing elements <span class="math inline">\(\big[k_{i\frac{d}{p}},...,k_{(i+1)\frac{d}{p} -1} \big]\)</span>.</p>
<p>Attention for a linear transformer with this embedding function has the following JAX implementation (lines 5 and 6 correspond to the application of Eq 1):</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="kw">def</span> softplus_split_attn(K, Q, V, p):</span>
<span id="cb5-2"><a href="#cb5-2"></a>    t, d <span class="op">=</span> K.shape</span>
<span id="cb5-3"><a href="#cb5-3"></a>    Q, K <span class="op">=</span> softplus(Q), softplus(K)</span>
<span id="cb5-4"><a href="#cb5-4"></a>    Q_split <span class="op">=</span> Q.reshape([t, p, d<span class="op">//</span>p]).transpose(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">2</span>)</span>
<span id="cb5-5"><a href="#cb5-5"></a>    KT_split <span class="op">=</span> K.reshape([t, p, d<span class="op">//</span>p]).transpose(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">0</span>)</span>
<span id="cb5-6"><a href="#cb5-6"></a>    C_split <span class="op">=</span> jax.vmap(jnp.matmul)(Q_split, KT_split)</span>
<span id="cb5-7"><a href="#cb5-7"></a>    B <span class="op">=</span> jnp.prod(C_split, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb5-8"><a href="#cb5-8"></a>    B <span class="op">=</span> where(mask, B, <span class="dv">0</span>)</span>
<span id="cb5-9"><a href="#cb5-9"></a>    A <span class="op">=</span> B <span class="op">/</span> B.<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-10"><a href="#cb5-10"></a>    Y <span class="op">=</span> A <span class="op">@</span> V</span>
<span id="cb5-11"><a href="#cb5-11"></a>    <span class="cf">return</span> Y</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>See Appendix B for a numerically-stable implementation.</p>
<p>Let’s put this into action:</p>
<div class="column-body" style="text-align: center;">
<iframe width="600" height="350" src="plots/02_softplus_split.html">
</iframe>
</div>
<p>We see that increasing <span class="math inline">\(p\)</span> causes performance to improve. However, all values of <span class="math inline">\(p\)</span> tested still fall short of the baseline.</p>
</div></section><section id="b.-numerical-stability" class="level2 appendix"><h2 class="anchored quarto-appendix-heading">B. Numerical stability</h2><div class="quarto-appendix-contents">

<p>Numerical instabilities come from numbers underflowing (too small) or overflowing (too large). Solutions typically fall into a few main categories:</p>
<ul>
<li>Make sure a number is not too small or too large, e.g.&nbsp;turn <code>log(x)</code> into <code>log(x + ε)</code>. This prevents overflow.</li>
<li>Accumulate in <code>fp32</code>. When accumulating a long list of small values in half-precision, it is sometimes the case that each addition will underflow and no accumulation will occur at all.</li>
<li>Manipulate an equation to pull out some common factors and cancel them out algebraically, rather than letting them cancel out computationally. For example, to calculate <span class="math inline">\(\frac{x}{y}\)</span> where <span class="math inline">\(x = Am\)</span> and <span class="math inline">\(y = An\)</span> for some large <span class="math inline">\(A\)</span>, compute <code>m / n</code> instead of <code>x / y</code>.</li>
<li>Separate magnitude and sign, and work with magnitude in log-space, i.e.&nbsp;manipulate <code>sign(x)</code> and <code>log(abs(x))</code> instead of working with <code>x</code> directly. Convert back to linear-space with <code>sign(x) * exp(f(log(abs(x))))</code>, where <code>f</code> has already completed the relevant cancellations, so <code>x</code> is small enough too avoid overflow.</li>
</ul>
<p>With these techniques in mind, we can implement a numerically-stable version of the softplus-split approach. The main change is that we sum in log-space instead of multiplying, and preemptively cancel any large terms in the numerators with their corresponding denominators by subtracting the log-space row-max.</p>
<details>
<summary>
Expand for numerically-stable implementation of <strong>softplus-split</strong>.
</summary>
<div class="sourceCode" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a><span class="kw">def</span> softplus_split_tensor_product_attn(Q, K, V, p, ε):</span>
<span id="cb6-2"><a href="#cb6-2"></a>    t, d <span class="op">=</span> Q.shape <span class="co"># temporal &amp; head-width dimensions</span></span>
<span id="cb6-3"><a href="#cb6-3"></a>    <span class="cf">assert</span> d <span class="op">%</span> p <span class="op">==</span> <span class="dv">0</span></span>
<span id="cb6-4"><a href="#cb6-4"></a></span>
<span id="cb6-5"><a href="#cb6-5"></a>    <span class="co"># softplus to make simplex norm behave nicely</span></span>
<span id="cb6-6"><a href="#cb6-6"></a>    Q, K <span class="op">=</span> softplus(Q), softplus(K)</span>
<span id="cb6-7"><a href="#cb6-7"></a>    <span class="co"># split via a reshape and transpose</span></span>
<span id="cb6-8"><a href="#cb6-8"></a>    Q_split <span class="op">=</span> Q.reshape([t, p, d<span class="op">//</span>p]).transpose(<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">2</span>)</span>
<span id="cb6-9"><a href="#cb6-9"></a>    KT_split <span class="op">=</span> K.reshape([t, p, d<span class="op">//</span>p]).transpose(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">0</span>)</span>
<span id="cb6-10"><a href="#cb6-10"></a></span>
<span id="cb6-11"><a href="#cb6-11"></a>    <span class="co"># compute each dot product</span></span>
<span id="cb6-12"><a href="#cb6-12"></a>    split_D <span class="op">=</span> vmap(jnp.matmul)(Q_split, KT_split)</span>
<span id="cb6-13"><a href="#cb6-13"></a>    <span class="co"># combine splits in log space for numerical stability</span></span>
<span id="cb6-14"><a href="#cb6-14"></a>    log_C <span class="op">=</span> <span class="bu">sum</span>(log(split_D <span class="op">+</span> ε), axis<span class="op">=</span><span class="dv">0</span>, dtype<span class="op">=</span>float32).astype(Q.dtype)</span>
<span id="cb6-15"><a href="#cb6-15"></a>    <span class="co"># apply the causal mask</span></span>
<span id="cb6-16"><a href="#cb6-16"></a>    log_C <span class="op">=</span> where(tril(ones(log_C.shape)), log_C, <span class="op">-</span>inf)</span>
<span id="cb6-17"><a href="#cb6-17"></a>    <span class="co"># subtract rowmax for numerical stability</span></span>
<span id="cb6-18"><a href="#cb6-18"></a>    log_C <span class="op">-=</span> log_C.<span class="bu">max</span>(axis<span class="op">=-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-19"><a href="#cb6-19"></a></span>
<span id="cb6-20"><a href="#cb6-20"></a>    <span class="co"># return to linear space</span></span>
<span id="cb6-21"><a href="#cb6-21"></a>    B <span class="op">=</span> exp(log_C)</span>
<span id="cb6-22"><a href="#cb6-22"></a>    <span class="co"># compute the normalizing term, accumulating in float32 for numerical stability</span></span>
<span id="cb6-23"><a href="#cb6-23"></a>    denom <span class="op">=</span> B.<span class="bu">sum</span>(<span class="op">-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>, dtype<span class="op">=</span>float32).astype(B.dtype)</span>
<span id="cb6-24"><a href="#cb6-24"></a></span>
<span id="cb6-25"><a href="#cb6-25"></a>    <span class="co"># project to simplex, adding ε for numerical stability</span></span>
<span id="cb6-26"><a href="#cb6-26"></a>    A <span class="op">=</span> B <span class="op">/</span> (denom <span class="op">+</span> ε)</span>
<span id="cb6-27"><a href="#cb6-27"></a>    <span class="co"># compute output</span></span>
<span id="cb6-28"><a href="#cb6-28"></a>    Y <span class="op">=</span> A <span class="op">@</span> V</span>
<span id="cb6-29"><a href="#cb6-29"></a>    <span class="cf">return</span> Y</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<details>
<summary>
Expand for a numerically-stable implementation of <strong>softplus-repeat</strong>.
</summary>
<div class="sourceCode" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a><span class="kw">def</span> softplus_repeat_tensor_product_attn(Q, K, V, p, ε):</span>
<span id="cb7-2"><a href="#cb7-2"></a>    <span class="co"># use softplus to project to the positive quadrant</span></span>
<span id="cb7-3"><a href="#cb7-3"></a>    Q, K <span class="op">=</span> softplus(Q), softplus(K)</span>
<span id="cb7-4"><a href="#cb7-4"></a></span>
<span id="cb7-5"><a href="#cb7-5"></a>    <span class="co"># compute inner products</span></span>
<span id="cb7-6"><a href="#cb7-6"></a>    D <span class="op">=</span> Q <span class="op">@</span> K.T</span>
<span id="cb7-7"><a href="#cb7-7"></a></span>
<span id="cb7-8"><a href="#cb7-8"></a>    <span class="co"># raise to power, in log space for numerical stability</span></span>
<span id="cb7-9"><a href="#cb7-9"></a>    log_C <span class="op">=</span> p <span class="op">*</span> log(D <span class="op">+</span> ε)</span>
<span id="cb7-10"><a href="#cb7-10"></a>    <span class="co"># apply causal mask</span></span>
<span id="cb7-11"><a href="#cb7-11"></a>    log_C <span class="op">=</span> where(tril(ones(log_C.shape)), log_C, <span class="op">-</span>inf)</span>
<span id="cb7-12"><a href="#cb7-12"></a>    <span class="co"># subtract rowmax for numerical stability</span></span>
<span id="cb7-13"><a href="#cb7-13"></a>    log_C <span class="op">-=</span> log_C.<span class="bu">max</span>(axis<span class="op">=-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-14"><a href="#cb7-14"></a>    </span>
<span id="cb7-15"><a href="#cb7-15"></a>    <span class="co"># Return to linear space</span></span>
<span id="cb7-16"><a href="#cb7-16"></a>    B <span class="op">=</span> exp(log_C)</span>
<span id="cb7-17"><a href="#cb7-17"></a>    <span class="co"># Compute the normalizing term, accumulating in float32 for numerical stability</span></span>
<span id="cb7-18"><a href="#cb7-18"></a>    denom <span class="op">=</span> B.<span class="bu">sum</span>(<span class="op">-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>, dtype<span class="op">=</span>float32).astype(B.dtype)</span>
<span id="cb7-19"><a href="#cb7-19"></a>    </span>
<span id="cb7-20"><a href="#cb7-20"></a>    <span class="co"># project to simplex, adding ε for numerical stability</span></span>
<span id="cb7-21"><a href="#cb7-21"></a>    A <span class="op">=</span> B <span class="op">/</span> (denom <span class="op">+</span> ε)</span>
<span id="cb7-22"><a href="#cb7-22"></a>    <span class="co"># compute output</span></span>
<span id="cb7-23"><a href="#cb7-23"></a>    Y <span class="op">=</span> A <span class="op">@</span> V</span>
<span id="cb7-24"><a href="#cb7-24"></a>    <span class="cf">return</span> Y</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<details>
<summary>
Expand for a numerically-stable implementation of <strong>repeat</strong>.
</summary>
<div class="sourceCode" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a><span class="kw">def</span> repeat_tensor_product_attn(Q, K, V, p, ε):</span>
<span id="cb8-2"><a href="#cb8-2"></a>     <span class="co"># even only</span></span>
<span id="cb8-3"><a href="#cb8-3"></a>     <span class="cf">assert</span> p <span class="op">%</span> <span class="dv">2</span> <span class="op">==</span> <span class="dv">0</span></span>
<span id="cb8-4"><a href="#cb8-4"></a></span>
<span id="cb8-5"><a href="#cb8-5"></a>    <span class="co"># compute inner products</span></span>
<span id="cb8-6"><a href="#cb8-6"></a>    D <span class="op">=</span> Q <span class="op">@</span> K.T</span>
<span id="cb8-7"><a href="#cb8-7"></a></span>
<span id="cb8-8"><a href="#cb8-8"></a>    <span class="co"># raise to power, in log space for numerical stability</span></span>
<span id="cb8-9"><a href="#cb8-9"></a>    log_C <span class="op">=</span> p <span class="op">*</span> log(D <span class="op">+</span> ε)</span>
<span id="cb8-10"><a href="#cb8-10"></a>    <span class="co"># apply causal mask</span></span>
<span id="cb8-11"><a href="#cb8-11"></a>    log_C <span class="op">=</span> where(tril(ones(log_C.shape)), log_C, <span class="op">-</span>inf)</span>
<span id="cb8-12"><a href="#cb8-12"></a>    <span class="co"># subtract rowmax for numerical stability</span></span>
<span id="cb8-13"><a href="#cb8-13"></a>    log_C <span class="op">-=</span> log_C.<span class="bu">max</span>(axis<span class="op">=-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb8-14"><a href="#cb8-14"></a>    </span>
<span id="cb8-15"><a href="#cb8-15"></a>    <span class="co"># Return to linear space</span></span>
<span id="cb8-16"><a href="#cb8-16"></a>    B <span class="op">=</span> exp(log_C)</span>
<span id="cb8-17"><a href="#cb8-17"></a>    <span class="co"># Compute the normalizing term, accumulating in float32 for numerical stability</span></span>
<span id="cb8-18"><a href="#cb8-18"></a>    denom <span class="op">=</span> B.<span class="bu">sum</span>(<span class="op">-</span><span class="dv">1</span>, keepdims<span class="op">=</span><span class="va">True</span>, dtype<span class="op">=</span>float32).astype(B.dtype)</span>
<span id="cb8-19"><a href="#cb8-19"></a>    </span>
<span id="cb8-20"><a href="#cb8-20"></a>    <span class="co"># project to simplex, adding ε for numerical stability</span></span>
<span id="cb8-21"><a href="#cb8-21"></a>    A <span class="op">=</span> B <span class="op">/</span> (denom <span class="op">+</span> ε)</span>
<span id="cb8-22"><a href="#cb8-22"></a>    <span class="co"># compute output</span></span>
<span id="cb8-23"><a href="#cb8-23"></a>    Y <span class="op">=</span> A <span class="op">@</span> V</span>
<span id="cb8-24"><a href="#cb8-24"></a>    <span class="cf">return</span> Y</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>



</div></section><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body" data-entry-spacing="0" role="list">
<div id="ref-katharopoulos2020transformers" class="csl-entry" role="listitem">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret, <span>“Transformers are rnns: Fast autoregressive transformers with linear attention,”</span> in <em>International conference on machine learning</em>, PMLR, 2020, pp. 5156–5165.</div>
</div>
<div id="ref-wang2020linformer" class="csl-entry" role="listitem">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">S. Wang, B. Z. Li, M. Khabsa, H. Fang, and H. Ma, <span>“Linformer: Self-attention with linear complexity,”</span> <em>arXiv preprint arXiv:2006.04768</em>, 2020.</div>
</div>
<div id="ref-schlag2021linear" class="csl-entry" role="listitem">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">I. Schlag, K. Irie, and J. Schmidhuber, <span>“Linear transformers are secretly fast weight programmers,”</span> in <em>International conference on machine learning</em>, PMLR, 2021, pp. 9355–9366.</div>
</div>
<div id="ref-kacham2023polysketchformer" class="csl-entry" role="listitem">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline">P. Kacham, V. Mirrokni, and P. Zhong, <span>“Polysketchformer: Fast transformers via sketches for polynomial kernels,”</span> <em>arXiv preprint arXiv:2310.01655</em>, 2023.</div>
</div>
<div id="ref-zhang2024hedgehog" class="csl-entry" role="listitem">
<div class="csl-left-margin">[5] </div><div class="csl-right-inline">M. Zhang, K. Bhatia, H. Kumbong, and C. Ré, <span>“The hedgehog &amp; the porcupine: Expressive linear attentions with softmax mimicry,”</span> <em>arXiv preprint arXiv:2402.04347</em>, 2024.</div>
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>This is also known as the <a href="https://en.wikipedia.org/wiki/Kernel_method">kernel method</a>. It is essential when one works with infinite-dimensional embeddings, and it’s also useful in this case to avoid materializing large-but-finite embeddings.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>This state-size threshold is admittedly somewhat arbitrary. In principle, larger states are possible with clever sharding; but for excessively large states, which must be sharded across a huge number of GPUs, the hardware cost of sharding becomes completely prohibitive. In this article, we are training models whose parameters fit on a single GPU, so it seems reasonable to use the memory of a single GPU as the threshold for tractability.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>A natural choice for the ordering <span class="math inline">\(\sigma\)</span> is <a href="https://en.wikipedia.org/wiki/Row-_and_column-major_order#:~:text=In%20row%2Dmajor%20order%2C%20the,column%20in%20column%2Dmajor%20order.">row major ordering</a>.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{buckman2024,
  author = {Buckman, Jacob and Gelada, Carles and Zhang, Sean},
  publisher = {Manifest AI},
  title = {Symmetric {Power} {Transformers}},
  date = {2024-07-25},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-buckman2024" class="csl-entry quarto-appendix-citeas" role="listitem">
<div class="">J.
Buckman, C. Gelada, and S. Zhang, <span>“Symmetric Power
Transformers.”</span> Manifest AI, Jul. 25, 2024.</div>
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="m-a-n-i-f-e-s-t/website" data-repo-id="R_kgDOLA6vSg" data-category="Announcements" data-category-id="DIC_kwDOLA6vSs4Cgv3x" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" data-loading="lazy" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>© 2024 Manifest AI</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>