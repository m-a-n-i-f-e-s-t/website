<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.54">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Jacob Buckman">
<meta name="author" content="Carles Gelada">
<meta name="author" content="Saurabh Kumar">
<meta name="author" content="Sean Zhang">
<meta name="dcterms.date" content="2024-12-10">

<title>Improving Symmetric Power Transformers with Conformal Transformations – Manifest AI</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<link href="../../favicon.png" rel="icon" type="image/png">
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-V0D26E23Q3"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-V0D26E23Q3', { 'anonymize_ip': true});
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Improving Symmetric Power Transformers with Conformal Transformations - Manifest AI">
<meta property="og:description" content="TODO">
<meta property="og:image" content="/thumbnails/symmetric-power-transformers.png">
<meta property="og:site_name" content="Manifest AI">
<meta name="twitter:title" content="Optimizing Symmetric Power Transformers - Manifest AI">
<meta name="twitter:description" content="TODO">
<meta name="twitter:image" content="https://manifestai.com/thumbnails/symmetric-power-transformers.png">
<meta name="twitter:creator" content="@manifest__ai">
<meta name="twitter:site" content="@manifest__ai">
<meta name="twitter:card" content="summary">
<meta name="citation_title" content="Improving Symmetric Power Transformers with Conformal Transformations">
<meta name="citation_author" content="Jacob Buckman">
<meta name="citation_author" content="Carles Gelada">
<meta name="citation_author" content="Saurabh Kumar">
<meta name="citation_author" content="Sean Zhang">
<meta name="citation_publication_date" content="2024-12-10">
<meta name="citation_cover_date" content="2024-12-10">
<meta name="citation_year" content="2024">
<meta name="citation_online_date" content="2024-12-10">
<meta name="citation_language" content="en">
<meta name="citation_publisher" content="Manifest AI">
<meta name="citation_reference" content="citation_title=Roformer: Enhanced transformer with rotary position embedding;,citation_author=Jianlin Su;,citation_author=Murtadha Ahmed;,citation_author=Yu Lu;,citation_author=Shengfeng Pan;,citation_author=Wen Bo;,citation_author=Yunfeng Liu;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_volume=568;,citation_journal_title=Neurocomputing;,citation_publisher=Elsevier;">
<meta name="citation_reference" content="citation_title=Random feature attention;,citation_author=Hao Peng;,citation_author=Nikolaos Pappas;,citation_author=Dani Yogatama;,citation_author=Roy Schwartz;,citation_author=Noah A Smith;,citation_author=Lingpeng Kong;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_journal_title=arXiv preprint arXiv:2103.02143;">
<meta name="citation_reference" content="citation_title=Fine-tuning pre-trained transformers into decaying fast weights;,citation_author=Huanru Henry Mao;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_journal_title=arXiv preprint arXiv:2210.04243;">
<meta name="citation_reference" content="citation_title=Gateloop: Fully data-controlled linear recurrence for sequence modeling;,citation_author=Tobias Katsch;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_journal_title=arXiv preprint arXiv:2311.01927;">
<meta name="citation_reference" content="citation_title=Transformers are SSMs: Generalized models and efficient algorithms through structured state space duality;,citation_author=Tri Dao;,citation_author=Albert Gu;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_journal_title=arXiv preprint arXiv:2405.21060;">
<meta name="citation_reference" content="citation_title=You only cache once: Decoder-decoder architectures for language models;,citation_author=Yutao Sun;,citation_author=Li Dong;,citation_author=Yi Zhu;,citation_author=Shaohan Huang;,citation_author=Wenhui Wang;,citation_author=Shuming Ma;,citation_author=Quanlu Zhang;,citation_author=Jianyong Wang;,citation_author=Furu Wei;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_journal_title=arXiv preprint arXiv:2405.05254;">
<meta name="citation_reference" content="citation_title=Gated linear attention transformers with hardware-efficient training;,citation_author=Songlin Yang;,citation_author=Bailin Wang;,citation_author=Yikang Shen;,citation_author=Rameswar Panda;,citation_author=Yoon Kim;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_journal_title=arXiv preprint arXiv:2312.06635;">
<meta name="citation_reference" content="citation_title=Mamba: Linear-time sequence modeling with selective state spaces;,citation_author=Albert Gu;,citation_author=Tri Dao;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_journal_title=arXiv preprint arXiv:2312.00752;">
<meta name="citation_reference" content="citation_title=Retentive network: A successor to transformer for large language models;,citation_author=Yutao Sun;,citation_author=Li Dong;,citation_author=Shaohan Huang;,citation_author=Shuming Ma;,citation_author=Yuqing Xia;,citation_author=Jilong Xue;,citation_author=Jianyong Wang;,citation_author=Furu Wei;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_journal_title=arXiv preprint arXiv:2307.08621;">
<meta name="citation_reference" content="citation_title=Compute-Optimal Context Size;,citation_author=Jacob Buckman;,citation_author=Carles Gelada;,citation_publication_date=2024-05-16;,citation_cover_date=2024-05-16;,citation_year=2024;,citation_language=en;,citation_publisher=Manifest AI;">
<meta name="citation_reference" content="citation_title=Symmetric Power Transformers;,citation_author=Jacob Buckman;,citation_author=Carles Gelada;,citation_author=Sean Zhang;,citation_publication_date=2024-08-15;,citation_cover_date=2024-08-15;,citation_year=2024;,citation_language=en;,citation_publisher=Manifest AI;">
<meta name="citation_reference" content="citation_title=Train short, test long: Attention with linear biases enables input length extrapolation;,citation_author=Ofir Press;,citation_author=Noah A Smith;,citation_author=Mike Lewis;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_journal_title=arXiv preprint arXiv:2108.12409;">
</head>

<body class="floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../m-logo-tight.png" alt="" class="navbar-logo">
    </a>
  </div>
          <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/m-a-n-i-f-e-s-t/power-attention"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://discord.gg/aFsCgDraGP"> <i class="bi bi-discord" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/manifest__ai"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
   
  <ul>
  <li><a href="#section" id="toc-section" class="nav-link active" data-scroll-target="#section"></a>
  <ul class="collapse">
  <li><a href="#symmetric-power-transformers" id="toc-symmetric-power-transformers" class="nav-link" data-scroll-target="#symmetric-power-transformers">1. Symmetric Power Transformers</a>
  <ul class="collapse">
  <li><a href="#rotary-embeddings-rope" id="toc-rotary-embeddings-rope" class="nav-link" data-scroll-target="#rotary-embeddings-rope">1.1 Rotary embeddings (RoPE)</a></li>
  </ul></li>
  <li><a href="#gating" id="toc-gating" class="nav-link" data-scroll-target="#gating">2. Gating</a></li>
  <li><a href="#learned-rotary-embeddings" id="toc-learned-rotary-embeddings" class="nav-link" data-scroll-target="#learned-rotary-embeddings">3. Learned Rotary Embeddings</a>
  <ul class="collapse">
  <li><a href="#full-training-curves" id="toc-full-training-curves" class="nav-link" data-scroll-target="#full-training-curves">Full Training Curves</a></li>
  </ul></li>
  <li><a href="#conformal-state-transformations-optional-reading" id="toc-conformal-state-transformations-optional-reading" class="nav-link" data-scroll-target="#conformal-state-transformations-optional-reading">4. Conformal State Transformations (Optional Reading)</a></li>
  <li><a href="#equivalance-between-gating-and-alibi-optional-reading" id="toc-equivalance-between-gating-and-alibi-optional-reading" class="nav-link" data-scroll-target="#equivalance-between-gating-and-alibi-optional-reading">5. Equivalance between Gating and ALiBi (Optional Reading)</a></li>
  </ul></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Improving Symmetric Power Transformers with Conformal Transformations</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Authors</div>
    <div class="quarto-title-meta-contents">
             <p>Jacob Buckman </p>
             <p>Carles Gelada </p>
             <p>Saurabh Kumar </p>
             <p>Sean Zhang </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">December 10, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<div class="hidden">
<p><span class="math display">\[
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\sft}{\text{softmax}}
\newcommand{\List}{\text{List}}
\newcommand{\Seq}{\text{Seq}}
\newcommand{\SeqT}{\text{SeqT}}
\newcommand{\CSeqT}{\text{CSeqT}}
\newcommand{\Dist}{\text{Dist}}
\newcommand{\SM}{\text{SM}}
\newcommand{\Fn}{\text{Fn}}
\newcommand{\Tok}{\text{Tok}}
\newcommand{\Aij}{ A_{[i,j]}}
\newcommand{\ten}{\small\text{tensor}}
\newcommand{\sym}{\small\text{symmetric}}
\]</span></p>
</div>
<style>
summary {
  cursor: pointer;
  padding: 8px;
  background-color: #f0f0f0; /* Light grey background */
  border-radius: 8px; /* Rounded corners for consistency */
}

/* Hover state changes the background color to a lighter grey */
summary:hover {
  background-color: #e0e0e0; /* Lighter grey on hover */
}

/* Override hover effect when details is open */
details[open] summary:hover {
  background-color: #f0f0f0; /* Maintain the same color as the non-hover state */
}

details[open] summary {
  padding: 8px;
}

details {
  padding: 0; /* No padding when details is not open */
  border-radius: 8px; /* Rounded corners for consistency */
  background-color: #f0f0f0; /* Light grey background */
}

details[open] {
  padding: 8px; /* Padding inside the details for content alignment */
}
</style>
<!-- # Conformal embeddings: a unification of data dependent gating and RoPE -->
<section id="section" class="level1 page-columns page-full">
<h1></h1>
<p>In a previous <a href="https://manifestai.com/articles/symmetric-power-transformers/">article</a>, we introduced symmetric power transformers, which are a variant of linear transformers with an embedding function based on the theory of symmetric tensors. We demonstrated that when using a context size of <span class="math inline">\(4096\)</span> on the LongCrawl64 dataset, the symmetric power transformer successfully closes the performance gap to classic softmax transformers using a tractably-small state. Specifically, at context size <span class="math inline">\(4096\)</span> a symmetric power transformer with <span class="math inline">\(p=4\)</span> satisfies two desired properties: (1) performance matches that of a softmax transformer baseline and (2) has a state size small enough to be tractable (<span class="math inline">\(\leq\)</span> <span class="math inline">\(80\)</span> GB).</p>
<p>While having a recurrent formulation allows for efficient training and inference (training cost is linear instead of quadratic and inference cost is constant instead of linear), the capacity constraint on the recurrent state poses a fundamental limitation. A symmetric power transformer with a finite dimensional state size can only store a small fraction of the information that a softmax transformer stores. Since symmetric power transformers don’t have any easy way to erase information in their states, one might worry that they might suffer from poor performance when the context size is large. Indeed, we see that this issue emerges in two ways: (1) at training time and (2) at inference time.</p>
<p><strong>1. At training time.</strong> Symmetric power transformers suffer degraded performance compared to the softmax baseline as the context size grows. We see this trend begin to emerge when the symmetric power <span class="math inline">\(p = 4\)</span>, and it occurs at smaller context sizes when <span class="math inline">\(p=2\)</span>.</p>
<div class="column-body" style="text-align: center;">
<iframe width="600" height="410" src="plots/relative_loss_softmax_vs_sympow_dropdown.html">
</iframe>
</div>
<p>It is evident that the effect is more pronounced for the <span class="math inline">\(p=2\)</span> (<span class="math inline">\(39\)</span> MB state size) model than the <span class="math inline">\(p=4\)</span> (<span class="math inline">\(14\)</span> GB state size). But solving this poor scaling of the training context size will be important even for the large models if we want to push them to millions of tokens in the context.</p>
<p>In the linear transformer literature, gating has been shown to help with this problem <span class="citation" data-cites="gu2023mamba"><a href="#ref-gu2023mamba" role="doc-biblioref">[1]</a></span>, <span class="citation" data-cites="yang2023gated"><a href="#ref-yang2023gated" role="doc-biblioref">[2]</a></span>, <span class="citation" data-cites="dao2024transformers"><a href="#ref-dao2024transformers" role="doc-biblioref">[3]</a></span>, <span class="citation" data-cites="sun2024you"><a href="#ref-sun2024you" role="doc-biblioref">[4]</a></span>. Gating allows the network to erase some of the information that is placed on the state. A common approach is to set a different rate of forgetting per head <span class="citation" data-cites="sun2023retentive"><a href="#ref-sun2023retentive" role="doc-biblioref">[5]</a></span>.</p>
<p><strong>2. At inference time.</strong> If we evaluate the ability of a trained model to make predictions at different context lengths (including ones longer than the ones used during training), we see that there is hardly any ability to generalize beyond the training context. The following contextwise plot shows just that. Following our previous <a href="https://manifestai.com/articles/compute-optimal-context-size">article</a>, it shows the average loss at different context lengths. The training context size is <span class="math inline">\(16384\)</span> which is indicated by the dashed red line.</p>
<div class="column-body" style="text-align: center;">
<iframe width="600" height="400" src="plots/context_wise_scaling_plot_train_context_16k_softmax_vs_sympow.html">
</iframe>
</div>
<p>In the transformer literature, one approach has become widely adopted to remediate this problem. ALiBi positional embeddings <span class="citation" data-cites="press2021train"><a href="#ref-press2021train" role="doc-biblioref">[6]</a></span> have been shown to make transformers generalize well beyond the training context size. Surprisingly, it turns out that ALiBi and gating are mathematically equivalent (Section 5 gives more details). Further, when we think of standard transformers as linear transformers using an infinite dimensional embedding function, the linear bias of ALiBi corresponds exactly to multiplicative gating on this infinite dimensional state.</p>
<p>One important thing to keep in mind is that any architectural modification to a linear transformer variant must be compatible with both the recurrent and attention formulations. We need to ensure this compatibility both when introducing gating and when integrating rotary embeddings with symmetric power transformers.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>In our previous article we briefly discussed the fact symmetric power transformers are compatible with rotary embeddings, but we didn’t expand on the recurrent implementation of rotary embeddings.</p>
</div></div><p>In this article, we make the following contributions:</p>
<ul>
<li>We apply learned (and data dependent) multiplicative gating developed in prior work <span class="citation" data-cites="dao2024transformers"><a href="#ref-dao2024transformers" role="doc-biblioref">[3]</a></span> to symmetric power transformers. We describe how to implement gating in both the attention and recurrent forms of the symmetric power transformer architecture.</li>
<li>We discuss how to implement rotary embeddings with symmetric power transformers in both the attention and recurrent formulations.</li>
<li>We introduce learned and data dependent rotary embeddings by allowing the model to learn adjustments to the standard rotations applied at each time step.</li>
<li>We highlight that the ideas of gating and rotary embedings can be conceptually unified into a single object: a conformal transformation of the state. This unification does not only hold for our sympow transformers, we’ll see that any classic transformer with rotary embeddings and Alibi can be thought of as an RNN with an infinite dimensional state that has a conformal transformation.</li>
</ul>
<p>All of these ideas combine to form the conformal symmetric power tranformer, which we call <strong>conformal-sympow</strong>. Our experiments on the Longcrawl64 dataset demonstrate that conformal-sympow further improves the performance of sympow, especially for longer context lengths. You can see that it doesn’t suffer from the degraded scaling of training context:</p>
<div class="column-body" style="text-align: center;">
<iframe width="600" height="410" src="plots/relative_loss_softmax_vs_conformal_sympow_dropdown.html">
</iframe>
</div>
<p>And, at inference time generalizes well beyond the training context:</p>
<div class="column-body" style="text-align: center;">
<iframe width="600" height="400" src="plots/context_wise_scaling_plot_train_context_16k_softmax_vs_conformal_sympow.html">
</iframe>
</div>
<p>We are happy with this architecture. Our next post will bring everything together by implementing conformal-sympow efficiently with its recurrent and chunked formulations.</p>
<section id="symmetric-power-transformers" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="symmetric-power-transformers">1. Symmetric Power Transformers</h2>
<p>We begin with a high level overview of symmetric power transformers. The inputs to the layer are sequences of <span class="math inline">\(Q_i, K_i, V_i \in \R^d\)</span> of queries, keys, and values, where <span class="math inline">\(i\)</span> ranges from <span class="math inline">\(1\)</span> to the sequence length <span class="math inline">\(t\)</span>. The outputs are a sequence <span class="math inline">\(Y_i\in \R^d\)</span>. In the <em>attention formulation</em>, the formula for the output vectors is: <span class="math display">\[
Y_i = \sum_{j=1}^i A_{ij} V_j \qquad A_{ij}  = \frac{B_{ij}}{\sum_{k=1}^i B_{ik}} \qquad B_{ij} = (Q_i^T K_j)^p
\qquad \text{(sympow)}
\]</span> We refer to <span class="math inline">\(A_{ij}\)</span> as the attention scores and <span class="math inline">\(B_{ij}\)</span> as the preattention scores (mirroring the preactivation/activation lanugage often use to desrive the hidden values of an MLP before and after the nonlinearity).</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>It is important that the power <span class="math inline">\(p\)</span> is even because that guarantees the denominator is positive, which makes <span class="math inline">\(A_{i1}, \cdots, A_{ii}\)</span> a valid probability distribution. In turn, this makes the outputs <span class="math inline">\(Y_i\)</span> a convex combinatoin of <span class="math inline">\(V_1, \cdots, V_i\)</span>.</p>
</div></div><p>The exact same outputs <span class="math inline">\(Y_i\)</span> can be computed via a <em>recurrent formulation</em>. Doing so invovles an embedding function <span class="math inline">\(\phi^p : \R^d \to \R^D\)</span>. The vector <span class="math inline">\(\phi^p(k)\)</span> contains the same information as <span class="math inline">\({k\otimes \cdots \otimes k}\)</span>, repeatedly taking tensor product <span class="math inline">\(p\)</span> times. But it does so much more efficiently because it removes a lot of symmetry in the tensor product. Thus <span class="math inline">\(D &lt;&lt; d^p\)</span>. Using this embedding function, we can write the recurrent equations: <span class="math display">\[
Y_{i} = \frac{S_i \phi^p(Q_i)}{Z_i \phi^p(Q_i)} \qquad Z_i = Z_{i-1} + \phi^p(K_i)^T \qquad S_i = S_{i-1} + V_i \phi^p(K_i)^T
\]</span> where <span class="math inline">\(Z_0\)</span> and <span class="math inline">\(S_0\)</span> are <span class="math inline">\(\mathcal 0\)</span> vectors in their respective spaces. Since <span class="math inline">\(S_i \in \R^{d \times D}\)</span> and <span class="math inline">\(Z_i \in \R^{D}\)</span>, the size of the state is <span class="math inline">\(D(d+1)\)</span>.</p>
<p>These two forms give rise to a variety of algorithms for training linear transformers, with differing computational properties. <a href="https://manifestai.com/articles/linear-transformers-are-faster/">Read our earlier article on linear transformers for a detailed explanation.</a></p>
<section id="rotary-embeddings-rope" class="level3">
<h3 class="anchored" data-anchor-id="rotary-embeddings-rope">1.1 Rotary embeddings (RoPE)</h3>
<p>In our previous <a href="https://manifestai.com/articles/symmetric-power-transformers/">post</a> on symmetric power transformers, we briefly discussed that even symmetric power transformers were compatible with RoPE. In this section, we will give the issue the proper discussion it deserves by deriving the attention and recurrent implementations.</p>
<p>Rotary embeddings <span class="citation" data-cites="su2024roformer"><a href="#ref-su2024roformer" role="doc-biblioref">[7]</a></span> encode time information by rotating the keys and queries by an amount proportional to their corresponding timestep. The rotation matrix <span class="math inline">\(R\in \R^{d\times d}\)</span> tells us how much we want to rotate every timestep, so that: <span class="math display">\[
Q'_{i} = R^i Q_i \qquad K'_j = R^j K_j
\]</span> Then the preattention is changed to: <span class="math display">\[
B_{ij} = \left({Q'_{i}}^T K'_j \right)^p = \left({Q_{i}}^T (R^{i-j})^T K_j \right)^p
\qquad \text{(sympow rotary)}
\]</span> It is evident that the effect of rotation of the embeddings is <em>relative</em> because it modulates interaction between <span class="math inline">\(Q_i\)</span> and <span class="math inline">\(K_j\)</span> depending only on the time difference <span class="math inline">\(i-j\)</span>.</p>
<p>The rotation matrix <span class="math inline">\(R\)</span> is constructed in a particular way. We start with some range of rotation rates <span class="math inline">\(\theta_1, \theta_2, \cdots, \theta_{\frac d 2}\)</span> defined by the formula <span class="math inline">\(\theta_i = \frac{2\pi}{N^{\frac{2(i-1)}{d}}}\)</span>, where <span class="math inline">\(N\)</span> is the maximum document size. The vector <span class="math inline">\(\theta\)</span> contains these rotation rates. Then, the rotation matrix is <span class="math display">\[\small
R(\theta) = \begin{pmatrix}
\cos(\theta_1) &amp; -\sin(\theta_1) &amp; 0 &amp; 0 &amp; \cdots &amp; 0 &amp; 0 \\
\sin(\theta_1) &amp; \cos(\theta_1)  &amp; 0 &amp; 0 &amp; \cdots &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; \cos(\theta_2) &amp; -\sin(\theta_2) &amp; \cdots &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; \sin(\theta_2) &amp; \cos(\theta_2)  &amp; \cdots &amp; 0 &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; \cos(\theta_{d/2}) &amp; -\sin(\theta_{d/2}) \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; \sin(\theta_{d/2}) &amp; \cos(\theta_{d/2})
\end{pmatrix}
\]</span></p>
<p>When multiplying a query or key vector by this rotation matrix, each pair of dimensions indexed by <span class="math inline">\(2j - 1\)</span> and <span class="math inline">\(2j\)</span> for <span class="math inline">\(j \in \{1, 2, ..., \frac{d}{2} \}\)</span> is rotated by a different amount <span class="math inline">\(\theta_j\)</span>. Rotating each pair by a different angle helps break symmetry and increases the expressiveness of the positional encodings.</p>
<p>A computational advantage of using rotation matrices of this form is that <span class="math inline">\(R(\theta)^k = R(k \theta)\)</span>, which massively simplifies the cost of computing all the <span class="math inline">\(Q'_i\)</span> and <span class="math inline">\(K'_j\)</span>.</p>
<details>
<summary>
Expand to see a proof of this fact
</summary>
<p>We are given a block diagonal rotation matrix <span class="math inline">\(R(\theta)\)</span>, where each <span class="math inline">\(2 \times 2\)</span> block corresponds to a rotation by some angle <span class="math inline">\(\theta_i\)</span>:</p>
<p><span class="math display">\[
R(\theta) = \begin{pmatrix}
\cos(\theta_1) &amp; -\sin(\theta_1) &amp; 0 &amp; 0 &amp; \cdots &amp; 0 &amp; 0 \\
\sin(\theta_1) &amp; \cos(\theta_1)  &amp; 0 &amp; 0 &amp; \cdots &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; \cos(\theta_2) &amp; -\sin(\theta_2) &amp; \cdots &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; \sin(\theta_2) &amp; \cos(\theta_2)  &amp; \cdots &amp; 0 &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; \cos(\theta_{d/2}) &amp; -\sin(\theta_{d/2}) \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; \cdots &amp; \sin(\theta_{d/2}) &amp; \cos(\theta_{d/2})
\end{pmatrix}.
\]</span></p>
<p>We aim to prove that <span class="math inline">\(R(\theta)^k = R(k \theta)\)</span>, where <span class="math inline">\(k\)</span> is a positive integer, and <span class="math inline">\(k \theta = (k \theta_1, k \theta_2, \dots, k \theta_{d/2})\)</span>.</p>
<p>We prove this statement by induction on <span class="math inline">\(k\)</span> for a single <span class="math inline">\(2 \times 2\)</span> rotation matrix <span class="math inline">\(R(\theta_i)\)</span>, and then extend it to the full block diagonal matrix.</p>
<p>For the base case (k = 1), we have:</p>
<p><span class="math display">\[R(\theta_i)^1 = R(\theta_i)\]</span>,</p>
<p>which is equivalent to <span class="math inline">\(R(1 \cdot \theta_i) = R(\theta_i)\)</span>. Thus, the base case holds.</p>
<p>Assume that for some positive integer <span class="math inline">\(k\)</span>, the property holds:</p>
<p><span class="math display">\[R(\theta_i)^k = R(k \theta_i).\]</span></p>
<p>We need to show that <span class="math inline">\(R(\theta_i)^{k+1} = R((k+1)\theta_i)\)</span>. Using the definition of matrix exponentiation:</p>
<p><span class="math display">\[R(\theta_i)^{k+1} = R(\theta_i) R(\theta_i)^k.\]</span></p>
<p>By the inductive hypothesis, <span class="math inline">\(R(\theta_i)^k = R(k\theta_i)\)</span>. Substituting this:</p>
<p><span class="math display">\[R(\theta_i)^{k+1} = R(\theta_i) R(k \theta_i).\]</span></p>
<p>The product of two rotation matrices corresponds to a rotation by the sum of their angles. Therefore:</p>
<p><span class="math display">\[R(\theta_i) R(k \theta_i) = R(\theta_i + (k \theta_i)) = R((k+1)\theta_i).\]</span></p>
<p>Thus, <span class="math inline">\(R(\theta_i)^{k+1} = R((k+1)\theta_i)\)</span>, completing the inductive step. By induction, the property holds for all ( k ).</p>
<p></p>
<p>Consider the block diagonal matrix ( R() ), where: <span class="math display">\[
R(\theta) = \begin{pmatrix}
R(\theta_1) &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; R(\theta_2) &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; R(\theta_{d/2})
\end{pmatrix}.
\]</span></p>
<p>Since each block <span class="math inline">\(R(\theta_i)\)</span> is independent of the others, the <span class="math inline">\(k\)</span>-th power of <span class="math inline">\(R(\theta)\)</span> is the block diagonal matrix with each block raised to the <span class="math inline">\(k\)</span>-th power: <span class="math display">\[
R(\theta)^k = \begin{pmatrix}
R(\theta_1)^k &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; R(\theta_2)^k &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; R(\theta_{d/2})^k
\end{pmatrix}.
\]</span></p>
<p>Using the result for a single rotation matrix, <span class="math inline">\(R(\theta_i)^k = R(k\theta_i)\)</span>, we get: <span class="math display">\[
R(\theta)^k = \begin{pmatrix}
R(k \theta_1) &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; R(k \theta_2) &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; R(k \theta_{d/2})
\end{pmatrix}.
\]</span></p>
<p>This is equivalent to the block diagonal matrix <span class="math inline">\(R(k\theta)\)</span>, where <span class="math inline">\(k\theta = (k\theta_1, k\theta_2, \dots, k\theta_{d/2})\)</span>.</p>
<p>Thus, by induction and the block diagonal structure, <span class="math inline">\(R(\theta)^k = R(k\theta)\)</span> for any positive integer <span class="math inline">\(k\)</span>.</p>
</details>
<p>Now we want to find the recurrent formulation of rotary embeddings with symmetric power transformers. A simple way we can do that is by including one extra vector in the recurrent state which is now a tuple <span class="math inline">\((S, Z, \mu)\)</span>, where <span class="math inline">\(\mu \in \R^{\frac d 2}\)</span>. The recurrent equations are given by <!--
$$
Z_i = Z_{i-1} + \phi^p(K'_i)^T \qquad S_i = S_{i-1} + V_i \phi^p(K'_i)^T \qquad \mu_i = \mu_{i-1} + \theta
$$
--></p>
<p><span class="math display">\[
Z_i = Z_{i-1} + \phi^p(R(\mu_i) K_i)^T \qquad S_i = S_{i-1} + V_i \phi^p(R(\mu_i) K_i)^T \qquad \mu_i = \mu_{i-1} + \theta
\]</span></p>
<p>Note we rotate the keys by <span class="math inline">\(R(\mu_i)\)</span> before using them.</p>
<p>Given <span class="math inline">\(S_i\)</span> and <span class="math inline">\(Z_i\)</span>, the outputs are the same as before, except that we rotate the queries by <span class="math inline">\(R(\mu)\)</span> before using them: <span class="math display">\[
Y_{i} = \frac{S_i \phi^p( R(\mu_i) Q_i)}{Z_i \phi^p( R(\mu_i) Q_i)}
\]</span></p>
<details>
<summary>
Expand for a proof of the equivalence between the state and recurrent formulations
</summary>
<p>We begin by writing the output <span class="math inline">\(Y_i\)</span> at time step <span class="math inline">\(i\)</span> in the attention formulation. For notational simplicity, let <span class="math inline">\(C_i = \sum_{k=1}^i (Q_i'^T K_k')^p = \sum_{k=1}^i \phi^p(Q_i')^T \phi^p(K_k')\)</span>.</p>
<p><span class="math display">\[
\begin{align}
Y_i &amp;= \sum_{j=1}^i \frac{\left( Q_i^T (R^{i-j})^T K_j\right) ^p V_j}{C_i} \\
    &amp;= \sum_{j=1}^i \frac{\left( Q_i^T (R^i)^T R^j K_j\right)^p V_j}{C_i} \\
    &amp;= \sum_{j=1}^i \frac{\left(Q_i^T R(\mu_i)^T R(\mu_j) K_j\right)^p V_j}{C_i} \\
    &amp;= \sum_{j=1}^i \frac{\left((R(\mu_i) Q_i)^T R(\mu_j) K_j\right)^p V_j}{C_i} \\
    &amp;= \sum_{j=1}^i \frac{\left(\phi^p(R(\mu_i) Q_i)^T \phi^p(R(\mu_j) K_j)\right) V_j}{C_i} \\
    &amp;= \sum_{j=1}^i \frac{V_j \phi^p(R(\mu_j) K_j)^T \phi^p(R(\mu_i) Q_i)}{C_i} \\
    &amp;= \frac{\left( \sum_{j=1}^i V_j \phi^p(R(\mu_j) K_j)^T \right)  \phi^p(R(\mu_i) Q_i) }{C_i} \\
    &amp;= \frac{S_i  \phi^p(R(\mu_i) Q_i) }{Z_i \phi^p(R(\mu_i) Q_i)}
\end{align}
\]</span></p>
<p>which is the recurrent formulation of the output <span class="math inline">\(Y_i\)</span>. The last line above uses the fact that</p>
<p><span class="math display">\[
\sum_{j=1}^i V_j \phi^p(R(\mu_j) K_j)^T = S_i
\]</span></p>
<p>and</p>
<span class="math display">\[
\begin{align}
C_i &amp;= \sum_{k=1}^i \phi^p(Q_i')^T \phi^p(K_k') \\
    &amp;= \sum_{k=1}^i \phi^p(R(\mu_i) Q_i)^T \phi^p(R(\mu_k) K_k) \\
    &amp;= \sum_{k=1}^i \phi^p(R(\mu_k) K_k)^T \phi^p(R(\mu_i) Q_i) \\
    &amp;= Z_i \phi^p(R(\mu_i) Q_i)
\end{align}
\]</span>
</details>
</section>
</section>
<section id="gating" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="gating">2. Gating</h2>
<p>The basic idea of gating is that at each time step the state matrix <span class="math inline">\(S\in \R^{d \times D}\)</span> will be discounted by a scalar <span class="math inline">\(\gamma \in [0, 1]\)</span>. Discounting the state ``erases” past information stored in the state. This technique has been used extensively throughout the linear transformer literature <span class="citation" data-cites="peng2021random"><a href="#ref-peng2021random" role="doc-biblioref">[8]</a></span>, <span class="citation" data-cites="mao2022fine"><a href="#ref-mao2022fine" role="doc-biblioref">[9]</a></span>, <span class="citation" data-cites="katsch2023gateloop"><a href="#ref-katsch2023gateloop" role="doc-biblioref">[10]</a></span>. One common approach to implement gating is to manually pick a gating value for each head, usually using a range of large and small <span class="math inline">\(\gamma\)</span> for different heads to allow the model to keep track of short and long term interactions. But, naturally, the gating values can also be learnable parameters or even data dependent values, as has been thoroughly explored in prior work <span class="citation" data-cites="gu2023mamba"><a href="#ref-gu2023mamba" role="doc-biblioref">[1]</a></span>, <span class="citation" data-cites="yang2023gated"><a href="#ref-yang2023gated" role="doc-biblioref">[2]</a></span>, <span class="citation" data-cites="dao2024transformers"><a href="#ref-dao2024transformers" role="doc-biblioref">[3]</a></span>, <span class="citation" data-cites="sun2024you"><a href="#ref-sun2024you" role="doc-biblioref">[4]</a></span>.</p>
<p>After exploring with a few variations of gating, including fixed, learnable and data dependent versions, we ended up converging to the technique used in <span class="citation" data-cites="dao2024transformers"><a href="#ref-dao2024transformers" role="doc-biblioref">[3]</a></span>. The discount value at timestep <span class="math inline">\(i\)</span> is <span class="math inline">\(\gamma_i = \sigma(W_\gamma X_i)\)</span> where <span class="math inline">\(\sigma\)</span> refers to the sigmoid function, <span class="math inline">\(W_\gamma \in \mathbb{R}^{d \times 1}\)</span> and <span class="math inline">\(X_i\)</span> are the input sequence we used to compute the keys, queries, and values (e.g.&nbsp;<span class="math inline">\(K_i = W_K X_i\)</span>). When using symmetric power attention with power <span class="math inline">\(p\)</span>, the recurrent state update is simply <span class="math display">\[
Z_i = \gamma_i Z_{i-1} + \phi^p(K'_i)^T \qquad S_i = \gamma_i S_{i-1} + V_i \phi^p(K'_i)^T \qquad \mu_i = \mu_{i-1} + \theta
\]</span> Recall that <span class="math inline">\(K'_i = R(\mu_i) K_i\)</span>.</p>
<p>To write the attention formulation we define <span class="math inline">\(b_{ij} = \Pi_{k=j+1}^i \gamma_m\)</span>. Then, in the attention formulation, the preattention becomes <span class="math display">\[
B_{ij} = b_{ij} \; ( {Q'_i}^T K'_j)^p
\qquad \text{(sympow gated)}
\]</span></p>
<details>
<summary>
Expand for a derivation of the equivalence between the state and attention formulations
</summary>
<p>We begin by writing the output <span class="math inline">\(Y_i\)</span> at time step <span class="math inline">\(i\)</span> in the attention formulation. For notational simplicity, let <span class="math inline">\(C_i = \sum_{k=1}^i \beta_{ik} (Q_i'^T K_k')^p = \sum_{k=1}^i \beta_{ik} \phi^p(Q_i')^T \phi^p(K_k')\)</span>.</p>
<p><span class="math display">\[
\begin{align}
Y_i &amp;= \sum_{j=1}^i \frac{\beta_{ij} \left(Q_i'^T K'_j\right)^p V_j}{C_i} \\
    &amp;= \sum_{j=1}^i \frac{\beta_{ij} \left(\phi^p(Q'_i)^T \phi^p(K'_j)\right) V_j}{C_i} \\
    &amp;= \sum_{j=1}^i \frac{\beta_{ij} V_j \phi^p(K'_j)^T \phi^p(Q'_i)}{C_i} \\
    &amp;= \frac{\left( \sum_{j=1}^i \beta_{ij} V_j \phi^p(K'_j)^T \right)  \phi^p(Q'_i) }{C_i} \\
    &amp;= \frac{S_i  \phi^p(Q'_i) }{Z_i \phi^p(Q'_i)}
\end{align}
\]</span></p>
<p>which is the recurrent formulation of the output <span class="math inline">\(Y_i\)</span>. The last line above uses the fact that</p>
<p><span class="math display">\[
\sum_{j=1}^i \beta_{ij} V_j \phi^p(K'_j)^T = S_i
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\begin{align}
C_i &amp;= \sum_{k=1}^i \beta_{ik} \phi^p(Q_i')^T \phi^p(K_k') \\
    &amp;= \sum_{k=1}^i \beta_{ik} \phi^p(K_k')^T \phi^p(Q_i') \\
    &amp;= Z_i \phi^p(Q'_i)
\end{align}
\]</span></p>
<p>We prove that <span class="math inline">\(S_i = \sum_{j=1}^i \beta_{ij} V_j \phi^p(K'_j)^T\)</span> by induction.</p>
<p>As the base case, note that <span class="math inline">\(S_1 =  V_1 \phi^p(K'_1)^T\)</span></p>
For the inductive step, suppose that for <span class="math inline">\(k &gt; 1\)</span>, <span class="math inline">\(S_k = \sum_{j=1}^k \beta_{kj} V_j \phi^p(K'_j)^T\)</span>. Then <span class="math display">\[
\begin{align}
S_{k+1} &amp;= \gamma_{k+1} S_k + V_{k+1}\phi^p(K'_{k+1})^T \\
        &amp;= \gamma_{k+1} \left( \sum_{j=1}^k \beta_{kj} V_j \phi^p(K'_j)^T \right) + V_{k+1}\phi^p(K'_{k+1})^T \\
        &amp;= \left( \sum_{j=1}^k \beta_{(k+1)j} V_j \phi^p(K'_j)^T \right) + V_{k+1}\phi^p(K'_{k+1})^T \\
        &amp;= \sum_{j=1}^{k+1} \beta_{(k+1)j} V_j \phi^p(K'_j)^T
\end{align}
\]</span> This completes the inductive step.
</details>
<!--

In the recurrent form of a symmetric power transformer, we just need to modify the recurrent equation as:
$$
S_i = \gamma S_{i-1} + V_i \phi^p(K_i)^T
$$
In the corresponding attention formulation, the attention score $A_{ij}$ between the $i$-th query and $j$-th key is computed as follows:
$$
B_{ij} = \gamma^{(i-j)}  (Q_i^T K_j)^p \qquad \text{(sympow fixed gated)}
$$
The amount by which each key affects the output is determined by how far in the past the key is relative to the query. Since using a fixed $\gamma \in (0, 1)$ leads to exponential decay, the influence of keys from the distant past will be near zero. 

There are two reasonable ways we could try to implement gating:
* inverse gating on the keys. Keep attention the same
* similar to the mask

The second option is the one we could get to work. The seoncd suffers from numerical stability. Gating can also be implemented in a numerally stable way.
**TODO: Explain numerical stability. Add code snippet here.**

Rather than using fixed gating, allowing each head in the model to learn its own gating values can be beneficial. Some heads may learn to store information from the distant past whereas other heads may learn to only retain recent information. Allowing gating to be input-dependent allows for further flexibility since inputs will modulate how much information from the past should be retained or discarded.

-->
<p>Let’s see what difference gating makes, and whether it solves the issues we encountered in the intro. We can look at the loss at the end of training on 400k documents. As we grow the train context size, gated sympow stays better than the baseline as far as we were able to test it!</p>
<div class="column-body" style="text-align: center;">
<iframe width="600" height="410" src="plots/relative_loss_gating.html">
</iframe>
</div>
<!--
Improves optimization:

:::{.column-body style="text-align: center;"}
<iframe width="600" height="450" src="plots/train_loss_curves_gating.html"></iframe>
:::
-->
<p>We also see that after adding learned gating, the symmetric power transformer is able to successfully generalize past the train context size.</p>
<div class="column-body" style="text-align: center;">
<iframe width="600" height="400" src="plots/context_wise_scaling_plot_train_context_16k_gating.html">
</iframe>
</div>
</section>
<section id="learned-rotary-embeddings" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="learned-rotary-embeddings">3. Learned Rotary Embeddings</h2>
<p>We now explore an intuitive idea of learning the rotation rates in rotary positional embeddings. There are many ways we can approach this. For example, the network could independently decide how much to rotate each of the 2D subspaces. We found that an efficient way to implement learning rotation rates that results in performance improvements is for the network to scale the fixed <span class="math inline">\(\theta\)</span> vector that RoPE usually applies. Similar to gating, we add parameters <span class="math inline">\(W_\beta\)</span> to each attention head. The network outputs a scalar <span class="math inline">\(\beta_i = 1 + \text{tanh}(W_\beta X_i)\)</span> and multiplies this value with the original fixed vector <span class="math inline">\(\theta\)</span>. In the recurrent formulation, this produces the equation <span class="math display">\[
\mu_i = \mu_{i-1} + \beta_i \theta
\]</span> and in the attention formulation <span class="math display">\[
B_{ij} = \left(Q_i R(c_{ij}\theta) K_j \right)^p \qquad c_{ij} = \sum_{k=j+1}^i \beta_i  \qquad \text{(conformal-sympow)}
\]</span> For reasons discussed in Section 4, we refer to this approach as conformal-sympow.</p>
<p>We can see that learning the rotary embeddings in addition to learning gating values further improves performance over sympow+gating with fixed rotary embeddings:</p>
<div class="column-body" style="text-align: center;">
<iframe width="600" height="410" src="plots/relative_loss.html">
</iframe>
</div>
<p>and generalizes past the train context size:</p>
<div class="column-body" style="text-align: center;">
<iframe width="600" height="400" src="plots/context_wise_scaling_plot_train_context_16k.html">
</iframe>
</div>
<section id="full-training-curves" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="full-training-curves">Full Training Curves</h3>
<p>Here we display all the training curves for all methods using symmetric power <span class="math inline">\(p=2\)</span> and <span class="math inline">\(p=4\)</span> at different context lengths. We can see that sympow+gating and conformal-sympow improves optimization throughout training.</p>
<div class="column-body" style="text-align: center;">
<iframe width="600" height="450" src="plots/train_loss_curves_all.html">
</iframe>
</div>
</section>
</section>
<section id="conformal-state-transformations-optional-reading" class="level2">
<h2 class="anchored" data-anchor-id="conformal-state-transformations-optional-reading">4. Conformal State Transformations (Optional Reading)</h2>
<p>*Note: this section is conceptual and has no practical implications. In this section, we show that gating and rotations can be unified into a single mathematical idea: a conformal state transformation.</p>
<p>We refer to the combination of gating and rotary embeddings as conformal-sympow. The reason stems from the fact that the combination of gating and rotary embeddings can be interpreted as applying a conformal linear transformation to the state in the recurrent formulation. A <strong><em>conformal linear transformation</em></strong> is a type of linear transformation that preserves angles between vectors while allowing uniform scaling of lengths. Mathematically, a conformal linear transformation in <span class="math inline">\(n\)</span>-dimensional Euclidean space can be expressed as: <span class="math display">\[
\mathbf{T}(\mathbf{x}) = s \mathbf{R} \mathbf{x},
\]</span> where <span class="math inline">\(s &gt; 0\)</span> is a scalar representing the scaling factor, <span class="math inline">\(\mathbf{R}\)</span> is an orthogonal matrix <span class="math inline">\(\mathbf{R}^T \mathbf{R} = \mathbf{I}\)</span>, and <span class="math inline">\(\mathbf{x}\)</span> is the input vector.</p>
<p>We will show that updating the recurrent state of a sympow transformer by applying gating and rotary embeddings is equivalent to right multiplying the state with a conformal linear transformation before adding new information:</p>
<p><span class="math display">\[
S_{i} = S_{i-1} (s\mathbf{R}) + V_i \phi^p(K_i)^T
\]</span></p>
<p>When applying gating, we can see that the discount value <span class="math inline">\(\gamma\)</span> plays the role of <span class="math inline">\(s\)</span> in the above equation.</p>
<p>Now, recall that the recurrent state update when applying rotary embeddings is <span class="math display">\[
Z_i = Z_{i-1} + \phi^p(K'_i)^T \qquad S_i = S_{i-1} + V_i \phi^p(K'_i)^T \qquad \mu_i = \mu_{i-1} + \theta
\]</span> where <span class="math inline">\(K'_i = R(\mu_i) K_i\)</span>.</p>
<p>We will show that the update equation <span class="math inline">\(S_i = S_{i-1} + V_i \phi^p(R(\mu_i)K_i)^T\)</span> is equivalent to the following update equation: <span class="math display">\[
S_i = S_{i-1} \bar{R}(\theta) + V_i \phi^p(K_i)^T
\]</span> for some rotation matrix <span class="math inline">\(\bar{R}(\theta)\)</span>.</p>
<p>This equivalence stems from the following result. If <span class="math inline">\(P\in \R^{d\times d}\)</span> is a rotation matrix, then there exists another rotation matrix <span class="math inline">\(\bar P \in \R^{D \times D}\)</span> s.t. <span class="math display">\[
\phi^p( P k) = \bar P \phi^p(k)
\]</span></p>
<details>
<summary>
Expand for a proof of this fact
</summary>
<p>Note that the symmetric power embedding function is equivalent to applying the tensor product and removing redundant information resulting from symmetry. For mathematical simplicity, we prove the corresponding result for which the embedding function is the repeated tensor product <span class="math inline">\(\otimes^p\)</span>. The corresponding proposition is stated below.</p>
<p>Let <span class="math inline">\(V\)</span> be a vector space with dimension <span class="math inline">\(d\)</span> and basis vectors <span class="math inline">\(\{ v_1, v_2, \dots, v_d \}\)</span>, and let <span class="math inline">\(P \in \mathbb{R}^{d \times d}\)</span> be a rotation matrix. Define the linear map <span class="math inline">\(\bar{P} : V^{\otimes p} \to V^{\otimes p}\)</span> (the tensor product of <span class="math inline">\(p\)</span> copies of <span class="math inline">\(V\)</span>) by its action on the basis elements as <span class="math display">\[
\bar{P}(v_{i_1} \otimes v_{i_2} \otimes \dots \otimes v_{i_p}) = (P v_{i_1}) \otimes (P v_{i_2}) \otimes \dots \otimes (P v_{i_p}) \quad \text{for all } i_1, i_2, \dots, i_p.
\]</span> Then <span class="math inline">\(\bar{P} \in \mathbb{R}^{d^p \times d^p}\)</span> is a rotation matrix.</p>
<p>We need to show that <span class="math inline">\(\bar{P}\)</span> satisfies the properties of a rotation matrix, namely: 1. <span class="math inline">\(\bar{P}\)</span> is an orthogonal matrix, i.e., <span class="math inline">\(\bar{P}^T \bar{P} = I\)</span>. 2. <span class="math inline">\(\det(\bar{P}) = 1\)</span>, so that <span class="math inline">\(\bar{P}\)</span> represents a proper rotation.</p>
<p><strong>Step 1: Orthogonality of <span class="math inline">\(\bar{P}\)</span>.</strong></p>
<p>Since <span class="math inline">\(\bar{P}\)</span> is defined by its action on the basis elements of <span class="math inline">\(V^{\otimes k}\)</span> as <span class="math display">\[
\bar{P}(v_{i_1} \otimes v_{i_2} \otimes \dots \otimes v_{i_p}) = (P v_{i_1}) \otimes (P v_{i_2}) \otimes \dots \otimes (P v_{i_p}),
\]</span> and <span class="math inline">\(P\)</span> is an orthogonal matrix, i.e., <span class="math inline">\(P^T P = I_d\)</span>, where <span class="math inline">\(I_d\)</span> is the identity matrix in <span class="math inline">\(\mathbb{R}^{d}\)</span>, we need to verify that <span class="math inline">\(\bar{P}\)</span> preserves the inner product in the tensor product space. The inner product of two basis elements <span class="math inline">\(v_{i_1} \otimes v_{i_2} \otimes \dots \otimes v_{i_p}\)</span> and <span class="math inline">\(v_{j_1} \otimes v_{j_2} \otimes \dots \otimes v_{j_p}\)</span> in <span class="math inline">\(V^{\otimes p}\)</span> is given by: <span class="math display">\[
\langle v_{i_1} \otimes v_{i_2} \otimes \dots \otimes v_{i_p}, v_{j_1} \otimes v_{j_2} \otimes \dots \otimes v_{j_k} \rangle = \prod_{p=1}^{p} \langle v_{i_p}, v_{j_p} \rangle.
\]</span> Applying <span class="math inline">\(\bar{P}\)</span> to this inner product, we get: <span class="math display">\[
\langle \bar{P}(v_{i_1} \otimes \dots \otimes v_{i_p}), \bar{P}(v_{j_1} \otimes \dots \otimes v_{j_p}) \rangle = \prod_{p=1}^{p} \langle P v_{i_p}, P v_{j_p} \rangle.
\]</span> Since <span class="math inline">\(P\)</span> is orthogonal, we have <span class="math inline">\(\langle P v_{i_p}, P v_{j_p} \rangle = \langle v_{i_p}, v_{j_p} \rangle\)</span> for each <span class="math inline">\(p\)</span>. Therefore, <span class="math inline">\(\bar{P}\)</span> preserves the inner product, meaning that <span class="math inline">\(\bar{P}\)</span> is an orthogonal matrix, i.e., <span class="math inline">\(\bar{P}^T \bar{P} = I_{d^p}\)</span>.</p>
<p><strong>Step 2: Determinant of <span class="math inline">\(\bar{P}\)</span>.</strong></p>
Next, we show that <span class="math inline">\(\det(\bar{P}) = 1\)</span>. Since <span class="math inline">\(\bar{P} = P \otimes P \otimes \dots \otimes P\)</span> (a <span class="math inline">\(p\)</span>-fold tensor product of <span class="math inline">\(P\)</span> with itself), we can use the property of the determinant for tensor products of matrices. Specifically, if <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are square matrices, then: <span class="math display">\[
\det(A \otimes B) = \det(A)^{\dim(B)} \det(B)^{\dim(A)}.
\]</span> In our case, since <span class="math inline">\(\bar{P} = P \otimes P \otimes \dots \otimes P\)</span>, we have: <span class="math display">\[
\det(\bar{P}) = \det(P)^{p \cdot d}.
\]</span> Since <span class="math inline">\(P\)</span> is a rotation matrix in <span class="math inline">\(\mathbb{R}^d\)</span>, we know that <span class="math inline">\(\det(P) = 1\)</span>. Therefore: <span class="math display">\[
\det(\bar{P}) = 1^{p \cdot d} = 1.
\]</span> Thus, <span class="math inline">\(\bar{P}\)</span> is a proper rotation matrix.
</details>
<p>Using the above result, the conformal-sympow recurrent state update can be written as follows:</p>
<p><span class="math display">\[
Z_i = Z_{i-1} (\gamma_i \bar{R}(\theta, \beta_i)) + \phi^p(K_i)^T \qquad S_i = S_{i-1} (\gamma_i \bar{R}(\theta, \beta_i)) + V_i \phi^p(K_i)^T
\]</span> where <span class="math inline">\(\bar{R}(\theta, \beta_i)\)</span> is a rotation matrix that depends on the fixed rotation rates <span class="math inline">\(\theta\)</span> and the scalar <span class="math inline">\(\beta_i\)</span>.</p>
</section>
<section id="equivalance-between-gating-and-alibi-optional-reading" class="level2">
<h2 class="anchored" data-anchor-id="equivalance-between-gating-and-alibi-optional-reading">5. Equivalance between Gating and ALiBi (Optional Reading)</h2>
<p>Attention with Linear Biases (ALiBi) <span class="citation" data-cites="press2021train"><a href="#ref-press2021train" role="doc-biblioref">[6]</a></span> is a type of positional encoding that significantly improves the ability of softmax transformers to extrapolate to evaluation contexts longer than the training context size. ALiBi biases query-key attention scores with a penalty that is proportional to the distance between the query and key. We now show that ALiBi is equivalent to applying scalar gating.</p>
<p>In a softmax transformer, the attention scores are computed as <span class="math display">\[
A_{ij}  = \frac{B_{ij}}{\sum_{k=1}^i B_{ik}} \qquad B_{ij} = \text{exp}(Q_i^T K_j)
\qquad \text{(softmax)}
\]</span> Recall that we refer to <span class="math inline">\(A_{ij}\)</span> as the attention scores and <span class="math inline">\(B_{ij}\)</span> as the pre-attention scores.</p>
<p>The pre-attention scores after applying ALiBi are <span class="math display">\[
B_{ij} = \text{exp}(Q_i^T K_j + m(j -i))
\qquad \text{(softmax + ALiBi)}
\]</span> where <span class="math inline">\(0 &lt; m &lt; 1\)</span> is a head-specific value that is fixed before training.</p>
<p>Note that <span class="math display">\[\text{exp}(Q_i^T K_j + m(j - i)) = \gamma^{(i - j)} \text{exp}(Q_i^T K_j)\]</span> where <span class="math inline">\(\gamma = \text{exp}(-m)\)</span>. Since <span class="math inline">\(-m &lt; 0\)</span>, <span class="math inline">\(0 &lt; \gamma &lt; 1\)</span>. Thus, the application of ALiBi is equivalent to applying scalar gating.</p>



</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body" data-entry-spacing="0" role="list">
<div id="ref-gu2023mamba" class="csl-entry" role="listitem">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">A. Gu and T. Dao, <span>“Mamba: Linear-time sequence modeling with selective state spaces,”</span> <em>arXiv preprint arXiv:2312.00752</em>, 2023.</div>
</div>
<div id="ref-yang2023gated" class="csl-entry" role="listitem">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">S. Yang, B. Wang, Y. Shen, R. Panda, and Y. Kim, <span>“Gated linear attention transformers with hardware-efficient training,”</span> <em>arXiv preprint arXiv:2312.06635</em>, 2023.</div>
</div>
<div id="ref-dao2024transformers" class="csl-entry" role="listitem">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">T. Dao and A. Gu, <span>“Transformers are SSMs: Generalized models and efficient algorithms through structured state space duality,”</span> <em>arXiv preprint arXiv:2405.21060</em>, 2024.</div>
</div>
<div id="ref-sun2024you" class="csl-entry" role="listitem">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline">Y. Sun <em>et al.</em>, <span>“You only cache once: Decoder-decoder architectures for language models,”</span> <em>arXiv preprint arXiv:2405.05254</em>, 2024.</div>
</div>
<div id="ref-sun2023retentive" class="csl-entry" role="listitem">
<div class="csl-left-margin">[5] </div><div class="csl-right-inline">Y. Sun <em>et al.</em>, <span>“Retentive network: A successor to transformer for large language models,”</span> <em>arXiv preprint arXiv:2307.08621</em>, 2023.</div>
</div>
<div id="ref-press2021train" class="csl-entry" role="listitem">
<div class="csl-left-margin">[6] </div><div class="csl-right-inline">O. Press, N. A. Smith, and M. Lewis, <span>“Train short, test long: Attention with linear biases enables input length extrapolation,”</span> <em>arXiv preprint arXiv:2108.12409</em>, 2021.</div>
</div>
<div id="ref-su2024roformer" class="csl-entry" role="listitem">
<div class="csl-left-margin">[7] </div><div class="csl-right-inline">J. Su, M. Ahmed, Y. Lu, S. Pan, W. Bo, and Y. Liu, <span>“Roformer: Enhanced transformer with rotary position embedding,”</span> <em>Neurocomputing</em>, vol. 568, p. 127063, 2024.</div>
</div>
<div id="ref-peng2021random" class="csl-entry" role="listitem">
<div class="csl-left-margin">[8] </div><div class="csl-right-inline">H. Peng, N. Pappas, D. Yogatama, R. Schwartz, N. A. Smith, and L. Kong, <span>“Random feature attention,”</span> <em>arXiv preprint arXiv:2103.02143</em>, 2021.</div>
</div>
<div id="ref-mao2022fine" class="csl-entry" role="listitem">
<div class="csl-left-margin">[9] </div><div class="csl-right-inline">H. H. Mao, <span>“Fine-tuning pre-trained transformers into decaying fast weights,”</span> <em>arXiv preprint arXiv:2210.04243</em>, 2022.</div>
</div>
<div id="ref-katsch2023gateloop" class="csl-entry" role="listitem">
<div class="csl-left-margin">[10] </div><div class="csl-right-inline">T. Katsch, <span>“Gateloop: Fully data-controlled linear recurrence for sequence modeling,”</span> <em>arXiv preprint arXiv:2311.01927</em>, 2023.</div>
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{buckman2024,
  author = {Buckman, Jacob and Gelada, Carles and Kumar, Saurabh and
    Zhang, Sean},
  publisher = {Manifest AI},
  title = {Improving {Symmetric} {Power} {Transformers} with {Conformal}
    {Transformations}},
  date = {2024-12-10},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-buckman2024" class="csl-entry quarto-appendix-citeas" role="listitem">
<div class="">J.
Buckman, C. Gelada, S. Kumar, and S. Zhang, <span>“Improving Symmetric
Power Transformers with Conformal Transformations.”</span> Manifest AI,
Dec. 10, 2024.</div>
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="m-a-n-i-f-e-s-t/website" data-repo-id="R_kgDOLA6vSg" data-category="Announcements" data-category-id="DIC_kwDOLA6vSs4Cgv3x" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" data-loading="lazy" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>© 2024 Manifest AI</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>