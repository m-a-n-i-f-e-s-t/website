<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Jacob Buckman">
<meta name="author" content="Carles Gelada">
<meta name="author" content="Saurabh Kumar">
<meta name="dcterms.date" content="2024-09-21">

<title>Optimizing Symmetric Power Transformers – Manifest AI</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<link href="../../favicon.png" rel="icon" type="image/png">
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-V0D26E23Q3"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-V0D26E23Q3', { 'anonymize_ip': true});
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Optimizing Symmetric Power Transformers - Manifest AI">
<meta property="og:description" content="TODO">
<meta property="og:image" content="/thumbnails/symmetric-power-transformers.png">
<meta property="og:site_name" content="Manifest AI">
<meta name="twitter:title" content="Optimizing Symmetric Power Transformers - Manifest AI">
<meta name="twitter:description" content="TODO">
<meta name="twitter:image" content="https://manifestai.com/thumbnails/symmetric-power-transformers.png">
<meta name="twitter:creator" content="@manifest__ai">
<meta name="twitter:site" content="@manifest__ai">
<meta name="twitter:card" content="summary">
<meta name="citation_title" content="Optimizing Symmetric Power Transformers">
<meta name="citation_author" content="Jacob Buckman">
<meta name="citation_author" content="Carles Gelada">
<meta name="citation_author" content="Saurabh Kumar">
<meta name="citation_publication_date" content="2024-09-21">
<meta name="citation_cover_date" content="2024-09-21">
<meta name="citation_year" content="2024">
<meta name="citation_online_date" content="2024-09-21">
<meta name="citation_language" content="en">
<meta name="citation_publisher" content="Manifest AI">
<meta name="citation_reference" content="citation_title=Gated linear attention transformers with hardware-efficient training;,citation_author=Songlin Yang;,citation_author=Bailin Wang;,citation_author=Yikang Shen;,citation_author=Rameswar Panda;,citation_author=Yoon Kim;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_journal_title=arXiv preprint arXiv:2312.06635;">
<meta name="citation_reference" content="citation_title=Mamba: Linear-time sequence modeling with selective state spaces;,citation_author=Albert Gu;,citation_author=Tri Dao;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_journal_title=arXiv preprint arXiv:2312.00752;">
<meta name="citation_reference" content="citation_title=Transformers are SSMs: Generalized models and efficient algorithms through structured state space duality;,citation_author=Tri Dao;,citation_author=Albert Gu;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_journal_title=arXiv preprint arXiv:2405.21060;">
<meta name="citation_reference" content="citation_title=You only cache once: Decoder-decoder architectures for language models;,citation_author=Yutao Sun;,citation_author=Li Dong;,citation_author=Yi Zhu;,citation_author=Shaohan Huang;,citation_author=Wenhui Wang;,citation_author=Shuming Ma;,citation_author=Quanlu Zhang;,citation_author=Jianyong Wang;,citation_author=Furu Wei;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_journal_title=arXiv preprint arXiv:2405.05254;">
</head>

<body class="floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../m-logo-tight.png" alt="" class="navbar-logo">
    </a>
  </div>
          <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://discord.gg/aFsCgDraGP"> <i class="bi bi-discord" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/manifest__ai"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
   
  <ul>
  <li><a href="#gating" id="toc-gating" class="nav-link active" data-scroll-target="#gating">1. Gating</a>
  <ul class="collapse">
  <li><a href="#learned-gating" id="toc-learned-gating" class="nav-link" data-scroll-target="#learned-gating">Learned Gating</a></li>
  
  </ul></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Optimizing Symmetric Power Transformers</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Authors</div>
    <div class="quarto-title-meta-contents">
             <p>Jacob Buckman </p>
             <p>Carles Gelada </p>
             <p>Saurabh Kumar </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">September 21, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<div class="hidden">
<p><span class="math display">\[
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\sft}{\text{softmax}}
\newcommand{\List}{\text{List}}
\newcommand{\Seq}{\text{Seq}}
\newcommand{\SeqT}{\text{SeqT}}
\newcommand{\CSeqT}{\text{CSeqT}}
\newcommand{\Dist}{\text{Dist}}
\newcommand{\SM}{\text{SM}}
\newcommand{\Fn}{\text{Fn}}
\newcommand{\Tok}{\text{Tok}}
\newcommand{\Aij}{ A_{[i,j]}}
\newcommand{\ten}{\small\text{tensor}}
\newcommand{\sym}{\small\text{symmetric}}
\newcommand{\flat}[1]{\text{flat}\left(#1\right)}
\newcommand{\stab}{\text{stab}}
\newcommand{\orbit}{\text{orbit}}
\]</span></p>
</div>
<style>
summary {
  cursor: pointer;
  padding: 8px;
  background-color: #f0f0f0; /* Light grey background */
  border-radius: 8px; /* Rounded corners for consistency */
}

/* Hover state changes the background color to a lighter grey */
summary:hover {
  background-color: #e0e0e0; /* Lighter grey on hover */
}

/* Override hover effect when details is open */
details[open] summary:hover {
  background-color: #f0f0f0; /* Maintain the same color as the non-hover state */
}

details[open] summary {
  padding: 8px;
}

details {
  padding: 0; /* No padding when details is not open */
  border-radius: 8px; /* Rounded corners for consistency */
  background-color: #f0f0f0; /* Light grey background */
}

details[open] {
  padding: 8px; /* Padding inside the details for content alignment */
}
</style>
<p>In our previous article, we introduced symmetric power transformers, which are a variant of linear transformers with an embedding function based on the theory of symmetric tensors. We demonstrated that when using a context size of <span class="math inline">\(4096\)</span> on the LongCrawl64 dataset, the symmetric power transformer successfully closes the performance gap to classic softmax transformers using a tractably-small state. Specifically, at context size <span class="math inline">\(4096\)</span> a symmetric power transformer with <span class="math inline">\(p=4\)</span> satisfies two desired properties: (1) performance matches that of a softmax transformer baseline and (2) has a state size small enough to be tractable (<span class="math inline">\(\leq\)</span> 80 GB).</p>
<p>While having a small state size allows for efficient training and inference, this capacity constraint poses a fundamental limitation. A symmetric power transformer with small state size can store only a small fraction of the information that a softmax transformer stores. Consequently, a model which naively tries to incorporate all information from a context into its state will suffer from poor performance when the context size is large. In fact, we see that the <span class="math inline">\(p=4\)</span> symmetric power transformer no longer matches the softmax transformer performance when the context size is <span class="math inline">\(65536\)</span>. When using a smaller symmetric power (<span class="math inline">\(p=2\)</span>), the symmetric power transformer performance does not match that of a softmax performer even at the smaller context size of <span class="math inline">\(4096\)</span>.</p>

<div class="no-row-height column-margin column-container"><div class="">
<div style="margin-top: 100px;">
<p>Note that a symmetric power transformer with <span class="math inline">\(p=2\)</span> has a state size that is <span class="math inline">\(0.027\%\)</span> of the state size with <span class="math inline">\(p=4\)</span>.</p>
</div>
</div></div><p>TODO: Include 2 plots here. One shows the gap between symmetric power (p=4) and softmax with context size 65536. The second shows the gap between symmetric power (p=2) and softmax with context size 4096.</p>
<ul>
<li>X-axis: Train steps</li>
<li>Y-axis: Loss</li>
<li>Methods: softmax, sympow</li>
</ul>
<p>TODO: Include plot here. Purpose: show that relative loss increases as context size increases.</p>
<ul>
<li>X-axis: context size (values: 1024, 4096, 16384, 65536)</li>
<li>Y-axis: Relative loss (sympow / softmax) at 50k steps.</li>
<li>Methods: Softmax, Sym Pow with p=4</li>
<li>We expect to see that there is a linear increase in relative loss as context size increases multiplicatively.</li>
</ul>
<p>In this article, we explore two optimizations to the symmetric power transformer that more effectively maximize the utility of the limited state capacity. The first is learning a gating mechanism so that the model can decide information to store. Gating allows the model to selectively forget information that is no longer relevant, freeing up space for injesting new information.</p>
<p>The second is learning adjustments to rotary embeddings to decide in embedding space information should be stored. Applying rotations with varying magnitude may allow the model to store some information densely in embedding space while other information further apart, making information more easily retrievable.</p>
<p><strong>TODO: Write a section on the general formulation for transformations to the queries and keys in the attention formulation and the corresponding recurrent state update form.</strong></p>
<p>Desired properties for attention:</p>
<ul>
<li>Set symmetric computation</li>
<li>Translation invariance</li>
</ul>
<p>General formulation: something like <span class="math display">\[\alpha e^{i \theta}\]</span></p>
<p>Question: Where should such a section be placed?</p>
<section id="gating" class="level2">
<h2 class="anchored" data-anchor-id="gating">1. Gating</h2>
<p>In linear transformers, gating is a mechanism used to forget information in the model’s internal state. Gating provides two key benefits, particularly when the context size is large:</p>
<ul>
<li><strong>Efficient Memory Utilization.</strong> Given the limited capacity of a model’s internal state, it cannot retain all information from long sequences. Gating helps the model prioritize what information to retain and what information to discard, keeping only relevant information from past tokens.</li>
<li><strong>Improved Optimization.</strong> Without gating, the gradient of the loss with respect to individual token embeddings may become smaller as context length increases because the contributions of each token to the overall output are averaged over a larger number of tokens.</li>
</ul>
<p>In the simplest form of multiplicative gating, at each time step the state is multiplied by a fixed scalar <span class="math inline">\(\gamma \in (0, 1)\)</span>. In the recurrent form of a linear transformer, this can be written as the following: <span class="math display">\[
S_i = \gamma S_{i-1} + V_i \phi(K_i)^T
\]</span></p>
<p>In the corresponding attention formulation, the attention score <span class="math inline">\(A_{ij}\)</span> between the <span class="math inline">\(i\)</span>-th query and <span class="math inline">\(j\)</span>-th key is computed as follows: <span class="math display">\[
A_{ij}  = \frac{ \gamma^{(i-j)}  \phi(Q_i)^T \phi(K_j)}{\sum_{k=1}^i \gamma^{(i-k)} \phi(Q_i)^T \phi(K_k) }
\]</span> The amount by which each key affects the output is determined by how far in the past the key is relative to the query. Since using a fixed <span class="math inline">\(\gamma \in (0, 1)\)</span> leads to exponential decay, the influence of keys from the distant past will be near zero.</p>
<p>When using a symmetric power transformer with power <span class="math inline">\(p\)</span>, the attention score becomes <span class="math display">\[
A_{ij}  = \frac{ \gamma^{(i-j)}  (Q_i^T K_j)^p}{\sum_{k=1}^i \gamma^{(i-k)} (Q_i^T K_k)^p }
\]</span></p>
<p>The corresponding recurrent state update is <span class="math display">\[
S_i = \gamma S_{i-1} + V_i \phi_{\text{TP}}(K_i)^T
\]</span></p>
<p>where <span class="math inline">\(\phi_{\text{TP}}\)</span> is the symmetric power embedding function which applied tensor product <span class="math inline">\(p\)</span> times. That is, for a vector <span class="math inline">\(x \in \mathbb{R}^d\)</span>,</p>
<p><span class="math display">\[\phi_{\text{TP}}(x) = \text{flat} \left( x \otimes x \otimes x \dots \otimes x \right) = \text{flat} \left( \bigotimes_{i=1}^{p} x \right)\]</span></p>
<section id="learned-gating" class="level3">
<h3 class="anchored" data-anchor-id="learned-gating">Learned Gating</h3>
<p>Rather than using fixed gating, allowing each head in the model to learn its own gating values can be beneficial. Some heads may learn to store information from the distant past whereas other heads may learn to only retain recent information. Allowing gating to be input-dependent allows for further flexibility since inputs will modulate how much information from the past should be retained or discarded. Using learned rather than fixed gating values has been thoroughly explored in prior work <span class="citation" data-cites="dao2024transformers"><a href="#ref-dao2024transformers" role="doc-biblioref">[1]</a></span>, <span class="citation" data-cites="sun2024you"><a href="#ref-sun2024you" role="doc-biblioref">[2]</a></span>, <span class="citation" data-cites="gu2023mamba"><a href="#ref-gu2023mamba" role="doc-biblioref">[3]</a></span>, <span class="citation" data-cites="yang2023gated"><a href="#ref-yang2023gated" role="doc-biblioref">[4]</a></span>. In this article, we use input-dependent scalar gating, where at each time step, the value of gating to apply is a learned function of the input <span class="citation" data-cites="dao2024transformers"><a href="#ref-dao2024transformers" role="doc-biblioref">[1]</a></span>. We replace the fixed scalar <span class="math inline">\(\gamma\)</span> with an input-dependent scalar <span class="math inline">\(\gamma_i = \sigma(x_i W_\gamma)\)</span> where <span class="math inline">\(W_\gamma \in \mathbb{R}^{d \times 1}\)</span>.</p>
<p>When using symmetric power attention with power <span class="math inline">\(p\)</span>, the attention score is <span class="math display">\[
A_{ij}  = \frac{ (\Pi_{m=j+1}^i \gamma_m)  (Q_i^T K_j)^p}{\sum_{k=1}^i (\Pi_{m=k+1}^i \gamma_m) (Q_i^T K_k)^p }
\]</span></p>
<p>The recurrent state update is</p>
<p><span class="math display">\[
S_i = \gamma_i S_{i-1} + V_i \phi_\text{TP}(K_i)^T
\]</span></p>
<p>TODO: Include 2 plots here which whow show gating helps with both performance and with optimization. One shows the gap between symmetric power (p=4) with and without gating and softmax with context size 65536. The second shows the gap between symmetric power (p=2) with and without gating and softmax with context size 4096.</p>
<ul>
<li>X-axis: Train steps</li>
<li>Y-axis: Loss</li>
<li>Methods: softmax, sympow, sympow + gating</li>
</ul>
<form action="https://buttondown.email/api/emails/embed-subscribe/manifestai" method="post" target="popupwindow" onsubmit="window.open('https://buttondown.email/manifestai', 'popupwindow')" class="embeddable-buttondown-form">
  <label for="bd-email" style="font-weight:bold;margin-right:20px;margin-top:20px;">
  Subscribe to be notified of new posts:
  </label>
  <input style="width: 35%;" type="email" name="email" id="bd-email" placeholder="Email">
  
  <input style="width: 80px;" type="submit" value="Subscribe">
</form>
</section>

</section>

<div id="quarto-appendix" class="default"><section id="acknowledgments" class="level3 appendix"><h2 class="anchored quarto-appendix-heading">Acknowledgments</h2><div class="quarto-appendix-contents">

<p>We would like to thank …</p>



</div></section><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body" data-entry-spacing="0" role="list">
<div id="ref-dao2024transformers" class="csl-entry" role="listitem">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">T. Dao and A. Gu, <span>“Transformers are SSMs: Generalized models and efficient algorithms through structured state space duality,”</span> <em>arXiv preprint arXiv:2405.21060</em>, 2024.</div>
</div>
<div id="ref-sun2024you" class="csl-entry" role="listitem">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">Y. Sun <em>et al.</em>, <span>“You only cache once: Decoder-decoder architectures for language models,”</span> <em>arXiv preprint arXiv:2405.05254</em>, 2024.</div>
</div>
<div id="ref-gu2023mamba" class="csl-entry" role="listitem">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">A. Gu and T. Dao, <span>“Mamba: Linear-time sequence modeling with selective state spaces,”</span> <em>arXiv preprint arXiv:2312.00752</em>, 2023.</div>
</div>
<div id="ref-yang2023gated" class="csl-entry" role="listitem">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline">S. Yang, B. Wang, Y. Shen, R. Panda, and Y. Kim, <span>“Gated linear attention transformers with hardware-efficient training,”</span> <em>arXiv preprint arXiv:2312.06635</em>, 2023.</div>
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{buckman2024,
  author = {Buckman, Jacob and Gelada, Carles and Kumar, Saurabh},
  publisher = {Manifest AI},
  title = {Optimizing {Symmetric} {Power} {Transformers}},
  date = {2024-09-21},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-buckman2024" class="csl-entry quarto-appendix-citeas" role="listitem">
<div class="">J.
Buckman, C. Gelada, and S. Kumar, <span>“Optimizing Symmetric Power
Transformers.”</span> Manifest AI, Sep. 21, 2024.</div>
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="m-a-n-i-f-e-s-t/website" data-repo-id="R_kgDOLA6vSg" data-category="Announcements" data-category-id="DIC_kwDOLA6vSs4Cgv3x" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" data-loading="lazy" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>© 2024 Manifest AI</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>