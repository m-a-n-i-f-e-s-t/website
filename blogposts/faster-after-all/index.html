<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.54">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Jacob Buckman">
<meta name="author" content="Carles Gelada">
<meta name="dcterms.date" content="2024-01-05">

<title>Linear Transformers Are Faster After All – Manifest AI</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<link href="../../favicon.png" rel="icon" type="image/png">
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-V0D26E23Q3"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-V0D26E23Q3', { 'anonymize_ip': true});
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Linear Transformers Are Faster After All – Manifest AI">
<meta property="og:description" content="">
<meta property="og:image" content="https://manifestai.com/favicon.png">
<meta property="og:site_name" content="Manifest AI">
<meta name="citation_title" content="Linear Transformers Are Faster After All">
<meta name="citation_author" content="Jacob Buckman">
<meta name="citation_author" content="Carles Gelada">
<meta name="citation_publication_date" content="2024-01-05">
<meta name="citation_cover_date" content="2024-01-05">
<meta name="citation_year" content="2024">
<meta name="citation_online_date" content="2024-01-05">
<meta name="citation_language" content="en">
<meta name="citation_publisher" content="Manifest AI">
<meta name="citation_reference" content="citation_title=Transformers are rnns: Fast autoregressive transformers with linear attention;,citation_author=Angelos Katharopoulos;,citation_author=Apoorv Vyas;,citation_author=Nikolaos Pappas;,citation_author=François Fleuret;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_conference_title=International conference on machine learning;,citation_conference=PMLR;">
<meta name="citation_reference" content="citation_title=Efficient attention: Attention with linear complexities;,citation_author=Zhuoran Shen;,citation_author=Mingyuan Zhang;,citation_author=Haiyu Zhao;,citation_author=Shuai Yi;,citation_author=Hongsheng Li;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_conference_title=Proceedings of the IEEE/CVF winter conference on applications of computer vision;">
<meta name="citation_reference" content="citation_title=Flashattention-2: Faster attention with better parallelism and work partitioning;,citation_author=Tri Dao;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_journal_title=arXiv preprint arXiv:2307.08691;">
<meta name="citation_reference" content="citation_title=Exploring the limits of transfer learning with a unified text-to-text transformer;,citation_author=Colin Raffel;,citation_author=Noam Shazeer;,citation_author=Adam Roberts;,citation_author=Katherine Lee;,citation_author=Sharan Narang;,citation_author=Michael Matena;,citation_author=Yanqi Zhou;,citation_author=Wei Li;,citation_author=Peter J Liu;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_issue=1;,citation_volume=21;,citation_journal_title=The Journal of Machine Learning Research;,citation_publisher=JMLRORG;">
<meta name="citation_reference" content="citation_title=Attention is all you need;,citation_author=Ashish Vaswani;,citation_author=Noam Shazeer;,citation_author=Niki Parmar;,citation_author=Jakob Uszkoreit;,citation_author=Llion Jones;,citation_author=Aidan N Gomez;,citation_author=Łukasz Kaiser;,citation_author=Illia Polosukhin;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_volume=30;,citation_journal_title=Advances in neural information processing systems;">
<meta name="citation_reference" content="citation_title=Efficiently scaling transformer inference;,citation_author=Reiner Pope;,citation_author=Sholto Douglas;,citation_author=Aakanksha Chowdhery;,citation_author=Jacob Devlin;,citation_author=James Bradbury;,citation_author=Jonathan Heek;,citation_author=Kefan Xiao;,citation_author=Shivani Agrawal;,citation_author=Jeff Dean;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_volume=5;,citation_journal_title=Proceedings of Machine Learning and Systems;">
<meta name="citation_reference" content="citation_title=Triton: An intermediate language and compiler for tiled neural network computations;,citation_author=Philippe Tillet;,citation_author=Hsiang-Tsung Kung;,citation_author=David Cox;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_conference_title=Proceedings of the 3rd ACM SIGPLAN international workshop on machine learning and programming languages;">
<meta name="citation_reference" content="citation_title=Efficient transformers: A survey;,citation_author=Yi Tay;,citation_author=Mostafa Dehghani;,citation_author=Dara Bahri;,citation_author=Donald Metzler;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_issue=6;,citation_doi=10.1145/3530811;,citation_volume=55;,citation_journal_title=ACM Computing Surveys;,citation_publisher=ACM;">
<meta name="citation_reference" content="citation_title=Rethinking attention with performers;,citation_author=Krzysztof Choromanski;,citation_author=Valerii Likhosherstov;,citation_author=David Dohan;,citation_author=Xingyou Song;,citation_author=Andreea Gane;,citation_author=Tamas Sarlos;,citation_author=Peter Hawkins;,citation_author=Jared Davis;,citation_author=Afroz Mohiuddin;,citation_author=Lukasz Kaiser;,citation_author=others;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_journal_title=arXiv preprint arXiv:2009.14794;">
<meta name="citation_reference" content="citation_title=Cosformer: Rethinking softmax in attention;,citation_author=Zhen Qin;,citation_author=Weixuan Sun;,citation_author=Hui Deng;,citation_author=Dongxu Li;,citation_author=Yunshen Wei;,citation_author=Baohong Lv;,citation_author=Junjie Yan;,citation_author=Lingpeng Kong;,citation_author=Yiran Zhong;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_journal_title=arXiv preprint arXiv:2202.08791;">
<meta name="citation_reference" content="citation_title=Random feature attention;,citation_author=Hao Peng;,citation_author=Nikolaos Pappas;,citation_author=Dani Yogatama;,citation_author=Roy Schwartz;,citation_author=Noah A Smith;,citation_author=Lingpeng Kong;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_journal_title=arXiv preprint arXiv:2103.02143;">
<meta name="citation_reference" content="citation_title=The devil in linear transformer;,citation_author=Zhen Qin;,citation_author=Xiaodong Han;,citation_author=Weixuan Sun;,citation_author=Dongxu Li;,citation_author=Lingpeng Kong;,citation_author=Nick Barnes;,citation_author=Yiran Zhong;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_journal_title=arXiv preprint arXiv:2210.10340;">
<meta name="citation_reference" content="citation_title=Transformer quality in linear time;,citation_author=Weizhe Hua;,citation_author=Zihang Dai;,citation_author=Hanxiao Liu;,citation_author=Quoc Le;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_conference_title=International conference on machine learning;,citation_conference=PMLR;">
<meta name="citation_reference" content="citation_title=Retentive network: A successor to transformer for large language models;,citation_author=Yutao Sun;,citation_author=Li Dong;,citation_author=Shaohan Huang;,citation_author=Shuming Ma;,citation_author=Yuqing Xia;,citation_author=Jilong Xue;,citation_author=Jianyong Wang;,citation_author=Furu Wei;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_journal_title=arXiv preprint arXiv:2307.08621;">
<meta name="citation_reference" content="citation_title=Gated linear attention transformers with hardware-efficient training;,citation_author=Songlin Yang;,citation_author=Bailin Wang;,citation_author=Yikang Shen;,citation_author=Rameswar Panda;,citation_author=Yoon Kim;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_journal_title=arXiv preprint arXiv:2312.06635;">
<meta name="citation_reference" content="citation_title=A cheap linear attention mechanism with fast lookups and fixed-size representations;,citation_author=Alexandre Brébisson;,citation_author=Pascal Vincent;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;,citation_journal_title=arXiv preprint arXiv:1609.05866;">
<meta name="citation_reference" content="citation_title=Efficient attention: Attention with linear complexities;,citation_author=Zhuoran Shen;,citation_author=Mingyuan Zhang;,citation_author=Haiyu Zhao;,citation_author=Shuai Yi;,citation_author=Hongsheng Li;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_conference_title=Proceedings of the IEEE/CVF winter conference on applications of computer vision;">
</head>

<body class="floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../m-logo-tight.png" alt="" class="navbar-logo">
    </a>
  </div>
          <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/manifest__ai"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
   
  <ul>
  <li><a href="#linear-transformers" id="toc-linear-transformers" class="nav-link active" data-scroll-target="#linear-transformers">1. Linear Transformers</a></li>
  <li><a href="#parallel-implementations" id="toc-parallel-implementations" class="nav-link" data-scroll-target="#parallel-implementations">2. Parallel Implementations</a></li>
  <li><a href="#chunked-formulation" id="toc-chunked-formulation" class="nav-link" data-scroll-target="#chunked-formulation">3. Chunked Formulation</a></li>
  <li><a href="#sampling" id="toc-sampling" class="nav-link" data-scroll-target="#sampling">4. Sampling</a></li>
  <li><a href="#learning-performance" id="toc-learning-performance" class="nav-link" data-scroll-target="#learning-performance">5. Learning Performance</a>
  <ul class="collapse">
  
  
  </ul></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Linear Transformers Are Faster After All</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Authors</div>
    <div class="quarto-title-meta-contents">
             <p>Jacob Buckman </p>
             <p>Carles Gelada </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 5, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p><span class="math display">\[
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\sft}{\text{softmax}}
\newcommand{\List}{\text{List}}
\newcommand{\Seq}{\text{Seq}}
\newcommand{\SeqT}{\text{SeqT}}
\newcommand{\CSeqT}{\text{CSeqT}}
\newcommand{\Dist}{\text{Dist}}
\newcommand{\SM}{\text{SM}}
\newcommand{\Fn}{\text{Fn}}
\newcommand{\Tok}{\text{Tok}}
\newcommand{\Aij}{ A_{[i,j]}}
\]</span></p>
<p>It is well-known that removing the exponential from the attention layer of a transformer allows for a recurrent reformulation, with computational cost that is linear instead of quadratic on context length <span class="citation" data-cites="katharopoulos2020transformers"><a href="#ref-katharopoulos2020transformers" role="doc-biblioref">[1]</a></span>. One would expect that such an architecture would be far faster to train, especially when the context size is large. However, when training deep neural networks on hardware accelerators (e.g.&nbsp;GPUs), reductions in FLOPs do not always straightforwardly translate into practical speedups. Initial empirical experiments showed that large language models based on linear transformers train more slowly than classic transformers, and led many to dismiss the approach as nice-in-theory but unhelpful-in-practice <span class="citation" data-cites="Tay2022Efficient"><a href="#ref-Tay2022Efficient" role="doc-biblioref">[2]</a></span>.</p>
<p>At the moment, the conventional wisdom in the field is that a quadratic-cost algorithm with highly optimized hardware utilization (e.g.&nbsp;FlashAttention) gives the best training throughput <span class="citation" data-cites="dao2023flashattention"><a href="#ref-dao2023flashattention" role="doc-biblioref">[3]</a></span>. But this is mistaken. In this post, we explain several different ways of implementing linear transformers and we show that, in fact, they can produce massive speed-ups.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p><strong>Experimental setup.</strong> Each timing experiment was run on an H100 with batch size 1 and vocabulary size 50k. We start with an exact JAX replication of GPT2 (numerically tested against <a href="https://github.com/karpathy/nanoGPT">nanoGPT</a>), and modify only the self-attention layers. Our implementation also includes modern bells-and-whistles such as mixed-precision training.</p>
</div></div><p>The experiment below showcases the central takeaway. We compare the speed of an optimized transformer, a straightforward recurrent implementation of the linear transformer (as described in <span class="citation" data-cites="katharopoulos2020transformers"><a href="#ref-katharopoulos2020transformers" role="doc-biblioref">[1]</a></span>), and an optimized linear transformer (described in Section 3). We see that, despite exhibiting better growth with respect to context size, the linear transformer trains much more slowly than the transformer in wall-clock time. In contrast, our algorithm is strictly faster than even the highly-optimized FlashAttention baseline, at all context and model sizes. At moderately large context sizes, this speedup has an larger impact than FlashAttention itself.</p>
<div class="column-body">
<iframe width="700" height="500" src="plots/chunked_vs_baselines.html">
</iframe>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<p>As we increase the context size, we utilize more and more GPU DRAM, until eventually we encounter an out of memory (OOM) error and cannot run a training step anymore. Different algorithms have different memory requirements (for example, FlashAttention utilizes dramatically less memory than traditional attention), so the various algorithms we tested have different ranges on this plot.</p>
</div></div><p>But speed is only one part of the picture. We also need to know whether linear transformers actually minimize the loss as well as standard transformers do. Unfortunately, as the next plot shows, directly converting GPT2 to use linear transformer layers hurts learning significantly.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p><strong>Experimental setup.</strong> These experiments are each run on an 8xH100 node for 24 hours. For all runs, we use flash-attention. We use the chunked algorithm with selecting optimal chunk size (see Section 3) for linear transformer runs. The dataset used was c4 <span class="citation" data-cites="c4"><a href="#ref-c4" role="doc-biblioref">[4]</a></span>, tokenized by <a href="https://github.com/openai/tiktoken">tiktoken</a>’s GPT2 encoder. We plot the train loss because the dataset is large enough that all training was done in the single-epoch setting, and so there is no difference between train and heldout loss.</p>
</div></div><div class="column-body">
<iframe width="700" height="500" src="plots/loss_over_time.html">
</iframe>
</div>
<p>These results are discussed in detail in Section 5, but in short: linear transformers seem to become increasingly unstable as the sequence length grows, negatively affecting learning. This means that our linear transformers are somewhat useless,<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> as the positive impact from the speedup seen in long contexts is undermined by the negative impact of degraded learning.</p>
<p>Other variants of linear transformers have been proposed that claim resolve these learning issues <span class="citation" data-cites="choromanski2020rethinking qin2022cosformer peng2021random qin2022devil hua2022transformer sun2023retentive yang2023gated"><a href="#ref-choromanski2020rethinking" role="doc-biblioref">[5]</a>–<a href="#ref-yang2023gated" role="doc-biblioref">[11]</a></span>, but we do not survey them today. Since our main motivation in this post is to share our insights on how to implement efficient linear transformers we stick with the most natural variant, which is most closely analogous to standard transformers. In a future post, we will explain how to improve the learning of linear transformers, allowing us to efficiently train unprecedentedly-long-context language models with super-fast sampling.</p>
<section id="linear-transformers" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="linear-transformers">1. Linear Transformers</h2>
<p>The inputs to a transformer layer are sequences of <span class="math inline">\(Q_i, K_i, V_i \in \R^d\)</span> of query, key and values, where <span class="math inline">\(i\)</span> ranges from <span class="math inline">\(1\)</span> to the sequence length <span class="math inline">\(t\)</span>. The outputs are a sequence <span class="math inline">\(Y_i\in \R^d\)</span>. The well-known formula for the transformer layer, first popularized by Vaswani et al <span class="citation" data-cites="vaswani2017attention"><a href="#ref-vaswani2017attention" role="doc-biblioref">[12]</a></span>, is: <span class="math display">\[
Y_i^\text{Transformer} = \sum_{j=1}^i e^{Q^T_i K_j} V_j
\]</span> Here we are excluding the denominator of the softmax for simplicity of notation. Although the normalization provided by the denominator is important for good learning performance, it doesn’t have a major impact on the computational cost or implementation speed, which are our main focus here.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>Even though we omit the normalization for the mathematical exposition, it is included in our implementation for all experiments.</p>
</div></div><p>The formula for the <em>linear transformer</em> (LT) layer is quite similar: just change the term <span class="math inline">\(e^{Q^T_i K_j} \to  Q^T_i K_j\)</span> yielding <span class="math display">\[
Y_i^\text{LinearTransformer} = \sum_{j=1}^i Q^T_i K_j V_j
\]</span></p>

<div class="no-row-height column-margin column-container"><div class="">
<p>All our experiments with linear transformers also include a normalization factor, which we empirically found to be important for learning performance. Drawing on <span class="citation" data-cites="katharopoulos2020transformers"><a href="#ref-katharopoulos2020transformers" role="doc-biblioref">[1]</a></span>, we divide each <span class="math inline">\(Y_i\)</span> by <span class="math inline">\(\sum_{j=1}^i Q^T_i K_j\)</span> after eunsuring the sum is positive by making keys and queries live in the positive quadrant using <code>softplus</code>.</p>
</div></div><p>This layer is “linear” in that the outputs <span class="math inline">\(Y\)</span> are linearly related to all of <span class="math inline">\(Q\)</span>, <span class="math inline">\(K\)</span>, and <span class="math inline">\(V\)</span>.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> From now on, we will omit the superscript of <span class="math inline">\(Y_i^\text{LinearTransformer}\)</span> and just write <span class="math inline">\(Y_i\)</span>. To begin our exploration of the computational cost of linear transformers, consider the following implementation.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> LT_attention(Q, K, V):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Shapes of inputs are</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co">     Q: [t, d]  K: [t, d]  V: [t, d]</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Shapes of outputs are</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">     Y: [t, d]</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    t, d <span class="op">=</span> Q.shape</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    Y_list <span class="op">=</span> []</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(t):           <span class="co"># loop cost: O(t^2 d)</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>        Y_i <span class="op">=</span> zeros(d)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>        Q_i <span class="op">=</span> Q[i]</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(i):       <span class="co"># loop cost: O(id)</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>            A_ij <span class="op">=</span> inner(K[j], Q_i)  <span class="co"># cost: O(d)</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>            Y_i <span class="op">+=</span> A_ij <span class="op">*</span> V[j]   <span class="co"># cost: O(d)</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>        Y_list.append(Y_i)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> stack(Y_list)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Anyone who has worked with self-attention will recognize that this is a terrible implementation, since nested for-loops are not GPU-friendly. Don’t worry: in the next section, we will discuss implementations that parallelize computation, and thus run much faster on modern hardware. For now, the main point of this pseudocode is to highlight that first mode of computation, which we call <em>attention</em> formulation, has a FLOP cost of <span class="math inline">\(O(t^2 d)\)</span>.</p>
<p>The key to the massive speedups we saw in the introduction comes from leveraging linearity to restructure the computation. Consider the following factorization: <span class="math display">\[
Y_i = \sum_{j=1}^i Q^T_i K_j V_j = \underbrace{ \left (  \sum_{j=1}^i V_j  K_j^T\right )}_{S_i} \; \; Q_i
\]</span> Written in this form, we notice that the term labeled <span class="math inline">\(S_i \in \R^{d\times d}\)</span> can be thought of as a state summarizing all the relevant information up to time <span class="math inline">\(i\)</span>. It’s easy to rewrite into the following recurrent equations <span class="math display">\[
Y_{i} = S_i Q_i
\;\;\;\;\;\;\;\;\;\;\;\;
S_i = S_{i-1} + V_i K_i^T
\]</span> where we assume <span class="math inline">\(S_{0} = 0\in \R^{d\times d}\)</span>. Written in this form, we realize that a linear transformer is an RNN. We can also write pseudocode for this approach, which we call the <em>state</em> formulation, and analyze the cost:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> LT_state(Q, K, V):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Shapes of inputs are</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co">     Q: [t, d]  K: [t, d]  V: [t, d]</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Shapes of outputs are</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co">     Y: [t, d]</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    t, d <span class="op">=</span> Q.shape</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    S_i <span class="op">=</span> zeros(d, d) <span class="co"># shape [d,d]</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    Y_list <span class="op">=</span> []</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(t):        <span class="co"># loop cost: O(t d^2)</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>        S_i <span class="op">+=</span> outer(K[i], V[i]) <span class="co"># cost: O(d^2)</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>        Y_i <span class="op">=</span> S_i <span class="op">@</span> Q[i]      <span class="co"># cost: O(d^2)</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>        Y_list.append(Y_i)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> stack(Y_list)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>We see that the cost here is <span class="math inline">\(O(t d^2)\)</span>.</p>
<p>So, while a standard transformer layer always has cost <span class="math inline">\(O(t^2 d)\)</span>, linear transformers have two formulations with different costs. By switching from the attention formulation to the state formulation, we can change the cost from <span class="math inline">\(O(t^2 d)\)</span> to <span class="math inline">\(O(t d^2)\)</span>, trading a <span class="math inline">\(t\)</span> term for a <span class="math inline">\(d\)</span> term.</p>
</section>
<section id="parallel-implementations" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="parallel-implementations">2. Parallel Implementations</h2>
<p>In general, for-loops are a terrible way to implement anything that will run on a GPU. When using high-level frameworks like PyTorch or JAX, the easiest way to get high GPU utilization is to only use primitives that are already highly optimized for GPU: matrix multiplication, elementwise operations, etc. We can rewrite our attention and state algorithms in this style to make them efficient.</p>
<p>First, let’s do this for attention. Our main technique is to compute the attention matrix <span class="math inline">\(A\)</span>, which contains all the terms <code>outer(Q[i], K[j])</code> that appeared inside the for-loops of <code>LT_attention</code>, using a single heavyweight matrix multiply.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> LT_attention_parallel_no_flash(Q, K, V):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Shapes of inputs are</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co">     Q: [t, d]  K: [t, d]  V: [t, d]</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Shapes of outputs are</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co">     Y: [t, d]</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    t <span class="op">=</span> Q.shape[<span class="dv">0</span>]</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    M <span class="op">=</span> causal_mask(t)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    A_raw <span class="op">=</span> Q <span class="op">@</span> K.T  <span class="co"># cost O(t^2 d)</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    A <span class="op">=</span> A_raw <span class="op">*</span> M    <span class="co"># cost O(t^2)</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    Y <span class="op">=</span> A <span class="op">@</span> V        <span class="co"># cost O(t^2 d)</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Y</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This implementation lies at the core of nearly every transformer of the past five years, powering all of the AI models that have recently exploded in popularity. Since it is composed of such a small number of ultra-parallelizable ops, it obtains high GPU utilization. Recently, specialized <em>flash attention</em> kernels <span class="citation" data-cites="dao2023flashattention"><a href="#ref-dao2023flashattention" role="doc-biblioref">[3]</a></span> have been used to get even further speedups by avoiding explicitly storing the attention matrix <span class="math inline">\(A\)</span>, and thereby saving both memory and time spent on memory-transfers. Algorithmically, though, flash attention is a variant of parallel attention, and has the same computational cost (as measured in FLOPs). We use <code>LT_attention_parallel</code> to refer to the flash attention implementation.</p>
<p>Next, let’s parallelize the recurrent formulation. This optimization is less well-known. The key is to compute all the terms <span class="math inline">\(V_i K^T_i\)</span> in parallel, and then use a cumulative-sum, which can be <a href="https://en.wikipedia.org/wiki/Prefix_sum">parallelized</a>, to combine them.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> LT_state_parallel(Q, K, V):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Shapes of inputs are</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co">     Q: [t, d]  K: [t, d]  V: [t, d]</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Shapes of outputs are</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co">     Y: [t, d]</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    P <span class="op">=</span> V[:,:,<span class="va">None</span>] <span class="op">@</span> K[:,<span class="va">None</span>,:]  <span class="co"># cost: O(t d^2)</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    S <span class="op">=</span> cumsum(P, axis<span class="op">=</span><span class="dv">0</span>)          <span class="co"># cost: O(log_2(t) t d^2)</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    Y <span class="op">=</span> S <span class="op">@</span> Q[:,:,<span class="va">None</span>]            <span class="co"># cost: O(t d^2)</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Y[:,:<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The cost in FLOPs of this algorithm is <span class="math inline">\(O(\log_2(t) t d^2)\)</span>.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
<p>Now that we have four mathematically-equivalent implementations of a linear transformer, let’s time the training step of our GPT2 models and see how big of a difference it makes. For our <code>LT_attention_parallel</code> implementation, we use a custom linear self-attention flash kernel we implemented in Triton <span class="citation" data-cites="tillet2019triton"><a href="#ref-tillet2019triton" role="doc-biblioref">[13]</a></span> based on <a href="https://triton-lang.org/main/getting-started/tutorials/06-fused-attention.html">OpenAI’s FlashAttention2 implementation</a>.</p>
<div class="column-body">
<iframe width="700" height="500" src="plots/four_algos.html">
</iframe>
</div>
<p>Here are some takeaways:</p>
<ul>
<li>As expected, the <code>attention</code> variants all have a quadratic asymptotic cost (slope of 2 on a log-log plot<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>). The <code>state</code> variants all have linear asymptotic cost (slope 1). <a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></li>
<li><code>LT_state_parallel</code> is an order-of-magnitude faster than <code>LT_state</code>.</li>
<li><code>LT_attention_parallel_no_flash</code> is two orders-of-magnitude faster than <code>LT_attention</code>.</li>
<li><code>LT_attention_parallel</code> seems to asymptotically stabilize into being an order-of-magnitude faster than <code>LT_attention_parallel_no_flash</code>.</li>
<li>For the majority of settings, <code>LT_attention_parallel</code> is the fastest. (This is the linear version of the algorithm used by the standard transformer.)</li>
<li>Parallel attention is the fastest algorithm for small context sizes. However, <code>LT_state_parallel</code> overcomes <code>LT_attention_parallel_no_flash</code> at around 13k context size, and overcomes <code>LT_attention_parallel</code> at around 100k.</li>
</ul>
<p>Overall, these results paint a clear picture: use the attention algorithm for small contexts and the state algorithm for large contexts. But do we really face a binary choice? Can we combine the state and attention ideas and get the best of both words?</p>
</section>
<section id="chunked-formulation" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="chunked-formulation">3. Chunked Formulation</h2>
<p>It’s evident that, for small context sizes, computing the <span class="math inline">\(t\)</span> by <span class="math inline">\(t\)</span> attention matrix is much more efficient than computing many <span class="math inline">\(d\)</span> by <span class="math inline">\(d\)</span> state matrices. But as <span class="math inline">\(t\)</span> grows, there is a point where the quadratic cost of the attention matrix ends up dominating. Noticing that attention is extremely efficient for small <span class="math inline">\(t\)</span> and that states are necessary for large <span class="math inline">\(t\)</span> motivates doing one last reworking of the LT equation.</p>
<p>Let <span class="math inline">\(c \in \N\)</span> be a positive integer that we’ll call the <em>chunk size</em>. For any <span class="math inline">\(i\in \N\)</span> find the unique <span class="math inline">\(n\in \Z\)</span> s.t. <span class="math inline">\(cn &lt; i \le c(n+1)\)</span>. We can easily see that the following equations are equivalent to the previous ones. <span class="math display">\[
Y_{i} = S_{cn}Q_i + \sum_{j=cn+1}^i Q_i^T K_j V_j
\;\;\;\;\;\;\;\;\;\;\;\;
S_{c(n+1)} = S_{cn} + \sum_{j=cn+1}^{c(n+1)} V_j K_j^T
\]</span> The key idea is that we are only going to compute a subset of all states: <span class="math inline">\(S_0, S_c, S_{2c}, \cdots\)</span>. Then, to compute each output <span class="math inline">\(Y_i\)</span>, we need only to take into account the contribution via the most recent state <span class="math inline">\(S_{cn}\)</span>, as well as the contribution (computed via attention) of all moments in time <span class="math inline">\(j\)</span> in the range <span class="math inline">\(cn &lt; j \le i\)</span>.</p>
<p>As pseudocode, this looks like:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> LT_attention_with_initial_state(S, Q, K, V):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Shapes of inputs are</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co">     S: [d, d]  Q: [c, d]  K: [c, d]  V: [c, d]</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Shapes of outputs are</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="co">     Y: [c, d]</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    Y_state <span class="op">=</span> Q <span class="op">@</span> S                               <span class="co"># cost O(c d^2)</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    Y_attention <span class="op">=</span> LT_attention_parallel(Q, K, V)  <span class="co"># cost O(c^2 d)</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    Y <span class="op">=</span> Y_state <span class="op">+</span> Y_attention                     <span class="co"># cost O(cd)</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Y</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> LT_chunked(Q, K, V, c):</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="co">    Shapes of inputs are</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="co">     Q: [t, d]  K: [t, d]  V: [t, d], c: int</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="co">    Shapes of outputs are</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a><span class="co">     Y: [t, d]</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>    t, d <span class="op">=</span> Q.shape</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> t <span class="op">%</span> c <span class="op">==</span> <span class="dv">0</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>    Q_, K_, V_ <span class="op">=</span> [arr.reshape(t<span class="op">//</span>c, c, d)</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>    `               <span class="cf">for</span> arr <span class="kw">in</span> [Q,K,V]]</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>    P_ <span class="op">=</span> K_.transpose([<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">1</span>]) <span class="op">@</span> V_  <span class="co"># cost O(t d^2)</span></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>    S_ <span class="op">=</span> cumsum(P_, axis<span class="op">=</span><span class="dv">0</span>) <span class="op">-</span> P_     <span class="co"># cost O(log_2(t/c)(t/c)d^2)</span></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>    Y_ <span class="op">=</span> vmap(LT_attention_with_initial_state, axis<span class="op">=</span><span class="dv">0</span>)(</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>                S_, Q_, K_, V_)      <span class="co"># cost O(td^2 + tcd)</span></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Y_.reshape(t, d)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The cost is <span class="math inline">\(O\left(td^2 + tcd + \log_2(t/c)(t/c)d^2\right)\)</span>, once again avoiding a quadratic dependency on <span class="math inline">\(t\)</span>. Also, note that this algorithm makes an inner call to <code>LT_attention_parallel</code>, so we can use a flash-attention kernel to do that part of the computation.</p>
<p>This algorithm has a hyperparameter, the chunk size, which must be set correctly for optimal performance. Fortunately, it is inexpensive to identify the best chunk size empirically, since measuring just a few steps of training at each chunk size is sufficient to identify which is the fastest. In the plot below, we plot the speed of the optimally-chunked algorithm in each setting.</p>
<div class="column-body">
<iframe width="700" height="500" src="plots/chunked_vs_all.html">
</iframe>
</div>
<p>We see <code>LT_chunked</code> gives the desired best-of-both-worlds behavior, as it is equal-or-better than all other approaches in all settings. And we now see the massive (&amp; rapidly growing) speedup relative to standard self-attention, finally unlocking the true power of linear transformers.</p>
</section>
<section id="sampling" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="sampling">4. Sampling</h2>
<p>When working with language models, efficient training is not the only performance that deserves consideration. Once the model is trained, how expensive is it to utilize? When we are sampling we have a sequence of tokens, <span class="math inline">\(z_1 \cdots z_t\)</span>, and we want to sample the next token, <span class="math inline">\(z_{t+1}\)</span>.</p>
<p>The most efficient algorithm to sample from traditional transformers is called the <em>KV-cache</em> algorithm <span class="citation" data-cites="pope2023efficiently"><a href="#ref-pope2023efficiently" role="doc-biblioref">[14]</a></span>. This algorithm assumes that when we generate token <span class="math inline">\(z_{t+1}\)</span>, we will have already computed and cached all the <span class="math inline">\(K_i, V_i\)</span> for all <span class="math inline">\(0 \le i \le t\)</span>. In order to compute the output of the attention layer at time <span class="math inline">\(t+1\)</span> given this cached information, we can use <span class="math display">\[
Y_{t+1}^\text{Transformer} = \sum_{j=1}^{t+1} e^{Q^T_i K_j} V_j
\]</span> It is easy to see that this is an <span class="math inline">\(O(td)\)</span> operation. In other words, as the sequence length grows, sampling each subsequent token becomes more computationally expensive.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> This is one of the major limitations of the classic transformer architecture.</p>
<p>With linear transformers, however, we have access to the recurrent formulation. A linear transformer is an RNN where the state has size <span class="math inline">\(O(d^2)\)</span>. <span class="math display">\[
Y_{i} = S_i Q_i
\;\;\;\;\;\;\;\;\;\;\;\;
S_i = S_{i-1} + V_i K_i^T
\]</span> We can compare the time it takes to generate any particular token when sampling a sequence:</p>
<div class="column-body">
<iframe width="700" height="500" src="plots/sampling_speeds.html">
</iframe>
</div>
<p>As expected, we see that the time to sample of the linear transformer is independent of context length, whereas that of the KV-cache transformer grows linearly. This leads to a large gap in inference speed at large contexts.<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a></p>
</section>
<section id="learning-performance" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="learning-performance">5. Learning Performance</h2>
<p>Until now, our focus has been on how to implement linear transformers efficiently. But an architecture that runs really fast is useless unless it also learns well. We will now compare the learning curves of the GPT2 baselines with their linear transformer counterparts, at various context sizes.</p>
<p>In order to control for the fact that longer contexts help learning by introducing more tokens per update, we hold the number of tokens-per-update constant across context sizes by decreasing batch size. At all context-lengths, each update sees <span class="math inline">\(2^{19}\)</span> tokens.<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> Importantly, for this set of experiments, we have used the dataset c4 <span class="citation" data-cites="c4"><a href="#ref-c4" role="doc-biblioref">[4]</a></span>, which does not have much long-term structure: it consists of a shuffled collection of short articles, of average length ~500 tokens.</p>
<p>First, let’s look at the parameter scaling law of GPT2 and our modified Linear-GPT2. The following plot shows the final performance after 24h of training on our 8xH100 server for each scale and each context size. Click the second tab if you want to see the full learning curves.</p>
<div class="column-body">
<iframe width="700" height="500" src="plots/parameter_scaling.html">
</iframe>
</div>
<p>Both architectures scale similarly with respect to model size, but there is a gap in performance. This gap seems to grow as context size increases, together with more loss spikes and general instability. It’s possible that the gap can be explained by the linear transformer not effectively utilizing its context. To explore whether or not this is the case, we can use these same experiments, but visualize all context-lengths together.</p>
<div class="column-body">
<iframe width="700" height="500" src="plots/context_scaling.html">
</iframe>
</div>
<p>We see a dramatically different trend between the two architectures. For GPT2, each increase in context length slows down the initial pace of learning, but ultimately all context-lengths (beyond at least 1024) converge to a similar final loss. Whereas for Linear-GPT2, not only does increasing the context slow down learning in the beginning, but it also causes convergence to a worse final performance and nasty-looking loss spikes.</p>
<p>The results for GPT2 are what we would expect from a healthy model, given the setting. Note that since our dataset consists of short articles, of average length ~500 tokens, we can assume that even a context window of 1024 would include nearly everything relevant. Thus, we wouldn’t expect that increasing the context length beyond this point would decrease the loss the model ultimately converges to. Longer-context models do however need to learn to <em>ignore</em> many more irrelevant tokens, explaining the slowed initial learning.<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a></p>
<p>In contrast, the results for Linear-GPT2 seem to indicate some underlying instability of the linear transformer architecture. It doesn’t even seem capable of ignoring irrelevant information, let alone exploiting useful long-term structure.</p>
<p>Remedying these learning issues will be the main focus of our future work in linear transformers. Our current architecture is essentially a one-to-one clone of GPT2, sticking as architecturally close to the original as possible; aspects such as initializations, normalizations and hyperparameters were directly copied. It is well-known that these decisions can have a large impact on scaling and stability and often need to be tuned in an architecture-specific way. In the literature, various other linear variants of self-attention have been proposed, that incorporate techniques such as gating mechanisms in order to improve stability and learning <span class="citation" data-cites="choromanski2020rethinking qin2022cosformer peng2021random qin2022devil hua2022transformer sun2023retentive yang2023gated"><a href="#ref-choromanski2020rethinking" role="doc-biblioref">[5]</a>–<a href="#ref-yang2023gated" role="doc-biblioref">[11]</a></span>. A future post will include a thorough study of the impact of all of these choices.</p>
<p>Ultimately, it may be impossible for any linear transformer to perfectly match the context scaling laws of the classic transformer baseline. Although removing the exponential seems like a minor change, it represents a meaningful decrease in expressivity.<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> But as we’ve shown, linear transformers can be trained orders-of-magnitude more efficiently. On equivalent compute, linear transformers can be trained for many more steps; or, keeping steps constant as well, we can train a much larger model. If this additional scaling is sufficient to compensate for the reduced expressivity, linear transformers will be the more efficient architecture overall.</p>
<form action="https://buttondown.email/api/emails/embed-subscribe/manifestai" method="post" target="popupwindow" onsubmit="window.open('https://buttondown.email/manifestai', 'popupwindow')" class="embeddable-buttondown-form">
  <label for="bd-email" style="font-weight:bold;margin-right:20px;margin-top:20px;">
  Subscribe to be notified of new posts:
  </label>
  <input style="width: 35%;" type="email" name="email" id="bd-email" placeholder="Email">
  
  <input style="width: 80px;" type="submit" value="Subscribe">
</form>


</section>


<div id="quarto-appendix" class="default"><section id="related-work" class="level3 appendix"><h2 class="anchored quarto-appendix-heading">Related Work</h2><div class="quarto-appendix-contents">

<p><span class="citation" data-cites="shen2021efficient de2016cheap katharopoulos2020transformers shen2021efficient"><a href="#ref-katharopoulos2020transformers" role="doc-biblioref">[1]</a>, <a href="#ref-shen2021efficient" role="doc-biblioref">[15]</a>, <a href="#ref-shen2021efficient" role="doc-biblioref">[15]</a>, <a href="#ref-de2016cheap" role="doc-biblioref">[16]</a></span> are the first references to linear transformers we know of. In the broader literature of efficient transformers, this family of approaches is now sometimes referred to as “kernel” methods <span class="citation" data-cites="Tay2022Efficient"><a href="#ref-Tay2022Efficient" role="doc-biblioref">[2]</a></span>. Our experiments used the simple kernel from <span class="citation" data-cites="katharopoulos2020transformers"><a href="#ref-katharopoulos2020transformers" role="doc-biblioref">[1]</a></span>, but many other kernels have been proposed, many of which claim to better approximate the softmax and so may ameliorate the learning issues we highlighted <span class="citation" data-cites="choromanski2020rethinking qin2022cosformer peng2021random"><a href="#ref-choromanski2020rethinking" role="doc-biblioref">[5]</a>–<a href="#ref-peng2021random" role="doc-biblioref">[7]</a></span>. Other work has also claimed to improve learning of linear transformers via other techniques, including regularization and gating <span class="citation" data-cites="qin2022devil hua2022transformer sun2023retentive"><a href="#ref-qin2022devil" role="doc-biblioref">[8]</a>–<a href="#ref-sun2023retentive" role="doc-biblioref">[10]</a></span>.</p>
<p>Algorithmically, the majority of the literature either focuses on non-causal linear transformers (in which case parallelizable training with linear context is straightforward, by simply structuring computation as <span class="math inline">\(Q (K^T V)\)</span>), or uses only the <code>LT_state</code> algorithm. Some work <span class="citation" data-cites="hua2022transformer sun2023retentive"><a href="#ref-hua2022transformer" role="doc-biblioref">[9]</a>, <a href="#ref-sun2023retentive" role="doc-biblioref">[10]</a></span> uses a chunk-based approach, with attention within chunks and state propagation across chunks, processing each chunk sequentially. <span class="citation" data-cites="yang2023gated"><a href="#ref-yang2023gated" role="doc-biblioref">[11]</a></span> describes a chunked approach similar to the one we describe here, where intra-chunk calculation is conducted in parallel, but use a sequential algorithm (implemented as a Triton kernel) instead of the parallel prefix scan for inter-chunk calculation. To our knowledge, the fully parallel <code>LT_chunked</code> algorithm we describe in Section 3 is novel.</p>
</div></section><section id="acknowledgments" class="level3 appendix"><h2 class="anchored quarto-appendix-heading">Acknowledgments</h2><div class="quarto-appendix-contents">

<p>We would like to thank: Jono Ridgway for helping to prepare the release; Eric Alcaide for introducing us to the associative scan algorithm; Jannis Fengler for working on the tooling to confirm our JAX GPT2 numerically replicates NanoGPT; Joel Einbinder and Aaron Mondal for their assistance in setting up our engineering stack; Warfa Jibril, Tony Pezzullo, and Desh Raj for feedback on a draft of the blog post; Sander Dieleman for pointing us towards some early linear transformer literature. …</p>



</div></section><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body" data-entry-spacing="0" role="list">
<div id="ref-katharopoulos2020transformers" class="csl-entry" role="listitem">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">A. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret, <span>“Transformers are rnns: Fast autoregressive transformers with linear attention,”</span> in <em>International conference on machine learning</em>, PMLR, 2020, pp. 5156–5165.</div>
</div>
<div id="ref-Tay2022Efficient" class="csl-entry" role="listitem">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">Y. Tay, M. Dehghani, D. Bahri, and D. Metzler, <span>“Efficient transformers: A survey,”</span> <em>ACM Computing Surveys</em>, vol. 55, no. 6, pp. 1–28, 2022, doi: <a href="https://doi.org/10.1145/3530811">10.1145/3530811</a>.</div>
</div>
<div id="ref-dao2023flashattention" class="csl-entry" role="listitem">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">T. Dao, <span>“Flashattention-2: Faster attention with better parallelism and work partitioning,”</span> <em>arXiv preprint arXiv:2307.08691</em>, 2023.</div>
</div>
<div id="ref-c4" class="csl-entry" role="listitem">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline">C. Raffel <em>et al.</em>, <span>“Exploring the limits of transfer learning with a unified text-to-text transformer,”</span> <em>The Journal of Machine Learning Research</em>, vol. 21, no. 1, pp. 5485–5551, 2020.</div>
</div>
<div id="ref-choromanski2020rethinking" class="csl-entry" role="listitem">
<div class="csl-left-margin">[5] </div><div class="csl-right-inline">K. Choromanski <em>et al.</em>, <span>“Rethinking attention with performers,”</span> <em>arXiv preprint arXiv:2009.14794</em>, 2020.</div>
</div>
<div id="ref-qin2022cosformer" class="csl-entry" role="listitem">
<div class="csl-left-margin">[6] </div><div class="csl-right-inline">Z. Qin <em>et al.</em>, <span>“Cosformer: Rethinking softmax in attention,”</span> <em>arXiv preprint arXiv:2202.08791</em>, 2022.</div>
</div>
<div id="ref-peng2021random" class="csl-entry" role="listitem">
<div class="csl-left-margin">[7] </div><div class="csl-right-inline">H. Peng, N. Pappas, D. Yogatama, R. Schwartz, N. A. Smith, and L. Kong, <span>“Random feature attention,”</span> <em>arXiv preprint arXiv:2103.02143</em>, 2021.</div>
</div>
<div id="ref-qin2022devil" class="csl-entry" role="listitem">
<div class="csl-left-margin">[8] </div><div class="csl-right-inline">Z. Qin <em>et al.</em>, <span>“The devil in linear transformer,”</span> <em>arXiv preprint arXiv:2210.10340</em>, 2022.</div>
</div>
<div id="ref-hua2022transformer" class="csl-entry" role="listitem">
<div class="csl-left-margin">[9] </div><div class="csl-right-inline">W. Hua, Z. Dai, H. Liu, and Q. Le, <span>“Transformer quality in linear time,”</span> in <em>International conference on machine learning</em>, PMLR, 2022, pp. 9099–9117.</div>
</div>
<div id="ref-sun2023retentive" class="csl-entry" role="listitem">
<div class="csl-left-margin">[10] </div><div class="csl-right-inline">Y. Sun <em>et al.</em>, <span>“Retentive network: A successor to transformer for large language models,”</span> <em>arXiv preprint arXiv:2307.08621</em>, 2023.</div>
</div>
<div id="ref-yang2023gated" class="csl-entry" role="listitem">
<div class="csl-left-margin">[11] </div><div class="csl-right-inline">S. Yang, B. Wang, Y. Shen, R. Panda, and Y. Kim, <span>“Gated linear attention transformers with hardware-efficient training,”</span> <em>arXiv preprint arXiv:2312.06635</em>, 2023.</div>
</div>
<div id="ref-vaswani2017attention" class="csl-entry" role="listitem">
<div class="csl-left-margin">[12] </div><div class="csl-right-inline">A. Vaswani <em>et al.</em>, <span>“Attention is all you need,”</span> <em>Advances in neural information processing systems</em>, vol. 30, 2017.</div>
</div>
<div id="ref-tillet2019triton" class="csl-entry" role="listitem">
<div class="csl-left-margin">[13] </div><div class="csl-right-inline">P. Tillet, H.-T. Kung, and D. Cox, <span>“Triton: An intermediate language and compiler for tiled neural network computations,”</span> in <em>Proceedings of the 3rd ACM SIGPLAN international workshop on machine learning and programming languages</em>, 2019, pp. 10–19.</div>
</div>
<div id="ref-pope2023efficiently" class="csl-entry" role="listitem">
<div class="csl-left-margin">[14] </div><div class="csl-right-inline">R. Pope <em>et al.</em>, <span>“Efficiently scaling transformer inference,”</span> <em>Proceedings of Machine Learning and Systems</em>, vol. 5, 2023.</div>
</div>
<div id="ref-shen2021efficient" class="csl-entry" role="listitem">
<div class="csl-left-margin">[15] </div><div class="csl-right-inline">Z. Shen, M. Zhang, H. Zhao, S. Yi, and H. Li, <span>“Efficient attention: Attention with linear complexities,”</span> in <em>Proceedings of the IEEE/CVF winter conference on applications of computer vision</em>, 2021, pp. 3531–3539.</div>
</div>
<div id="ref-de2016cheap" class="csl-entry" role="listitem">
<div class="csl-left-margin">[16] </div><div class="csl-right-inline">A. de Brébisson and P. Vincent, <span>“A cheap linear attention mechanism with fast lookups and fixed-size representations,”</span> <em>arXiv preprint arXiv:1609.05866</em>, 2016.</div>
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>As discussed in Section 4, a second benefit of linear transformers is that the cost to sample a token does not grow with context size. Perhaps one could argue that this improvement in sampling speed could, on its own, justify using linear transformers for applications where the inference costs vastly exceed training costs. But it is evident to us that, for linear transformers to become actually useful, we need to address these instability issues.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>It is <em>not</em> named for the fact that the computational cost is linear with respect to <span class="math inline">\(t\)</span>! That is just a coincidence. (And is not even always true, as we will see.)<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Interestingly, <code>LT_state_parallel</code> is actually more expensive than <code>LT_state</code>. (This is in contrast with the attention formulation, where <code>LT_attention</code> and <code>LT_attention_parallel</code> share the same <span class="math inline">\(O(t^2 d)\)</span> cost.) As we will see in a moment, this extra <span class="math inline">\(\log_2(t)\)</span> factor is well worth the parallelization benefits.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>If <span class="math inline">\(y=x^2\)</span>, a log-log plot where <span class="math inline">\(y'=\log_a(y)\)</span> and <span class="math inline">\(x'=\log_a(x)\)</span> for any base <span class="math inline">\(a\)</span>, then <span class="math inline">\(y'=\log_a(y) = \log_a(x^2) = 2 \log_a(x) = 2 x'\)</span>. So the graph will be a line with slope 2.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>The reason we see the expected slopes asymptotically is that we are timing a full GPT2 architecture which has many other components besideds the attention layer. If we were only timing the attention layer, the plots would all be straight lines.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>An interesting connection is that the KV-cache can be understood as the state of an RNN with non-constant state size; namely, one whose state-size is <span class="math inline">\(O(td)\)</span>.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>This comparison may not be completely fair. In these experiments, our implementation of neither sampling algorithm makes use of specialized kernels. A lot of the ideas of flash attention can be used to write a much faster KV cache sampling algorithm; on the other hand, it’s unclear if much improvement is possible on the recurrent sampling. Thus, it’s possible that with engineering effort the gap between the two algorithms could become smaller. However, the overall pattern will certainly remain the same.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>e.g.&nbsp;runs with context-size 1024 would have batch-size of <span class="math inline">\(2^{19} / 2^{10} = 2^{9} = 512\)</span>.<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>Put another way: doubling the size of the input vastly increases the size of the function space over which gradient descent must search, and it’s intuitive that in a larger space it takes somewhat longer to find a good solution.<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>We plan to elaborate on this topic in a future blog post.<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@misc{buckman2024,
  author = {Buckman, Jacob and Gelada, Carles},
  publisher = {Manifest AI},
  title = {Linear {Transformers} {Are} {Faster} {After} {All}},
  date = {2024-01-05},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-buckman2024" class="csl-entry quarto-appendix-citeas" role="listitem">
<div class="">J.
Buckman and C. Gelada, <span>“Linear Transformers Are Faster After
All.”</span> Manifest AI, Jan. 05, 2024.</div>
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="m-a-n-i-f-e-s-t/website" data-repo-id="R_kgDOLA6vSg" data-category="Announcements" data-category-id="DIC_kwDOLA6vSs4Cgv3x" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" data-loading="lazy" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">
<p>© 2024 Manifest AI</p>
</div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




</body></html>